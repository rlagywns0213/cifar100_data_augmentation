{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"0512_1_random_change_2st_0.2_0.3.ipynb","provenance":[{"file_id":"18b80wpeQD1Wj6NjwZFFhZGDuMWWW1TIZ","timestamp":1619872182172},{"file_id":"1pSPNcOLDPSKONSeGALl1Jplu9ET9z-1q","timestamp":1619081668342}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"17b4a0ffd61d4aa090cd66fea58350da":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fed5e172d45e4812a6e282b9431a95b2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8bb6e75d3c214513b7b177673a97e562","IPY_MODEL_b9504b298f9b4f279994c506196814c2"]}},"fed5e172d45e4812a6e282b9431a95b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8bb6e75d3c214513b7b177673a97e562":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b7fd4e13e1d446c5945f35465e01c61e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":169001437,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":169001437,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9fa9c90e07dc4179a88f2ae8e75aa721"}},"b9504b298f9b4f279994c506196814c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_41026a655d8f4fcc9e704aa2ddb3fe6d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 169001984/? [01:23&lt;00:00, 2031832.68it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d19fa2eae832467c8905d869e5f4bfbc"}},"b7fd4e13e1d446c5945f35465e01c61e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9fa9c90e07dc4179a88f2ae8e75aa721":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"41026a655d8f4fcc9e704aa2ddb3fe6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d19fa2eae832467c8905d869e5f4bfbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"OSFGYaIDG6f0"},"source":["Random_erase Data Augmentation.\n","\n","+ https://github.com/zhunzhong07/Random-Erasing\n"]},{"cell_type":"markdown","metadata":{"id":"vCVSE5-UboYl"},"source":["##**Import all neceassary packages**"]},{"cell_type":"code","metadata":{"id":"5YBMwPsubsbX","executionInfo":{"status":"ok","timestamp":1620799438293,"user_tz":-540,"elapsed":25796,"user":{"displayName":"김효준","photoUrl":"","userId":"12098101409760390036"}}},"source":["import numpy as np\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import random\n","import torch.backends.cudnn as cudnn\n","from torch.optim.lr_scheduler import MultiStepLR\n","\n","from torchvision import datasets, transforms\n","\n","from tqdm.notebook import tqdm as tqdm\n","import math\n","import random"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L88afYXKMSdL"},"source":["##**Model - Define ResNet Model**"]},{"cell_type":"code","metadata":{"id":"eMFSLTnkMQdq","executionInfo":{"status":"ok","timestamp":1620799361628,"user_tz":-540,"elapsed":3885,"user":{"displayName":"김효준","photoUrl":"","userId":"12098101409760390036"}}},"source":["'''ResNet18/34/50/101/152 in Pytorch.'''\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18(num_classes=10):\n","    return ResNet(BasicBlock, [2,2,2,2], num_classes)\n","\n","def ResNet34(num_classes=10):\n","    return ResNet(BasicBlock, [3,4,6,3], num_classes)\n","\n","def ResNet50(num_classes=10):\n","    return ResNet(Bottleneck, [3,4,6,3], num_classes)\n","\n","def ResNet101(num_classes=10):\n","    return ResNet(Bottleneck, [3,4,23,3], num_classes)\n","\n","def ResNet152(num_classes=10):\n","    return ResNet(Bottleneck, [3,8,36,3], num_classes)\n","\n","def test_resnet():\n","    net = ResNet50()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test_resnet()"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qjM3cl279Lvg"},"source":["##**Utils**"]},{"cell_type":"code","metadata":{"id":"gIvuSgE49Kvu","executionInfo":{"status":"ok","timestamp":1620799361628,"user_tz":-540,"elapsed":3883,"user":{"displayName":"김효준","photoUrl":"","userId":"12098101409760390036"}}},"source":["class AverageMeter(object):\n","    r\"\"\"Computes and stores the average and current value\n","    \"\"\"\n","    def __init__(self, name, fmt=':f'):\n","        self.name = name\n","        self.fmt = fmt\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","    def __str__(self):\n","        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n","        return fmtstr.format(**self.__dict__)\n","\n","\n","class ProgressMeter(object):\n","    def __init__(self, num_batches, *meters, prefix=\"\"):\n","        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n","        self.meters = meters\n","        self.prefix = prefix\n","\n","    def print(self, batch):\n","        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n","        entries += [str(meter) for meter in self.meters]\n","        print('\\t'.join(entries))\n","\n","    def _get_batch_fmtstr(self, num_batches):\n","        num_digits = len(str(num_batches // 1))\n","        fmt = '{:' + str(num_digits) + 'd}'\n","        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n","\n","\n","def accuracy(output, target, topk=(1,)):\n","    r\"\"\"Computes the accuracy over the $k$ top predictions for the specified values of k\n","    \"\"\"\n","    with torch.no_grad():\n","        maxk = max(topk)\n","        batch_size = target.size(0)\n","\n","        # _, pred = output.topk(maxk, 1, True, True)\n","        # pred = pred.t()\n","        # correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","        # faster topk (ref: https://github.com/pytorch/pytorch/issues/22812)\n","        _, idx = output.sort(descending=True)\n","        pred = idx[:,:maxk]\n","        pred = pred.t()\n","        correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","        res = []\n","        for k in topk:\n","            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n","            res.append(correct_k.mul_(100.0 / batch_size))\n","        return res"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9s8oXpzdMvol"},"source":["##**Parameter Settings**"]},{"cell_type":"code","metadata":{"id":"Pjeqawi9cNK6","executionInfo":{"status":"ok","timestamp":1620799361628,"user_tz":-540,"elapsed":3880,"user":{"displayName":"김효준","photoUrl":"","userId":"12098101409760390036"}}},"source":["dataset = 'cifar100' # cifar10 or cifar100\n","model = 'resnet34' # resnet18, resnet50, resnet101\n","batch_size = 128  # Input batch size for training (default: 128)\n","epochs = 150 # Number of epochs to train (default: 200)\n","learning_rate = 0.1 # Learning rate\n","data_augmentation = True # Traditional data augmentation such as augmantation by flipping and cropping?\n","# cutout = True # Apply Cutout?\n","# n_holes = 1 # Number of holes to cut out from image\n","# length = 16 # Length of the holes\n","seed = 0 # Random seed (default: 0)\n","print_freq = 30\n","cuda = torch.cuda.is_available()\n","cudnn.benchmark = True  # Should make training should go faster for large models\n","\n","np.random.seed(0)\n","random.seed(0)\n","# Random Erasing\n","p=0\n","sh=0.4\n","r1=0.3\n","\n","\n","\n","torch.manual_seed(seed)\n","if cuda:\n","    torch.cuda.manual_seed(seed)\n","\n","test_id = dataset + '_' + model"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eXL_PBj6cVoe"},"source":["##**Load and preprocess data**"]},{"cell_type":"code","metadata":{"id":"L_ITbu_h6H1R","executionInfo":{"status":"ok","timestamp":1620799363670,"user_tz":-540,"elapsed":874,"user":{"displayName":"김효준","photoUrl":"","userId":"12098101409760390036"}}},"source":["class Randomchanging(object):\n","    '''\n","    Class that performs Random Erasing in Random Erasing Data Augmentation by Zhong et al. \n","    -------------------------------------------------------------------------------------\n","    probability: The probability that the operation will be performed.\n","    sl: min erasing area\n","    sh: max erasing area\n","    r1: min aspect ratio\n","    mean: erasing value\n","    -------------------------------------------------------------------------------------\n","    '''\n","    def __init__(self, probability = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3):\n","        self.probability = probability\n","        self.sl = sl\n","        self.sh = sh\n","        self.r1 = r1\n","       \n","    def __call__(self, img):\n","\n","        if random.uniform(0, 1) > self.probability:\n","            return img\n","\n","        for attempt in range(100):\n","            area = img.size()[1] * img.size()[2]\n","       \n","            target_area = random.uniform(self.sl, self.sh) * area\n","            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n","\n","            h = int(round(math.sqrt(target_area * aspect_ratio)))\n","            w = int(round(math.sqrt(target_area / aspect_ratio)))\n","\n","            if w < img.size()[2] and h < img.size()[1]:\n","                x1 = random.randint(0, img.size()[1] - h)\n","                y1 = random.randint(0, img.size()[2] - w)\n","                change_x1 = random.randint(0, img.size()[1] - h) # 새로운 위치 정의\n","                change_y1 = random.randint(0, img.size()[2] - w) # 새로운 위치 정의\n","                if img.size()[0] == 3:\n","                    img[0, x1:x1+h, y1:y1+w] = img[0, change_x1:change_x1+h, change_y1:change_y1+w] # 새로운 위치의 값으로 변경\n","                    img[1, x1:x1+h, y1:y1+w] = img[1,change_x1:change_x1+h, change_y1:change_y1+w] # 새로운 위치의 값으로 변경\n","                    img[2, x1:x1+h, y1:y1+w] = img[2, change_x1:change_x1+h, change_y1:change_y1+w] # 새로운 위치의 값으로 변경\n","                else:\n","                    img[0, x1:x1+h, y1:y1+w] = img[0, change_x1:change_x1+h, change_y1:change_y1+w] # 새로운 위치의 값으로 변경\n","                return img\n","\n","        return img"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120,"referenced_widgets":["17b4a0ffd61d4aa090cd66fea58350da","fed5e172d45e4812a6e282b9431a95b2","8bb6e75d3c214513b7b177673a97e562","b9504b298f9b4f279994c506196814c2","b7fd4e13e1d446c5945f35465e01c61e","9fa9c90e07dc4179a88f2ae8e75aa721","41026a655d8f4fcc9e704aa2ddb3fe6d","d19fa2eae832467c8905d869e5f4bfbc"]},"id":"9pNFGOTZbs7z","executionInfo":{"status":"ok","timestamp":1620799381209,"user_tz":-540,"elapsed":9768,"user":{"displayName":"김효준","photoUrl":"","userId":"12098101409760390036"}},"outputId":"211b2ad8-500b-4749-add0-6e3716eba227"},"source":["# Image Preprocessing\n","normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n","                                     std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n","\n","train_transform = transforms.Compose([])\n","if data_augmentation:\n","    train_transform.transforms.append(transforms.RandomCrop(32, padding=4))\n","    train_transform.transforms.append(transforms.RandomHorizontalFlip())\n","train_transform.transforms.append(transforms.ToTensor())\n","train_transform.transforms.append(normalize)\n","train_transform.transforms.append(Randomchanging(probability = 0.2, sh = 0.2, r1 = 0.3 ))\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    normalize])\n","\n","\n","num_classes = 100\n","train_dataset = datasets.CIFAR100(root='data/',\n","                                    train=True,\n","                                    transform=train_transform,\n","                                    download=True)\n","\n","test_dataset = datasets.CIFAR100(root='data/',\n","                                    train=False,\n","                                    transform=test_transform,\n","                                    download=True)\n","\n","\n","train_loader =torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True,\n","                                           pin_memory=True,\n","                                           num_workers=2)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False,\n","                                          pin_memory=True,\n","                                          num_workers=2)\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17b4a0ffd61d4aa090cd66fea58350da","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=169001437.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting data/cifar-100-python.tar.gz to data/\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"WT9MByVKtVuz","executionInfo":{"status":"ok","timestamp":1620792159104,"user_tz":-540,"elapsed":8452,"user":{"displayName":"김효준","photoUrl":"","userId":"12098101409760390036"}},"outputId":"9b3a0258-a34d-4249-99ee-b7ec044dd9fc"},"source":["import matplotlib.pyplot as plt\n","import math\n","def custom_imshow(img):\n","  img = img.numpy()\n","  img = np.transpose(img, (1, 2, 0))\n","  mean = np.array([x / 255.0 for x in [125.3, 123.0, 113.9]])\n","  std = np.array([x / 255.0 for x in [63.0, 62.1, 66.7]])\n","  img = img*std + mean\n","  plt.imshow(img)\n","  plt.show()\n","  \n","dataiter = iter(train_loader)\n","images, labels = dataiter.next()\n","for i in range(10):\n","    custom_imshow(images[i])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAbO0lEQVR4nO2de4ycV3nGn/eby168F+96HXt9XdsxhBAgCSakEGgIAgVKSSIhlFRCkYgwQiBBRf+I0qpQiT8AldtfVAaiBgQEyqVEbdoSDCIiKQlOCM7FiZM4NrZjex3v1Xufmbd/zLhaR+c5u/HuzA6c5yetdva8c+Y7c3ae+b45z7zvMXeHEOLPn2ylByCEaAwSuxCJILELkQgSuxCJILELkQgSuxCJkF9KZzO7HsDXAOQAfNPdP7/A/eXzCVFn3N1C7XahPruZ5QAcBPAuAMcA/A7ALe7+VKSPxC5EnWFiX8pl/FUAnnP3Q+4+C+BuADcs4fGEEHVkKWLfCODovL+P1dqEEE3Ikj6zLwYz2w1gd72PI4SIsxSxHwewed7fm2pt5+HuewDsAfSZXYiVZCmX8b8DsNPMtplZEcDNAO5ZnmEJIZabCz6zu3vJzD4B4H9Qtd7udPcnY30uvvhifPWrXw3G2osttF+xtTXYnuV5n3xLuA8AFArBxUoAQC5HQ8gy3o8R65FlfPqzrMAf0/h7dJaRWGTsWeSCq+IlHitXeKxCYrEJcf68LBKLPSZzm8qRYZQjDlVvdyeNXXTRWhorZpEXVoNY0md2d78XwL3LNBYhRB3RN+iESASJXYhEkNiFSASJXYhEkNiFSIS6f4NuPuW5EoYHzwRjJ6e4xTOLsG2RL7bTPsVikcayIreMcrwbWgphO8wQeTyL+EIR67CS8Vgu4g+yw1FLDkCWi3lX3KQqRay3cpn0i9haFvHQDDF7kPfzSrhfLvI/mxofobGt27bSWF9fH401AzqzC5EIErsQiSCxC5EIErsQiSCxC5EIDV2NrxgwRZIuHj1ymPY7OTYXbC+0dtA+WWS1tZjnq7fFIk9AKZMF4VKJr1i78ZXzWAJKLBvYIiv8LPGDrUpXH5DHYiv1WcxpYH0iCSGFPD/35CLjsDxPemrNh+2VbGqU9unv4k7Ixk0baCye5bPy6MwuRCJI7EIkgsQuRCJI7EIkgsQuRCJI7EIkQmMTYbyC8dJMMNY9sI3227Dh1eFALBEGPLHmzKGnaWzoxEka237J64LtbWvW0z5TFZ5ZY5FKaLnKNI1dCGVWEw6AV6IV2WiE1pkDr+MWswA9Uu/OPWy/AsBsmb+M2wthG+3gg7+kfaZGhmjsyjfy54xIsk4zoDO7EIkgsQuRCBK7EIkgsQuRCBK7EIkgsQuRCEuy3szsMIBxVP2Zkrvvit4fhmIunFVWdJ65lC+uCvfp6KV9Th56hsbuvfs7NDZ07Hkau+6Gm4Ptb3nfLbQP8t00xI2mhaMMlhEX23yIZcoBQCU2jkhdO5YBZhHnyo3baxYZx2yFZyp2FNuC7W193H6dGT5CY7G6e83Ocvjs73D3l5bhcYQQdUSX8UIkwlLF7gB+bmaPmNnu5RiQEKI+LPUy/hp3P25mFwG4z8yedvf759+h9iawGwB61qxZ4uGEEBfKks7s7n689nsQwE8BXBW4zx533+Xuuzo6eBkpIUR9uWCxm9kqM+s8dxvAuwE8sVwDE0IsL0u5jF8H4Kc1qycP4Hvu/t/RHmYwCx8yV+K2S6kctl1aK/y96sC+B2js2KOP0Ni2/rDNBwAzx/YF28dOvpH2WbXpzTRWjhSqLGcx6y1SxJJshZTFdniKJGvlIplt463hDEYAyBCex+5IMt9IgT+eTY/TWMX562A6vy7cpy1syQEARiP/F/Ja/FPggsXu7ocAvGEZxyKEqCOy3oRIBIldiESQ2IVIBIldiESQ2IVIhIYWnITz4oblSDbRBNu3LeYndfD9urLcLI11FXmBSIyGi1GOjPJ9w0pb+ONNGLeacpEsr9iOYhkpepiLJmvxRyxE9swrzPLYjIXHP13hz6u/NEFjv3/gHhp7+vCLNPaWmz4cbF/VEnl9GD8HejlWnLO5M+J0ZhciESR2IRJBYhciESR2IRJBYhciERq6Gm9wFDy8YlmKJMKcmQuv4I7N8ZXRwoZ+GrtoJ4915HnCRQsZ4+wQ3zJqauYYj5F6fAAQyT9BxfkqvpNEmOi/miQnAYBHVpi7p3gGzUQhPFejRT6/rx47TmOtx3jy0uO//i2Nrb349cH2/vVbaR/LuINSidXri2xt1QzozC5EIkjsQiSCxC5EIkjsQiSCxC5EIkjsQiRCg603oEDeXiwXed/paA82l4p8+Dsv2UFja659G42d3vcLGpubHA62H3zgp7TPazvHaGzda6+hseE2Pn5ri1h2hXDtt+kKr7k2TZKTAKCcn6Sxjkluy1khXCevaEO0z8STv6Ox7OxRGlvdx62y0urwXGXre2ifmZO8xl8p5ok2OTqzC5EIErsQiSCxC5EIErsQiSCxC5EIErsQibCg9WZmdwJ4H4BBd7+s1tYL4AcABgAcBvBBdw/7Uuc/FrIs/P6Sa+F2x6quzmB7Zzu3kzZEatqVJ7kddmxiisZmC+Hpmsv4sZ575Dc0tuPFgzS2eetOGptqjVhvvZuC7e3rXkf7zPbwDLDB1rDtCQD5dv4/6yA19LaN8fntyfhLqBA5L63u20Bjm99wRbC9tTX8mgKAiTyvk1eK1NAj5f+ahsWc2f8VwPUva7sdwF533wlgb+1vIUQTs6DYa/utv/ybEDcAuKt2+y4ANy7zuIQQy8yFfmZf5+4nardPorqjqxCiiVnyAp27OyKfVsxst5ntM7N94+O8SokQor5cqNhPmVk/ANR+D7I7uvsed9/l7rs6O/miiBCivlyo2O8BcGvt9q0AfrY8wxFC1IvFWG/fB3AtgD4zOwbgMwA+D+CHZnYbgCMAPriYgzn4BjlOLDkAaG8JW0193IFCxxDfEmiHT9MYz60C9p8NZ4e9/W9uoX22b+G21uz+X9OYn+HjH2/h2zV5Pnz1VOrmNllxkr8MduR5Rtl4J/8HdJEtsXY8fYT26esMZ+wBwP/Ocpu1r4svGW1Yty3YPneWf6Q8S4qiAvHtn2LFKJuBBcXu7uyV/M5lHosQoo7oG3RCJILELkQiSOxCJILELkQiSOxCJEJDC07CHSBZQ7F3ndX58DBbbZYfavgwjfW2T9DY9rVraOwg2YqsayC8nxgAbL/2OhorXXI1jZUzvn/cujaeHTY3Hi6k+MzYdtrn9CFuQ3WMPk9jvddyW7F7MGyxdTzzR9pn/et59tqWS8LZfAAwWO6lMfY1rjnnr7gx45aiRd215rbedGYXIhEkdiESQWIXIhEkdiESQWIXIhEkdiESoeF7vdF3l8geWkViaRBHDgBw+gy3rnzkFI3NRgoKri2GM8DanRdRHJnhsXJXP431RbYU63vhARqbfP5QsH3rdB/tk0MHjXW9eILG1j81QGMdq+aC7TN9h2mfB17kFuDENLcbt259FY31dIULZp4+y+1X5CPplMYzDpsdndmFSASJXYhEkNiFSASJXYhEkNiFSITGJsIAMPb+EqnfZSRWiSyMDs/y5eyJyELseCW8igwABYRXi9vGeL24Yo7XfpvwSC25My/R2JGh52jshVw40WTqJMniAdA7GnnOzidr9kG+Un98czg5ZfAIn6tTh57lx5pupbHWjfxlnCcr69bKa+t5gf/PmjvVJY7O7EIkgsQuRCJI7EIkgsQuRCJI7EIkgsQuRCIsZvunOwG8D8Cgu19Wa/ssgI8AOF272x3ufu9iDuhk95yszK2yPNk0KoskLOQ6w7XYAGB4jlsrsxGrLJcLbxv1xN7/pH3Wvuk9NNazZi2NrStw621sJ09cGapcHGyfe4YnmYw8coDGTo2+QGPHdvB6fZe+/wPB9v4yP1Zl7100djJH9w5FSyXyOsiRWI6f5ywf8XQjoazJk2QWc2b/VwDXB9q/4u6X134WJXQhxMqxoNjd/X4AQw0YixCijizlM/snzGy/md1pZvyaWQjRFFyo2L8OYAeAywGcAPAldkcz221m+8xs3/j42Qs8nBBiqVyQ2N39lLuX3b0C4BsArorcd4+773L3XZ2dfGFJCFFfLkjsZja/ntJNAJ5YnuEIIerFYqy37wO4FkCfmR0D8BkA15rZ5agmAR0G8NHFHMwdqFTCeUO5SDoRc0KKrTwTavX6jTT2+CSvM3dmaIzGOlvC9cwGn3qG9hnbx+vFdb+f23KDpIYbAPQN8Qy2d5wN12o7OcG3XTozyTPRRse4FVl61QCNld/06mC7neKZcohs55WFXU8AQHuRXzHmc+Hstsocfw3YHJ/7LOa9NTkLit3dbwk0f6sOYxFC1BF9g06IRJDYhUgEiV2IRJDYhUgEiV2IRGh4wUmQIouZ8fedAslEs4z3WROx3rZt59sFlXItNObEdmk7PUP7PPC9b9LY5ZH32o2XdtLY5KlJGms7MxJsb5/m3tVwB88ac+PbRq17w+U0tmom/NLqfWmU9pnKc//1VMR6K0cqj7I6plbi9ppP81gssY24yk2DzuxCJILELkQiSOxCJILELkQiSOxCJILELkQiNNR6MwA5Yr1FnDfkc+E+rXPc69iyNVx4EQCu/Ngnaez3z/GsrMHTp4Ptxx5+kPZ5/NHHaOy/vvg5GtuwlRf/WdXH7au9Q+EKYt2zXbTPm//qwzR2yV/+NY3NDvA5bn8xnD3YdyJsDQJAvpfbjQ9XwnMPAFmeFyvd0hZuL/JkPsxFMtuyjHfMmjwhTmd2IRJBYhciESR2IRJBYhciESR2IRKh4YkwRjITLLaU6eHV1vZIIkwxkrGQzU3QWGuZJ5mMjofru011hmvTAcCNf/sPNDY8zrM7zg4fobFSpFZbriVcc21LZOV8w5veRmPl9ZtprFLhCUAnfvtosH3kaLgdAC57Iz9W3zGybxiAqXaeyFMuh19v5Wle1rxc5seyyPZg0SyZJkBndiESQWIXIhEkdiESQWIXIhEkdiESQWIXIhEWs/3TZgDfBrAO1e2e9rj718ysF8APAAygugXUB9097E3No0Kstwqx1wCgpRC2NHJ5boOMDE3RWNfZMzTWkXFLZv3asMV25izffig3MEBjV176FhrLI1IjLYvEQOa3wv/V0zE3aY7baz50ksaefvjXwfbV5VO0z4atl9LYzBifY2sL240AkM/CNlou8rxanFtv+YjdmzV5JsxizuwlAJ9290sBXA3g42Z2KYDbAex1950A9tb+FkI0KQuK3d1PuPujtdvjAA4A2AjgBgB31e52F4Ab6zVIIcTSeUWf2c1sAMAVAB4CsM7dzyV/n0T1Ml8I0aQsWuxm1gHgxwA+5e7nVSZwdwfCHxbNbLeZ7TOzfeNnx5c0WCHEhbMosZtZAVWhf9fdf1JrPmVm/bV4P4DBUF933+Puu9x9V2cHr0QihKgvC4rdzAzV/dgPuPuX54XuAXBr7fatAH62/MMTQiwXi8l6eyuADwF43MzOFVS7A8DnAfzQzG4DcATABxd6IAdQJtZQucwzuUqzYRttdpb3meXuFEZneJbUgVP8o8YzRw8F21fFtk8qc5tvqsSz7zzXSmMteb5FFRC2jSrlSBZggb8MZia4VXbgwV/S2EuD4bnK9/K5+vcXDtDYbHc/je28mGf0WRa2dHPlEu2Tj+zjVMg3fse05WLBkbv7bwBage+dyzscIUS90DfohEgEiV2IRJDYhUgEiV2IRJDYhUiEhvoI7o5yJWyFVMCtkPJc2HorVXifkvP3sUOj3FoZ7xmgsU5SiLDL+ZZGM5HMsGKZF5z0HP/X5CP9jGTLGU/kgpX5XE2RIpsAcOrMURpr61sVbC/2rqd9urfygpNdGy+jsdLajTQ2PBW2ZycnedZbFpFFPsetw2Y/dzb36IQQy4bELkQiSOxCJILELkQiSOxCJILELkQiNDyFh2W9xQpOskyus5O8OOT46CiNtfX20tim9Ty7qjMLZ1fdf++PaJ+nnj9NYz2v4UUx8y0RW67Cn3fm4X6FSKHE6XFuQ52OWIdZe6T44prVwfaW3g20T+e619DY0Vl+rMlhnqnYWwlbh6de4nYpnMsiF7FEeb5Yc6AzuxCJILELkQgSuxCJILELkQgSuxCJ0ODVeId7ODGhVOLJKcOjZLW4zGu4FdrCiRgA0NfKYy0Zn5K5crj226ZXXU379MzxrYkmxrhj4DmeuTIy/CKNjY2HV/9jpdMmhvnK9NhLwaLBAIC+tXxLptb124Lts0XeZ2SKvwbaIw5KVuD7V+Xmwiv105N87gvGHy+LTGRGnKZmQWd2IRJBYhciESR2IRJBYhciESR2IRJBYhciERa03sxsM4Bvo7olswPY4+5fM7PPAvgIgHNezx3ufu+CR7RwwksWsbzaW8JWWevqLtqn1doiMV67bq7E940ql8Nj7Nm8g/bpoRGgu5NbgLmI/bN2FU+4GB4O10ibK0/SPr6Kz1XHlgEa61nNn93EZNgWfekM3w5rcpgn+BTHeUJOvsjrwuVbw3ZpX6TmYWcft/mi2z9Fko2agcX47CUAn3b3R82sE8AjZnZfLfYVd//n+g1PCLFcLGavtxMATtRuj5vZAQC8nKcQoil5RdcdZjYA4AoAD9WaPmFm+83sTjOLXbEKIVaYRYvdzDoA/BjAp9x9DMDXAewAcDmqZ/4vkX67zWyfme07e5Z/JhNC1JdFid3MCqgK/bvu/hMAcPdT7l529wqAbwC4KtTX3fe4+y5339XRwb8XLYSoLwuK3cwMwLcAHHD3L89rn1+/6SYATyz/8IQQy8ViVuPfCuBDAB43s8dqbXcAuMXMLkfVjjsM4KOLOaARS6nFuJ20htgnq1p5RllbpFZYJbJ90tAM/6gxNxXuNzfNa8nFmJrj9k9bZPsqzPLjrfbw/M6UIjXoJnm21uwUzyx8+uBxGhsfD2ebzcyGsx4BoC2L2I1FblN29HALtr0rfDW5fv062qc1MvcthVjWW3OzmNX43yBcSW9hT10I0TQ0+5uREGKZkNiFSASJXYhEkNiFSASJXYhEaGzBSTPkLHzIWGHD+++9J9g+W+FbRo0PvURjOefFHAuR4oVm4ffGXMQy6unl3yLu6w5vkQQAna3tNJbP8/doJ1tllUp8i6csYjXlnWeUbVnN7atsTXgbrbY2nmHX2dpKY8UKt+wmIs9tjFiHExG7dNOO8DZfALBhE08Lae5ykzqzC5EMErsQiSCxC5EIErsQiSCxC5EIErsQidBQ681gyHJhK6dIbC0AOPb8wWB7T98a2mdTB8+S6m7hFk9fT8QO6wpnV2WRsedyPBbdNyxirxWLPNuvQLKySiVuN7ZH7DBzno1YLvPH7F4dnseJs+FsOAD449GjNDY8NkRjlYjpNTUbtuXKEdv2zddcQ2PdEbu02dGZXYhEkNiFSASJXYhEkNiFSASJXYhEkNiFSISGWm8Vd8zMhLOXtm/ZTPsNDGwIthcKkT2+IplclUjWW+ztr1QO2zjt7TxDrSdi5bW2hAtpAvFtw1jRTgA4Ox7O8ipEnlguUuwTxudqcpIX5yyVws9teIRnI45E7TX+nEuzfH++bDb83PrXhV9TANDXzfd6+1NGZ3YhEkFiFyIRJHYhEkFiFyIRJHYhEmHB1XgzawVwP4CW2v1/5O6fMbNtAO4GsAbAIwA+5O68UBiAmZlpHDz0bDC2dfMm2q99VXi1uzzDa49NT/ItnkqR1fgssr0Po6vEt3FqiSSZTEfGH9vxdmDrAI1t3hp2NWIr+NW9OcPEnItYItL0dPi55SMOykUXXURjlvF+XubjnyPuz8YNvJZcewv/n3kk6caCGyc1D4s5s88AuM7d34Dq9szXm9nVAL4A4CvufjGAYQC31W+YQoilsqDYvcq500yh9uMArgPwo1r7XQBurMsIhRDLwmL3Z8/VdnAdBHAfgOcBjLj7uevXYwD4dZEQYsVZlNjdvezulwPYBOAqAJcs9gBmttvM9pnZvqmpC9vaWAixdF7Rary7jwD4FYC/ALDa7P93fNgEILhZt7vvcfdd7r4rtkGAEKK+LCh2M1trZqtrt9sAvAvAAVRF/4Ha3W4F8LN6DVIIsXQW4zP1A7jLqt5NBuCH7v4fZvYUgLvN7HMAfg/gWws9UKlcxvDwmWBsQz/fSsg9bHdMXuDHgkKB13CL1XdrIYkr7W08EaazvYPGhkdHIrExGrsoYtmtqpC5mhylfZhNBgCVSK22WA06lhy0rp8v7cxEnldsHLErRhZbu3Yt7ZNF6gZWyPwCQD5rbuttQbG7+34AVwTaD6H6+V0I8SeAvkEnRCJI7EIkgsQuRCJI7EIkgsQuRCIYs7XqcjCz0wCO1P7sA8ALkjUOjeN8NI7z+VMbx1Z3D/qKDRX7eQc22+fuu1bk4BqHxpHgOHQZL0QiSOxCJMJKin3PCh57PhrH+Wgc5/NnM44V+8wuhGgsuowXIhFWROxmdr2ZPWNmz5nZ7Ssxhto4DpvZ42b2mJnta+Bx7zSzQTN7Yl5br5ndZ2bP1n73rNA4Pmtmx2tz8piZvbcB49hsZr8ys6fM7Ekz+2StvaFzEhlHQ+fEzFrN7GEz+0NtHP9Ua99mZg/VdPMDM+MpmiHcvaE/AHKolrXaDqAI4A8ALm30OGpjOQygbwWO+3YAVwJ4Yl7bFwHcXrt9O4AvrNA4Pgvg7xo8H/0Arqzd7gRwEMCljZ6TyDgaOicADEBH7XYBwEMArgbwQwA319r/BcDHXsnjrsSZ/SoAz7n7Ia+Wnr4bwA0rMI4Vw93vB/DyXQxvQLVwJ9CgAp5kHA3H3U+4+6O12+OoFkfZiAbPSWQcDcWrLHuR15UQ+0YAR+f9vZLFKh3Az83sETPbvUJjOMc6dz9Ru30SAK/mUX8+YWb7a5f5df84MR8zG0C1fsJDWME5edk4gAbPST2KvKa+QHeNu18J4D0APm5mb1/pAQHVd3YgshtBffk6gB2o7hFwAsCXGnVgM+sA8GMAn3L380r1NHJOAuNo+Jz4Eoq8MlZC7McBzN+2hBarrDfufrz2exDAT7GylXdOmVk/ANR+D67EINz9VO2FVgHwDTRoTsysgKrAvuvuP6k1N3xOQuNYqTmpHfsVF3llrITYfwdgZ21lsQjgZgD3NHoQZrbKzDrP3QbwbgBPxHvVlXtQLdwJrGABz3PiqnETGjAnZmao1jA84O5fnhdq6JywcTR6TupW5LVRK4wvW218L6ornc8D+PsVGsN2VJ2APwB4spHjAPB9VC8H51D97HUbqnvm7QXwLIBfAOhdoXF8B8DjAPajKrb+BozjGlQv0fcDeKz2895Gz0lkHA2dEwCvR7WI635U31j+cd5r9mEAzwH4NwAtr+Rx9Q06IRIh9QU6IZJBYhciESR2IRJBYhciESR2IRJBYhciESR2IRJBYhciEf4Pn4Wgde7nyh4AAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZ+klEQVR4nO2dbYxc5XXH/2fedr0vtvfF715s8xYKJBCyJSShlCQlomkkEqlC0BbxAcVRFdRGTT8gGjW0n5KqSRSlLamTIEiVJqEBBEkRiWOl4SUJsIDBdnixDXa89tprs7bXu+vdeTv9MEO60OecXc/eubPh+f8ky7P3zHPvmefeM3fm+c85R1QVhJC3P5lWO0AISQcGOyGRwGAnJBIY7IREAoOdkEhgsBMSCbmFDBaRawF8FUAWwDdV9Qve83t7e3X9wLqFHDIRBNJqFwAAisZkz8Xiv0uaLiasHntytIjzwhJ+zS88v6Ohcaoa9KThYBeRLIB/BXANgGEAT4vIQ6r6a2vM+oF1+O+HH2z0kImREecDjXcyE6ZarTQ0LpPJJuyJd6zG5iOT5jx6vxWxTI57xWLJtBXyedPW6FxZrF51VqL7W8jH+MsB7FHVV1W1COB7AK5Lxi1CSNIsJNjXATgw6+/h+jZCyCKk6Qt0IrJZRIZEZGjs9bFmH44QYrCQYD8IYGDW3+vr296Eqm5R1UFVHezt613A4QghC2Ehwf40gPNEZJOIFADcAOChZNwihCRNw6vxqloWkVsB/Bg16e0uVd2VmGdkcdCgrOWukC8GFpN7njqUIAvS2VX1YQAPJ+QLIaSJ8Bd0hEQCg52QSGCwExIJDHZCIoHBTkgkLGg1/neXqmNL+P2vGRJPI7KWm6Nhz0fVPZQ3V8kmhaSZWJPmsQAA6l2PycE7OyGRwGAnJBIY7IREAoOdkEhgsBMSCemuxquiquGSP41V9Gls1dRfYXZooMQR1Hk/9WwO1UZWb8UZ0/Dic9m0ZMS6tBorqVWtNrZibc2VV5qs6lwgnh9eubDFkBjEOzshkcBgJyQSGOyERAKDnZBIYLATEgkMdkIiIVXpTeFIFxn7fceW5RyJJGl5reExTtcXV0JrZD4cPOltkZDLOq85a8tajXRiKZftri/F0oxp866ratXVYG1TSok3vLMTEgkMdkIigcFOSCQw2AmJBAY7IZHAYCckEhYkvYnIPgCnUNOXyqo66I9QW25SW36oWmlZTiaRl2WUeo0xg6p6WWNOdlhD/jf4mhvM1rKEvpwjkx04MGzahodt29TUpGmbnBoPbr/ooovMMQMD602bd858cdO5HlO65yahs39QVY8lsB9CSBPhx3hCImGhwa4AfiIiz4jI5iQcIoQ0h4V+jL9SVQ+KyEoAW0XkJVV9dPYT6m8CmwFg7bo1CzwcIaRRFnRnV9WD9f9HATwA4PLAc7ao6qCqDvb29izkcISQBdBwsItIp4h0v/EYwEcA7EzKMUJIsizkY/wqAA9ITQbKAfhPVX2k0Z1Vq052WML4EsmZv/9lHFmrqvbr8gpHOrULG2rJ5Mo7jpTnFl905aSwRJXJt5ljfvijH5m2u++627SVK0Xbj2zYx8997nZzzKZNA6atWLSz5Vy8bLk5rsikaDjYVfVVAJck6AshpIlQeiMkEhjshEQCg52QSGCwExIJDHZCIiHdXm+LBVe6OnMZpFHhxKuT6PYG82zSQEM6TxZqNHuwgWy56elp0zY29rrthyNTLu/tDm4vFOxLv1xpTC71ZEpvPjJOoc0k4Z2dkEhgsBMSCQx2QiKBwU5IJDDYCYmEFqzGN9qX6czwk0W8BI6k69M5iSTuobwX4CTQGMP8YzlGTzHw6uQZPlbVbq20etVK09bR2WXayiU7EaZaDq+s7969xxxz5R/+gWkTR4HIoWDaXNIJCd7ZCYkFBjshkcBgJyQSGOyERAKDnZBIYLATEgmpS29eTTYLK2HEzzuwjWauCNCYDOLIMZmsLU+VSnY9s3w+b+8zY582qz1R1agJBwDwJDSnLZeb3AHrPNuvuaOz3bRVneSUctF5bW3huRobO2EOmZ6xfSy02fLa5MSEaevssqXDqjfHCcI7OyGRwGAnJBIY7IREAoOdkEhgsBMSCQx2QiJhTulNRO4C8DEAo6p6cX1bL4DvA9gIYB+A61X1+JxHUzQkbVkSmyeveXi107y6cOVyWOKZmJgyx+z/zX7Ttn7DOtP2063bTNu5555v2t57xf/rrQkAKFbsbLOK8boAAGrfD8SZLKt2XbVin7P+Prvx59p1K0zbsdFjth+GZOe1w5qesOdq29b/MW1XXfk+07Zs2TLTVnQk2CSZz539bgDXvmXbbQC2qep5ALbV/yaELGLmDPZ6v/Wxt2y+DsA99cf3APh4wn4RQhKm0e/sq1R1pP74MGodXQkhi5gFL9Bp7Yuz+UVMRDaLyJCIDI2Nzf21nhDSHBoN9iMisgYA6v+PWk9U1S2qOqiqg7299gIMIaS5NBrsDwG4uf74ZgAPJuMOIaRZzEd6+y6AqwH0i8gwgM8D+AKAe0XkFgD7AVw/v8MpqtUzb5aUyYTfk6pVOxPq9LQtn5SLdoHCfXtfNW3PPvdccHvPcvsTy0uvvGTarvnIVabthw/db9o2bjjXtPX1hH1ZsbrfHNPT22vapk/bc1Uu2ZKdpW5Oz9jn5fx32K/rTz72VkHo/3j854+attGDw8HtbXk70+8nj/zYtD2/Y5dp+9DVV5q2iYmTpq3QZmf7Jcmcwa6qNxqmDyfsCyGkifAXdIREAoOdkEhgsBMSCQx2QiKBwU5IJKRacFKBhqQ31fAYL+ltanKyIdv+/XaW2rPPPBvcfvToEXNMb+9y0/byLjsT6uIL7cy21/YeMm3//m9fD/uxypbX3v/+95u2szfZctia1WtNm3mWxT7/HZ1tpu2mm28wbQL7fA49Ec5IXNptX/q/eOJnpq2nz86++9q/fM20nTxpF7i85po/Mm1Jwjs7IZHAYCckEhjshEQCg52QSGCwExIJDHZCIiHdXm+qZqaaiP2+UzD6nj333HZzzC9++UvTtsLJ8vKKUW7ctCG43cu+g9Nj7ZVXfm3aSjO2rnjKkXFOT4azynr7l5pjnn7SnquXdtlZe2vXrjdty3rCkuP6s+wim0dGwxlqAHD02GHTduzYiGnL5sJS36t77bkfGLDltZWrbP/3vmbLtieO20Uxf/WrJ0xbkvDOTkgkMNgJiQQGOyGRwGAnJBIY7IREQrqr8VBkNVzTrDRlt8AZ3v/WHhU1nvnFz80xo8P7TNvk8U7TprBX4zs7wuPO3miXzS8U7ClevrTLtO3a8aJpmxi3V+OXGYvuXXLaHHPOBjuh5bGnbMXjvvvvNW1rB8Kr1t58lEpOi6qKfX1MTdq2yky41typyaPmmLVr7GSdtkJYGQKAFSvtOn8bBuyEqOLMtGlLEt7ZCYkEBjshkcBgJyQSGOyERAKDnZBIYLATEgnzaf90F4CPARhV1Yvr2+4A8EkAb+gXt6vqw3Pt6/TEBHY88VjQll9iv+9MFcM1xnqW2ckine1rTNuyHjspZHzylGnr6gpLZcUZu32PNQYAsk7SzckTdkupwyOvmbZy6fXg9kPDe8wxa1fbiUFStds/ZcROAMpJWKI6dMCWvMbHx01bj9HWCgDGT02YtnIpLMst67br3R0/Zl9XwwcOmLaRo+F6dwBw8QX29djmyJFJMp87+90AQo22vqKql9b/zRnohJDWMmewq+qjAMK/aiGE/M6wkO/st4rICyJyl4iw8Tohi5xGg/1OAOcAuBTACIAvWU8Ukc0iMiQiQ+On7PrehJDm0lCwq+oRVa1orXvDNwBc7jx3i6oOqurg0m77N+mEkObSULCLyOylxU8A2JmMO4SQZjEf6e27AK4G0C8iwwA+D+BqEbkUtY5O+wB8aj4Hm5gYx2OPbgvaylk7Kyu/JCzjHD1y0hxTKduy1nsGLzVty7o6TJtkwpKM5uz3zA7DdwCYnrK/1lxwfrjeHQDksvZr+9UTTwW3S9b249ARWw6bcTKyljpztaIvLOdV7ZJ86O/pM21r1tqZecWSLb0dORqWKatlO7Mtm7Wlt77ldsuucsme46KT1WnJlEkzZ7Cr6o2Bzd9qgi+EkCbCX9AREgkMdkIigcFOSCQw2AmJBAY7IZGQasHJJZ2deOcHwr+/eWmX3Y5n1/awbc/uQ+aYbNbOahr+jZ1dtdTJhursCv8oKJezpzGXt209y2zpqmOJbZu23Uff0rBEJRk7M+/1E7aEOTBgt3jqXmbLUDOGHLZpk/3L6vVr7dZKXd129qB1XgCgOBOWWffs3m2OGX5tr2nLZQqm7ewB20ep2hlxRbWlviThnZ2QSGCwExIJDHZCIoHBTkgkMNgJiQQGOyGRIJrSsj8ArF7Vq39+4zVBW7Vop0P1Lg33yaqqXfCwVLb3d+K4LTWVi3a/sc6OsByWzYb7iQFAqWQXbCzN2MUt4fScy2bsLKmpqXCW2lTJzips61xi2k5P2pl5p8bteczmwxKmZG0pUp3z2d+/wrSJ2LLiqJEZ2d9ny3X5jH3Opk7Zr9k+Y4CInT14yrjkHnjkZWePNqoadIV3dkIigcFOSCQw2AmJBAY7IZHAYCckElJNhJmenMTup58OO9JmJxiMr14d3N7hrCIPrLfrmZ21Lrw/ANCqXZtsZia8bDp92l5pbV/Sbdq6u+0VZq3aa7unT9urxcMHR4Pbd758zBwzMRpuGQUApyftrJuZ03ZyR6Et/LrLFfuSm5yw5/Fwj23r6rTnePxk2P88VppjBgbC6g8AnHj9iGlrcxKiMk6kLWlbPO2fCCFvAxjshEQCg52QSGCwExIJDHZCIoHBTkgkzKf90wCAbwNYhVq7py2q+lUR6QXwfQAbUWsBdb2qHvf2lckAnR1haWt62k64+OVjQ8HtJyftZJcPf/D3TduV732HaZsp2gkjmUz4vXHJErtuXcZp1QQngSacylAj124fr6M7LEMdPmon3UxO2VJehyOJQuwkKpHwXIkz5vhJ+xo4Mmaflwsv2Gja+vvDSTKnp+2WUcWKXVsPjrymGfu8aNU+191dzvESZD539jKAz6rqhQCuAPBpEbkQwG0AtqnqeQC21f8mhCxS5gx2VR1R1Wfrj08BeBHAOgDXAbin/rR7AHy8WU4SQhbOGX1nF5GNAN4N4EkAq1R1pG46jNrHfELIImXewS4iXQDuA/AZVX3TbxC1VgEj+GVMRDaLyJCIDM2U7OIEhJDmMq9gF5E8aoH+HVW9v775iIisqdvXAAj+KFtVt6jqoKoOtuXtRQpCSHOZM9hFRFDrx/6iqn55lukhADfXH98M4MHk3SOEJMV80m0+AOAmADtEZHt92+0AvgDgXhG5BcB+ANfPtaNKBRgbD0sv2ZwjJxnSxPEJW07asXOfaVu5st+0ZZwPH21tYR87nVZNp4t2tlZH0flao/b7sIh92oZHwhLVoRE7e23aqf/Xt9xuaZRz/JguhuU8rzZgpmC/5oIjXWWX2OO6+8Ln5tBBOwuwmrPlwdUb7XZY06fscz0zbkt9bUvSkd7mDHZVfRx2Lb0PJ+sOIaRZ8Bd0hEQCg52QSGCwExIJDHZCIoHBTkgkpNr+KZ8RXZ4Pv7/k2uyWRrlCWDRwVBxkMnbhyIyTUZZxKgP29YVlqFLZPlalYjvZ2WXLjU7dS4gjoszMhI83fNguKjldLJm2/p6lpi0ntpNVDe+zvd32vb3Lbsl0bMyWDtessX+p/Z7LLglu37b1cXNMW7t9Lb7zXRtN2/p1toRWmbLn+MCB8Gu775Fwcda5YPsnQiKHwU5IJDDYCYkEBjshkcBgJyQSGOyEREKqvd7a2gs4/9xw1lDBkTtWregJbteSo085r2zkmC1DjR2zJZ7OjrCPM07WWC5rF2zUql1EcWLKzqDKZ+0ed3kJZ4dtWGf3vmt35r5vuS0ntRsyKgCcOB7uObdild3fbv2ms0zbbw6OmLZX9x4ybYf3Hw5uzziybXe7ncV4ye9dZNp67BZxyDsVRJcvPRnc3qj0ZsE7OyGRwGAnJBIY7IREAoOdkEhgsBMSCakmwmwaWKl3/E24VJ1m7eXzqZMngtsnx+xuU21GGyQAKCy166pNTdq1wkqlcF21nJM80+60hipO2cfyVvjzOXs1fmJiKrjdK+JdcKr+5jP29ZFRWw3JGq2yVq5eafvhzNV02W5RNXY8vJoNANOnw/OYzdgqSWeXrU4s77HnHiVnrpyEKM2Ej/dnf3WnfSwHJsIQEjkMdkIigcFOSCQw2AmJBAY7IZHAYCckEuZMhBGRAQDfRq0lswLYoqpfFZE7AHwSwNH6U29X1Ye9fWUEaM+HRaCyIwFWyuGEkVzBHpPLOTXoKnaSSV+nI5UVw8kMtXZ4YSrlGdPW1m7LP8s77WSMktMNV4xSZ8WSI+U5b/mZcHNeAMDklN1+y5LeULazRXJV+3LsytnyYPfqXtOmhv/qtNdSo34eAJQr9vmE2pIdnGskX3DGJch8st7KAD6rqs+KSDeAZ0Rka932FVX95+a5RwhJivn0ehsBMFJ/fEpEXgSwrtmOEUKS5Yy+s4vIRgDvBvBkfdOtIvKCiNwlIuGkc0LIomDewS4iXQDuA/AZVR0HcCeAcwBcitqd/0vGuM0iMiQiQ+MTdrEGQkhzmVewi0getUD/jqreDwCqekRVK6paBfANAJeHxqrqFlUdVNXBpV3O74oJIU1lzmCX2lLztwC8qKpfnrV9zaynfQLAzuTdI4QkxXxW4z8A4CYAO0Rke33b7QBuFJFLUZPj9gH41Ny7UoglhVRtuWNZb1iuEdjLBEUjQw0AytO2reRkm8GQB0uOrOVlFRartoRWyNuyXKHNlgc72tuD2/M5e36rTq+pSsW2LVliZw9WKuHjTU5MmmPgyHxZJysy48hyVq8vrdrnzJ8PJ7MtY88xjNqAAFApejmJyTGf1fjHAYRmzNXUCSGLC/6CjpBIYLATEgkMdkIigcFOSCQw2AmJhFTbP6kqikYGG4yiewAgQTEAqFSc7C+xJZJCu/2yi1P2r/yqlozmzGLWkH4AIFO15ZiOzrCEBgCFgiPLtYf36c1V2bFZciMAiJXZBsAyVdWWvMqwJVFV51xXnHuWMazqFMv0pLeMcS0CgJMsB3Gkw3LZkXsThHd2QiKBwU5IJDDYCYkEBjshkcBgJyQSGOyEREK60psAamgyGUeiKhlZQZ584mWN5bKO5GVkjQFAqRTWVryCk43avCyvqpMtl2sLv26vpGG5bO+v4hzLk/OyufB5bs/b81t19ufJYR6lcvicVZzX7J0XiH1/rHq93uw9YmraKWKZILyzExIJDHZCIoHBTkgkMNgJiQQGOyGRwGAnJBJSld4qlSrGTkwFbfmsLQ5ljYw4cWSQCUfO6OzsNG2iTpZaJiwbebJQPue8rkYzocQZZ/SBy2SdDLWsLVNCHMlLPMku7L9UnOKQ8ApHOm44mWg547rKZ+0x3vksOwU41ZnjUxP29Xhw9LhpSxLe2QmJBAY7IZHAYCckEhjshEQCg52QSBCvPREAiEg7gEcBtKG2ev8DVf28iGwC8D0AfQCeAXCTqtpFxGr78g9GCFkwqmFJaT539hkAH1LVS1Brz3ytiFwB4IsAvqKq5wI4DuCWpJwlhCTPnMGuNSbqf+br/xTAhwD8oL79HgAfb4qHhJBEmG9/9my9g+sogK0A9gI4ofrbusDDANY1x0VCSBLMK9hVtaKqlwJYD+ByABfM9wAisllEhkRkqEEfCSEJcEar8ap6AsDPALwPwHKR3/5ucz2Ag8aYLao6qKqDC/KUELIg5gx2EVkhIsvrj5cAuAbAi6gF/Z/Wn3YzgAeb5SQhZOHMR3p7F2oLcFnU3hzuVdV/FJGzUZPeegE8B+AvVNUtpkXpjZDmY0lvcwZ7kjDYCWk+C9HZCSFvAxjshEQCg52QSGCwExIJDHZCIiHVGnQAjgHYX3/cX/+71dCPN0M/3szvmh8bLEOq0tubDiwytBh+VUc/6EcsfvBjPCGRwGAnJBJaGexbWnjs2dCPN0M/3szbxo+WfWcnhKQLP8YTEgktCXYRuVZEXhaRPSJyWyt8qPuxT0R2iMj2NItriMhdIjIqIjtnbesVka0isrv+f0+L/LhDRA7W52S7iHw0BT8GRORnIvJrEdklIn9d357qnDh+pDonItIuIk+JyPN1P/6hvn2TiDxZj5vvi4jTtyuAqqb6D7VU2b0AzgZQAPA8gAvT9qPuyz4A/S047lUALgOwc9a2fwJwW/3xbQC+2CI/7gDwtynPxxoAl9UfdwN4BcCFac+J40eqcwJAAHTVH+cBPAngCgD3Arihvv3rAP7yTPbbijv75QD2qOqrWis9/T0A17XAj5ahqo8CGHvL5utQqxsApFTA0/AjdVR1RFWfrT8+hVpxlHVIeU4cP1JFayRe5LUVwb4OwIFZf7eyWKUC+ImIPCMim1vkwxusUtWR+uPDAFa10JdbReSF+sf8pn+dmI2IbATwbtTuZi2bk7f4AaQ8J80o8hr7At2VqnoZgD8G8GkRuarVDgG1d3bU3ohawZ0AzkGtR8AIgC+ldWAR6QJwH4DPqOr4bFuacxLwI/U50QUUebVoRbAfBDAw62+zWGWzUdWD9f9HATyA2qS2iiMisgYA6v+PtsIJVT1Sv9CqAL6BlOZERPKoBdh3VPX++ubU5yTkR6vmpH7sMy7yatGKYH8awHn1lcUCgBsAPJS2EyLSKSLdbzwG8BEAO/1RTeUh1Ap3Ai0s4PlGcNX5BFKYExERAN8C8KKqfnmWKdU5sfxIe06aVuQ1rRXGt6w2fhS1lc69AP6uRT6cjZoS8DyAXWn6AeC7qH0cLKH23esW1HrmbQOwG8BPAfS2yI//ALADwAuoBduaFPy4ErWP6C8A2F7/99G058TxI9U5AfAu1Iq4voDaG8vfz7pmnwKwB8B/AWg7k/3yF3SERELsC3SERAODnZBIYLATEgkMdkIigcFOSCQw2AmJBAY7IZHAYCckEv4XxWiVbfq9la4AAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Weeu3YVYwZ5T","executionInfo":{"status":"ok","timestamp":1620799438292,"user_tz":-540,"elapsed":57076,"user":{"displayName":"김효준","photoUrl":"","userId":"12098101409760390036"}},"outputId":"71995abf-b173-412b-b528-9f1190ce4654"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T5jt2VAIwc_H","executionInfo":{"status":"ok","timestamp":1620799442344,"user_tz":-540,"elapsed":613,"user":{"displayName":"김효준","photoUrl":"","userId":"12098101409760390036"}}},"source":["path = '/content/drive/MyDrive/4-1 실전기계학습/mid_project'\n","augmentation_method = '0512_1_random_change_0.2_0.2_0.3_2nd'"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"-JnpiaK3sl0w","executionInfo":{"status":"ok","timestamp":1620799444454,"user_tz":-540,"elapsed":583,"user":{"displayName":"김효준","photoUrl":"","userId":"12098101409760390036"}}},"source":["def load_model(model, path, method) :\n","\n","    dir_ckpt = pathlib.Path(path)\n","    dir_path = dir_ckpt / method / 'checkpoint'\n","    ckpt_file = dir_path / 'model_best.pth'\n","\n","    if cuda:\n","        checkpoint = torch.load(ckpt_file, map_location=lambda storage, loc: storage.cuda(0))\n","        try:\n","            model.load_state_dict(checkpoint['model'])\n","        except:\n","            model.module.load_state_dict(checkpoint['model'])\n","    else:\n","        checkpoint = torch.load(ckpt_file, map_location=lambda storage, loc: storage)\n","        try:\n","            model.load_state_dict(checkpoint['model'])\n","        except:\n","            # create new OrderedDict that does not contain `module.`\n","            new_state_dict = OrderedDict()\n","            for k, v in checkpoint['model'].items():\n","                if k[:7] == 'module.':\n","                    name = k[7:] # remove `module.`\n","                else:\n","                    name = k[:]\n","                new_state_dict[name] = v\n","\n","            model.load_state_dict(new_state_dict)\n","\n","    return checkpoint"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"A4VAeJxUsnPL","executionInfo":{"status":"ok","timestamp":1620799445743,"user_tz":-540,"elapsed":752,"user":{"displayName":"김효준","photoUrl":"","userId":"12098101409760390036"}}},"source":["import pathlib\n","\n","def save_model(state, path, method):\n","    dir_ckpt = pathlib.Path(path)\n","    dir_path = dir_ckpt / method / 'checkpoint'\n","    dir_path.mkdir(parents=True, exist_ok=True)\n","\n","    model_file = dir_path / 'model_best.pth'\n","    \n","    torch.save(state, model_file)\n","\n","def save_values(epoch, acc1_train, acc5_train, acc1_valid, acc5_valid, path, method):\n","    dir_ckpt = pathlib.Path(path)\n","    dir_path = dir_ckpt / method / 'values'\n","    dir_path.mkdir(parents=True, exist_ok=True)\n","\n","    acc1_train_file = dir_path / 'acc1_train.txt'\n","    acc5_train_file = dir_path / 'acc5_train.txt'\n","\n","    acc1_valid_file = dir_path / 'acc1_valid.txt'\n","    acc5_valid_file = dir_path / 'acc5_valid.txt'\n","\n","    with open(acc1_train_file, \"a\") as f :\n","        f.write(epoch + \", \" + acc1_train + '\\n')\n","\n","    with open(acc5_train_file, \"a\") as f :\n","        f.write(epoch + \", \" + acc5_train + '\\n')\n","\n","    with open(acc1_valid_file, \"a\") as f :\n","        f.write(epoch + \", \" + acc1_valid + '\\n')\n","\n","    with open(acc5_valid_file, \"a\") as f :\n","        f.write(epoch + \", \" + acc5_valid + '\\n')"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gITLQIAr9lAZ"},"source":["##**Main Training**"]},{"cell_type":"code","metadata":{"id":"s0-lYvAp9oHA","executionInfo":{"status":"ok","timestamp":1620799447496,"user_tz":-540,"elapsed":1130,"user":{"displayName":"김효준","photoUrl":"","userId":"12098101409760390036"}}},"source":["def train(train_loader, epoch, model, optimizer, criterion):\n","    batch_time = AverageMeter('Time', ':6.3f')\n","    losses = AverageMeter('Loss', ':.4e')\n","    top1 = AverageMeter('Acc@1', ':6.2f')\n","    top5 = AverageMeter('Acc@5', ':6.2f')\n","    progress = ProgressMeter(len(train_loader), batch_time, losses,\n","                             top1, top5, prefix=\"Epoch: [{}]\".format(epoch))\n","    # switch to train mode\n","    model.train()\n","\n","    end = time.time()\n","    for i, (input, target) in enumerate(train_loader):\n","        # measure data loading time\n","        input = input.cuda()\n","        target = target.cuda()\n","\n","        # compute output\n","        output = model(input)\n","        loss = criterion(output, target)\n","\n","        # measure accuracy and record loss, accuracy \n","        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n","        losses.update(loss.item(), input.size(0))\n","        top1.update(acc1[0].item(), input.size(0))\n","        top5.update(acc5[0].item(), input.size(0))\n","\n","        # compute gradient and do SGD step\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        if i % print_freq == 0:\n","            progress.print(i)\n","\n","    print('==> Train Accuracy: Acc@1 {top1.avg:.3f} || Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n","\n","    return top1.avg, top5.avg\n","\n","def test(test_loader,epoch, model):\n","    top1 = AverageMeter('Acc@1', ':6.2f')\n","    top5 = AverageMeter('Acc@5', ':6.2f')\n","    model.eval()\n","    for i,(input,target) in enumerate(test_loader):\n","        input = input.cuda()\n","        target = target.cuda()\n","\n","        output = model(input)\n","        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n","        top1.update(acc1[0].item(), input.size(0))\n","        top5.update(acc5[0].item(), input.size(0))\n","    print('==> Test Accuracy:  Acc@1 {top1.avg:.3f} || Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n","    return top1.avg, top5.avg"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"qvdAvmx1UkyG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620805427091,"user_tz":-540,"elapsed":5978885,"user":{"displayName":"김효준","photoUrl":"","userId":"12098101409760390036"}},"outputId":"022135bf-99eb-444d-bb52-3bfe58c097c4"},"source":["model = ResNet34(num_classes=num_classes).cuda()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9, nesterov=True, weight_decay=5e-4)\n","\n","scheduler = MultiStepLR(optimizer, milestones=[60, 90, 120], gamma=0.2)\n","\n","criterion = torch.nn.CrossEntropyLoss().cuda()\n","###########################################################\n","best_acc = 0\n","for epoch in range(epochs):\n","    print(\"\\n----- epoch: {}, lr: {} -----\".format(\n","        epoch, optimizer.param_groups[0][\"lr\"]))\n","\n","    # train for one epoch\n","    start_time = time.time()\n","    train_acc1, train_acc5 = train(train_loader, epoch, model, optimizer, criterion)\n","    test_acc1, test_acc5 = test(test_loader,epoch,model)\n","\n","    elapsed_time = time.time() - start_time\n","    print('==> {:.2f} seconds to train this epoch\\n'.format(elapsed_time))\n","    # learning rate scheduling\n","    scheduler.step()\n","    \n","    # Save model for best accuracy\n","    if best_acc < test_acc1:\n","        best_acc = test_acc1\n","        state = {'epoch': epoch + 1,\n","                 'model': model.state_dict(),\n","                 'optimizer': optimizer.state_dict(),\n","                 'scheduler' : scheduler.state_dict()\n","                 }\n","        save_model(state, path, augmentation_method)\n","    save_values(str(epoch + 1), str(train_acc1), str(train_acc5), str(test_acc1), str(test_acc5), path, augmentation_method)\n","        \n","torch.save(model.state_dict(),'model_latest1.pt')\n","print(f\"Best Top-1 Accuracy: {best_acc}\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["\n","----- epoch: 0, lr: 0.1 -----\n","Epoch: [0][  0/391]\tTime  1.008 ( 1.008)\tLoss 4.7583e+00 (4.7583e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   6.25 (  6.25)\n","Epoch: [0][ 30/391]\tTime  0.089 ( 0.121)\tLoss 4.6713e+00 (5.3942e+00)\tAcc@1   0.00 (  1.21)\tAcc@5   4.69 (  5.32)\n","Epoch: [0][ 60/391]\tTime  0.093 ( 0.108)\tLoss 4.5474e+00 (5.0181e+00)\tAcc@1   1.56 (  1.32)\tAcc@5   6.25 (  5.55)\n","Epoch: [0][ 90/391]\tTime  0.092 ( 0.103)\tLoss 4.5336e+00 (4.8685e+00)\tAcc@1   2.34 (  1.66)\tAcc@5   9.38 (  6.72)\n","Epoch: [0][120/391]\tTime  0.090 ( 0.101)\tLoss 4.3131e+00 (4.7622e+00)\tAcc@1   6.25 (  2.01)\tAcc@5  16.41 (  8.21)\n","Epoch: [0][150/391]\tTime  0.092 ( 0.099)\tLoss 4.2738e+00 (4.6674e+00)\tAcc@1   4.69 (  2.57)\tAcc@5  15.62 (  9.88)\n","Epoch: [0][180/391]\tTime  0.093 ( 0.098)\tLoss 4.2092e+00 (4.5926e+00)\tAcc@1   3.12 (  2.87)\tAcc@5  14.06 ( 11.11)\n","Epoch: [0][210/391]\tTime  0.092 ( 0.097)\tLoss 4.1620e+00 (4.5286e+00)\tAcc@1   5.47 (  3.23)\tAcc@5  14.06 ( 12.34)\n","Epoch: [0][240/391]\tTime  0.089 ( 0.097)\tLoss 4.1529e+00 (4.4746e+00)\tAcc@1   6.25 (  3.54)\tAcc@5  19.53 ( 13.46)\n","Epoch: [0][270/391]\tTime  0.099 ( 0.096)\tLoss 4.0919e+00 (4.4286e+00)\tAcc@1   4.69 (  3.85)\tAcc@5  20.31 ( 14.51)\n","Epoch: [0][300/391]\tTime  0.095 ( 0.096)\tLoss 3.9893e+00 (4.3873e+00)\tAcc@1   9.38 (  4.18)\tAcc@5  30.47 ( 15.60)\n","Epoch: [0][330/391]\tTime  0.094 ( 0.096)\tLoss 4.1845e+00 (4.3538e+00)\tAcc@1   3.91 (  4.44)\tAcc@5  23.44 ( 16.45)\n","Epoch: [0][360/391]\tTime  0.093 ( 0.096)\tLoss 4.0103e+00 (4.3210e+00)\tAcc@1   7.03 (  4.70)\tAcc@5  23.44 ( 17.30)\n","Epoch: [0][390/391]\tTime  0.553 ( 0.097)\tLoss 4.1048e+00 (4.2948e+00)\tAcc@1   8.75 (  4.90)\tAcc@5  25.00 ( 18.01)\n","==> Train Accuracy: Acc@1 4.902 || Acc@5 18.014\n","==> Test Accuracy:  Acc@1 9.100 || Acc@5 29.040\n","==> 41.04 seconds to train this epoch\n","\n","\n","----- epoch: 1, lr: 0.1 -----\n","Epoch: [1][  0/391]\tTime  0.271 ( 0.271)\tLoss 3.9655e+00 (3.9655e+00)\tAcc@1   8.59 (  8.59)\tAcc@5  26.56 ( 26.56)\n","Epoch: [1][ 30/391]\tTime  0.093 ( 0.100)\tLoss 3.8339e+00 (3.8931e+00)\tAcc@1   9.38 (  8.92)\tAcc@5  28.91 ( 29.69)\n","Epoch: [1][ 60/391]\tTime  0.088 ( 0.097)\tLoss 3.8158e+00 (3.8723e+00)\tAcc@1   7.81 (  8.84)\tAcc@5  31.25 ( 29.52)\n","Epoch: [1][ 90/391]\tTime  0.093 ( 0.098)\tLoss 3.8248e+00 (3.8667e+00)\tAcc@1   9.38 (  8.72)\tAcc@5  34.38 ( 29.67)\n","Epoch: [1][120/391]\tTime  0.093 ( 0.097)\tLoss 3.8478e+00 (3.8562e+00)\tAcc@1  10.94 (  8.80)\tAcc@5  31.25 ( 30.19)\n","Epoch: [1][150/391]\tTime  0.094 ( 0.096)\tLoss 3.6449e+00 (3.8490e+00)\tAcc@1  10.16 (  8.90)\tAcc@5  32.81 ( 30.46)\n","Epoch: [1][180/391]\tTime  0.090 ( 0.096)\tLoss 3.7169e+00 (3.8420e+00)\tAcc@1  14.06 (  9.01)\tAcc@5  32.81 ( 30.65)\n","Epoch: [1][210/391]\tTime  0.092 ( 0.095)\tLoss 3.5955e+00 (3.8290e+00)\tAcc@1  13.28 (  9.20)\tAcc@5  36.72 ( 31.08)\n","Epoch: [1][240/391]\tTime  0.090 ( 0.095)\tLoss 3.6329e+00 (3.8175e+00)\tAcc@1  11.72 (  9.54)\tAcc@5  42.19 ( 31.53)\n","Epoch: [1][270/391]\tTime  0.096 ( 0.095)\tLoss 3.6109e+00 (3.8016e+00)\tAcc@1  12.50 (  9.79)\tAcc@5  41.41 ( 32.06)\n","Epoch: [1][300/391]\tTime  0.091 ( 0.095)\tLoss 3.6577e+00 (3.7878e+00)\tAcc@1  11.72 ( 10.02)\tAcc@5  36.72 ( 32.40)\n","Epoch: [1][330/391]\tTime  0.096 ( 0.094)\tLoss 3.6450e+00 (3.7763e+00)\tAcc@1  13.28 ( 10.28)\tAcc@5  37.50 ( 32.88)\n","Epoch: [1][360/391]\tTime  0.093 ( 0.094)\tLoss 3.7215e+00 (3.7653e+00)\tAcc@1  13.28 ( 10.47)\tAcc@5  35.16 ( 33.22)\n","Epoch: [1][390/391]\tTime  0.082 ( 0.094)\tLoss 3.7310e+00 (3.7505e+00)\tAcc@1  13.75 ( 10.76)\tAcc@5  30.00 ( 33.71)\n","==> Train Accuracy: Acc@1 10.756 || Acc@5 33.712\n","==> Test Accuracy:  Acc@1 16.210 || Acc@5 42.290\n","==> 40.00 seconds to train this epoch\n","\n","\n","----- epoch: 2, lr: 0.1 -----\n","Epoch: [2][  0/391]\tTime  0.306 ( 0.306)\tLoss 3.4134e+00 (3.4134e+00)\tAcc@1  19.53 ( 19.53)\tAcc@5  50.00 ( 50.00)\n","Epoch: [2][ 30/391]\tTime  0.093 ( 0.100)\tLoss 3.5454e+00 (3.5134e+00)\tAcc@1  10.16 ( 15.15)\tAcc@5  38.28 ( 41.23)\n","Epoch: [2][ 60/391]\tTime  0.090 ( 0.097)\tLoss 3.2367e+00 (3.4984e+00)\tAcc@1  21.88 ( 15.22)\tAcc@5  46.09 ( 41.03)\n","Epoch: [2][ 90/391]\tTime  0.092 ( 0.095)\tLoss 3.7239e+00 (3.4900e+00)\tAcc@1  11.72 ( 15.53)\tAcc@5  34.38 ( 41.36)\n","Epoch: [2][120/391]\tTime  0.108 ( 0.098)\tLoss 3.5942e+00 (3.4746e+00)\tAcc@1  13.28 ( 15.90)\tAcc@5  39.84 ( 41.88)\n","Epoch: [2][150/391]\tTime  0.088 ( 0.097)\tLoss 3.2539e+00 (3.4593e+00)\tAcc@1  16.41 ( 16.10)\tAcc@5  51.56 ( 42.54)\n","Epoch: [2][180/391]\tTime  0.088 ( 0.096)\tLoss 3.4103e+00 (3.4487e+00)\tAcc@1  16.41 ( 16.24)\tAcc@5  44.53 ( 43.02)\n","Epoch: [2][210/391]\tTime  0.092 ( 0.096)\tLoss 3.4356e+00 (3.4389e+00)\tAcc@1  17.97 ( 16.42)\tAcc@5  46.09 ( 43.34)\n","Epoch: [2][240/391]\tTime  0.091 ( 0.096)\tLoss 3.2917e+00 (3.4233e+00)\tAcc@1  17.19 ( 16.65)\tAcc@5  43.75 ( 43.87)\n","Epoch: [2][270/391]\tTime  0.096 ( 0.095)\tLoss 3.1359e+00 (3.4049e+00)\tAcc@1  21.09 ( 16.94)\tAcc@5  50.78 ( 44.38)\n","Epoch: [2][300/391]\tTime  0.092 ( 0.095)\tLoss 3.2342e+00 (3.3915e+00)\tAcc@1  17.97 ( 17.26)\tAcc@5  50.78 ( 44.77)\n","Epoch: [2][330/391]\tTime  0.095 ( 0.095)\tLoss 3.3006e+00 (3.3791e+00)\tAcc@1  25.78 ( 17.57)\tAcc@5  54.69 ( 45.17)\n","Epoch: [2][360/391]\tTime  0.098 ( 0.095)\tLoss 3.1209e+00 (3.3678e+00)\tAcc@1  25.00 ( 17.75)\tAcc@5  53.12 ( 45.49)\n","Epoch: [2][390/391]\tTime  0.083 ( 0.095)\tLoss 3.1086e+00 (3.3553e+00)\tAcc@1  26.25 ( 17.96)\tAcc@5  52.50 ( 45.85)\n","==> Train Accuracy: Acc@1 17.962 || Acc@5 45.846\n","==> Test Accuracy:  Acc@1 20.690 || Acc@5 50.670\n","==> 40.09 seconds to train this epoch\n","\n","\n","----- epoch: 3, lr: 0.1 -----\n","Epoch: [3][  0/391]\tTime  0.305 ( 0.305)\tLoss 3.0257e+00 (3.0257e+00)\tAcc@1  23.44 ( 23.44)\tAcc@5  50.78 ( 50.78)\n","Epoch: [3][ 30/391]\tTime  0.089 ( 0.100)\tLoss 3.0303e+00 (3.1475e+00)\tAcc@1  24.22 ( 21.67)\tAcc@5  53.12 ( 51.86)\n","Epoch: [3][ 60/391]\tTime  0.111 ( 0.097)\tLoss 3.1623e+00 (3.1474e+00)\tAcc@1  20.31 ( 22.08)\tAcc@5  50.78 ( 52.05)\n","Epoch: [3][ 90/391]\tTime  0.101 ( 0.099)\tLoss 3.1104e+00 (3.1281e+00)\tAcc@1  21.09 ( 22.54)\tAcc@5  52.34 ( 52.36)\n","Epoch: [3][120/391]\tTime  0.092 ( 0.098)\tLoss 3.3863e+00 (3.1291e+00)\tAcc@1  18.75 ( 22.29)\tAcc@5  42.19 ( 52.22)\n","Epoch: [3][150/391]\tTime  0.091 ( 0.097)\tLoss 3.0307e+00 (3.1233e+00)\tAcc@1  22.66 ( 22.36)\tAcc@5  53.12 ( 52.29)\n","Epoch: [3][180/391]\tTime  0.094 ( 0.096)\tLoss 3.1770e+00 (3.1116e+00)\tAcc@1  22.66 ( 22.54)\tAcc@5  50.00 ( 52.58)\n","Epoch: [3][210/391]\tTime  0.099 ( 0.096)\tLoss 3.1857e+00 (3.1078e+00)\tAcc@1  21.09 ( 22.74)\tAcc@5  48.44 ( 52.78)\n","Epoch: [3][240/391]\tTime  0.098 ( 0.095)\tLoss 3.2183e+00 (3.0980e+00)\tAcc@1  21.88 ( 22.84)\tAcc@5  53.12 ( 53.08)\n","Epoch: [3][270/391]\tTime  0.090 ( 0.095)\tLoss 3.1474e+00 (3.0868e+00)\tAcc@1  21.09 ( 23.05)\tAcc@5  55.47 ( 53.34)\n","Epoch: [3][300/391]\tTime  0.089 ( 0.095)\tLoss 2.9078e+00 (3.0770e+00)\tAcc@1  25.00 ( 23.27)\tAcc@5  58.59 ( 53.51)\n","Epoch: [3][330/391]\tTime  0.092 ( 0.095)\tLoss 2.9227e+00 (3.0647e+00)\tAcc@1  30.47 ( 23.50)\tAcc@5  57.03 ( 53.83)\n","Epoch: [3][360/391]\tTime  0.091 ( 0.095)\tLoss 2.9828e+00 (3.0582e+00)\tAcc@1  28.91 ( 23.58)\tAcc@5  57.81 ( 53.95)\n","Epoch: [3][390/391]\tTime  0.081 ( 0.094)\tLoss 2.8562e+00 (3.0462e+00)\tAcc@1  26.25 ( 23.78)\tAcc@5  63.75 ( 54.28)\n","==> Train Accuracy: Acc@1 23.778 || Acc@5 54.278\n","==> Test Accuracy:  Acc@1 25.530 || Acc@5 55.860\n","==> 40.01 seconds to train this epoch\n","\n","\n","----- epoch: 4, lr: 0.1 -----\n","Epoch: [4][  0/391]\tTime  0.309 ( 0.309)\tLoss 2.7923e+00 (2.7923e+00)\tAcc@1  26.56 ( 26.56)\tAcc@5  64.06 ( 64.06)\n","Epoch: [4][ 30/391]\tTime  0.091 ( 0.100)\tLoss 2.9793e+00 (2.8586e+00)\tAcc@1  27.34 ( 27.60)\tAcc@5  55.47 ( 59.73)\n","Epoch: [4][ 60/391]\tTime  0.091 ( 0.097)\tLoss 2.9302e+00 (2.8598e+00)\tAcc@1  26.56 ( 27.11)\tAcc@5  56.25 ( 59.14)\n","Epoch: [4][ 90/391]\tTime  0.092 ( 0.096)\tLoss 2.6669e+00 (2.8469e+00)\tAcc@1  31.25 ( 27.46)\tAcc@5  64.84 ( 59.44)\n","Epoch: [4][120/391]\tTime  0.107 ( 0.098)\tLoss 2.8009e+00 (2.8334e+00)\tAcc@1  28.12 ( 27.93)\tAcc@5  57.81 ( 59.59)\n","Epoch: [4][150/391]\tTime  0.094 ( 0.097)\tLoss 2.7793e+00 (2.8277e+00)\tAcc@1  32.03 ( 28.06)\tAcc@5  61.72 ( 59.92)\n","Epoch: [4][180/391]\tTime  0.094 ( 0.096)\tLoss 3.0183e+00 (2.8209e+00)\tAcc@1  25.78 ( 28.13)\tAcc@5  50.78 ( 59.96)\n","Epoch: [4][210/391]\tTime  0.090 ( 0.096)\tLoss 2.7634e+00 (2.8145e+00)\tAcc@1  32.03 ( 28.30)\tAcc@5  60.16 ( 60.05)\n","Epoch: [4][240/391]\tTime  0.090 ( 0.095)\tLoss 2.7994e+00 (2.8093e+00)\tAcc@1  28.91 ( 28.43)\tAcc@5  65.62 ( 60.12)\n","Epoch: [4][270/391]\tTime  0.089 ( 0.095)\tLoss 2.8163e+00 (2.8016e+00)\tAcc@1  27.34 ( 28.54)\tAcc@5  60.16 ( 60.27)\n","Epoch: [4][300/391]\tTime  0.090 ( 0.095)\tLoss 2.9279e+00 (2.7977e+00)\tAcc@1  25.78 ( 28.69)\tAcc@5  58.59 ( 60.41)\n","Epoch: [4][330/391]\tTime  0.089 ( 0.095)\tLoss 2.5094e+00 (2.7902e+00)\tAcc@1  38.28 ( 28.92)\tAcc@5  64.06 ( 60.65)\n","Epoch: [4][360/391]\tTime  0.092 ( 0.095)\tLoss 2.9395e+00 (2.7873e+00)\tAcc@1  25.00 ( 29.04)\tAcc@5  57.81 ( 60.67)\n","Epoch: [4][390/391]\tTime  0.081 ( 0.094)\tLoss 2.6782e+00 (2.7797e+00)\tAcc@1  31.25 ( 29.19)\tAcc@5  62.50 ( 60.84)\n","==> Train Accuracy: Acc@1 29.188 || Acc@5 60.838\n","==> Test Accuracy:  Acc@1 31.830 || Acc@5 63.550\n","==> 39.98 seconds to train this epoch\n","\n","\n","----- epoch: 5, lr: 0.1 -----\n","Epoch: [5][  0/391]\tTime  0.295 ( 0.295)\tLoss 2.5293e+00 (2.5293e+00)\tAcc@1  33.59 ( 33.59)\tAcc@5  65.62 ( 65.62)\n","Epoch: [5][ 30/391]\tTime  0.095 ( 0.100)\tLoss 2.3221e+00 (2.5805e+00)\tAcc@1  39.06 ( 33.64)\tAcc@5  71.09 ( 65.57)\n","Epoch: [5][ 60/391]\tTime  0.092 ( 0.097)\tLoss 2.4768e+00 (2.5714e+00)\tAcc@1  35.16 ( 33.48)\tAcc@5  69.53 ( 65.65)\n","Epoch: [5][ 90/391]\tTime  0.093 ( 0.095)\tLoss 2.6811e+00 (2.5776e+00)\tAcc@1  28.12 ( 33.27)\tAcc@5  63.28 ( 65.57)\n","Epoch: [5][120/391]\tTime  0.080 ( 0.097)\tLoss 2.8443e+00 (2.5824e+00)\tAcc@1  26.56 ( 33.16)\tAcc@5  57.81 ( 65.66)\n","Epoch: [5][150/391]\tTime  0.090 ( 0.096)\tLoss 2.3365e+00 (2.5762e+00)\tAcc@1  39.84 ( 33.50)\tAcc@5  73.44 ( 65.83)\n","Epoch: [5][180/391]\tTime  0.091 ( 0.096)\tLoss 2.5667e+00 (2.5676e+00)\tAcc@1  37.50 ( 33.52)\tAcc@5  65.62 ( 66.18)\n","Epoch: [5][210/391]\tTime  0.090 ( 0.095)\tLoss 2.5551e+00 (2.5629e+00)\tAcc@1  34.38 ( 33.63)\tAcc@5  62.50 ( 66.17)\n","Epoch: [5][240/391]\tTime  0.107 ( 0.095)\tLoss 2.4763e+00 (2.5606e+00)\tAcc@1  38.28 ( 33.67)\tAcc@5  69.53 ( 66.18)\n","Epoch: [5][270/391]\tTime  0.092 ( 0.095)\tLoss 2.4945e+00 (2.5579e+00)\tAcc@1  36.72 ( 33.76)\tAcc@5  64.84 ( 66.33)\n","Epoch: [5][300/391]\tTime  0.092 ( 0.094)\tLoss 2.4333e+00 (2.5498e+00)\tAcc@1  35.16 ( 33.80)\tAcc@5  71.88 ( 66.53)\n","Epoch: [5][330/391]\tTime  0.092 ( 0.094)\tLoss 2.6253e+00 (2.5423e+00)\tAcc@1  32.03 ( 34.03)\tAcc@5  60.94 ( 66.73)\n","Epoch: [5][360/391]\tTime  0.104 ( 0.094)\tLoss 2.3793e+00 (2.5360e+00)\tAcc@1  40.62 ( 34.22)\tAcc@5  69.53 ( 66.80)\n","Epoch: [5][390/391]\tTime  0.081 ( 0.094)\tLoss 2.7912e+00 (2.5275e+00)\tAcc@1  28.75 ( 34.46)\tAcc@5  57.50 ( 67.01)\n","==> Train Accuracy: Acc@1 34.462 || Acc@5 67.006\n","==> Test Accuracy:  Acc@1 36.860 || Acc@5 69.670\n","==> 39.94 seconds to train this epoch\n","\n","\n","----- epoch: 6, lr: 0.1 -----\n","Epoch: [6][  0/391]\tTime  0.309 ( 0.309)\tLoss 2.4548e+00 (2.4548e+00)\tAcc@1  34.38 ( 34.38)\tAcc@5  66.41 ( 66.41)\n","Epoch: [6][ 30/391]\tTime  0.089 ( 0.100)\tLoss 2.3052e+00 (2.3664e+00)\tAcc@1  39.84 ( 37.73)\tAcc@5  70.31 ( 69.91)\n","Epoch: [6][ 60/391]\tTime  0.095 ( 0.096)\tLoss 2.2246e+00 (2.3533e+00)\tAcc@1  38.28 ( 37.90)\tAcc@5  75.00 ( 70.44)\n","Epoch: [6][ 90/391]\tTime  0.093 ( 0.095)\tLoss 2.4862e+00 (2.3618e+00)\tAcc@1  38.28 ( 37.77)\tAcc@5  71.88 ( 70.51)\n","Epoch: [6][120/391]\tTime  0.114 ( 0.097)\tLoss 2.2585e+00 (2.3535e+00)\tAcc@1  39.06 ( 37.83)\tAcc@5  75.00 ( 70.67)\n","Epoch: [6][150/391]\tTime  0.090 ( 0.096)\tLoss 2.2587e+00 (2.3421e+00)\tAcc@1  39.84 ( 38.19)\tAcc@5  75.00 ( 70.96)\n","Epoch: [6][180/391]\tTime  0.092 ( 0.096)\tLoss 2.3141e+00 (2.3318e+00)\tAcc@1  39.06 ( 38.37)\tAcc@5  67.19 ( 71.09)\n","Epoch: [6][210/391]\tTime  0.092 ( 0.095)\tLoss 2.4159e+00 (2.3305e+00)\tAcc@1  33.59 ( 38.43)\tAcc@5  69.53 ( 71.14)\n","Epoch: [6][240/391]\tTime  0.094 ( 0.095)\tLoss 2.1098e+00 (2.3300e+00)\tAcc@1  42.97 ( 38.41)\tAcc@5  75.78 ( 71.08)\n","Epoch: [6][270/391]\tTime  0.090 ( 0.095)\tLoss 2.1453e+00 (2.3235e+00)\tAcc@1  42.97 ( 38.50)\tAcc@5  73.44 ( 71.19)\n","Epoch: [6][300/391]\tTime  0.100 ( 0.095)\tLoss 2.2436e+00 (2.3174e+00)\tAcc@1  36.72 ( 38.64)\tAcc@5  71.09 ( 71.25)\n","Epoch: [6][330/391]\tTime  0.093 ( 0.095)\tLoss 2.4535e+00 (2.3136e+00)\tAcc@1  33.59 ( 38.73)\tAcc@5  64.06 ( 71.33)\n","Epoch: [6][360/391]\tTime  0.101 ( 0.095)\tLoss 2.3980e+00 (2.3090e+00)\tAcc@1  39.84 ( 38.80)\tAcc@5  70.31 ( 71.43)\n","Epoch: [6][390/391]\tTime  0.083 ( 0.094)\tLoss 2.3167e+00 (2.3016e+00)\tAcc@1  41.25 ( 39.03)\tAcc@5  65.00 ( 71.56)\n","==> Train Accuracy: Acc@1 39.026 || Acc@5 71.562\n","==> Test Accuracy:  Acc@1 37.850 || Acc@5 70.420\n","==> 39.96 seconds to train this epoch\n","\n","\n","----- epoch: 7, lr: 0.1 -----\n","Epoch: [7][  0/391]\tTime  0.308 ( 0.308)\tLoss 2.0571e+00 (2.0571e+00)\tAcc@1  44.53 ( 44.53)\tAcc@5  79.69 ( 79.69)\n","Epoch: [7][ 30/391]\tTime  0.099 ( 0.100)\tLoss 2.3530e+00 (2.1062e+00)\tAcc@1  37.50 ( 43.02)\tAcc@5  74.22 ( 76.31)\n","Epoch: [7][ 60/391]\tTime  0.093 ( 0.096)\tLoss 2.3711e+00 (2.1041e+00)\tAcc@1  39.06 ( 42.90)\tAcc@5  71.88 ( 76.11)\n","Epoch: [7][ 90/391]\tTime  0.090 ( 0.095)\tLoss 2.0221e+00 (2.1226e+00)\tAcc@1  44.53 ( 42.80)\tAcc@5  77.34 ( 75.61)\n","Epoch: [7][120/391]\tTime  0.100 ( 0.097)\tLoss 2.1750e+00 (2.1188e+00)\tAcc@1  50.78 ( 42.82)\tAcc@5  78.12 ( 75.66)\n","Epoch: [7][150/391]\tTime  0.090 ( 0.096)\tLoss 2.3556e+00 (2.1175e+00)\tAcc@1  30.47 ( 42.73)\tAcc@5  72.66 ( 75.65)\n","Epoch: [7][180/391]\tTime  0.092 ( 0.096)\tLoss 2.0309e+00 (2.1105e+00)\tAcc@1  42.97 ( 42.98)\tAcc@5  76.56 ( 75.68)\n","Epoch: [7][210/391]\tTime  0.094 ( 0.095)\tLoss 2.0827e+00 (2.1114e+00)\tAcc@1  42.97 ( 43.07)\tAcc@5  80.47 ( 75.70)\n","Epoch: [7][240/391]\tTime  0.094 ( 0.095)\tLoss 2.3171e+00 (2.1079e+00)\tAcc@1  39.06 ( 43.20)\tAcc@5  71.88 ( 75.82)\n","Epoch: [7][270/391]\tTime  0.081 ( 0.095)\tLoss 2.0440e+00 (2.1094e+00)\tAcc@1  45.31 ( 43.21)\tAcc@5  78.12 ( 75.82)\n","Epoch: [7][300/391]\tTime  0.104 ( 0.094)\tLoss 2.1243e+00 (2.1134e+00)\tAcc@1  39.84 ( 43.21)\tAcc@5  73.44 ( 75.65)\n","Epoch: [7][330/391]\tTime  0.092 ( 0.094)\tLoss 1.9222e+00 (2.1134e+00)\tAcc@1  47.66 ( 43.16)\tAcc@5  82.81 ( 75.73)\n","Epoch: [7][360/391]\tTime  0.093 ( 0.094)\tLoss 2.1158e+00 (2.1122e+00)\tAcc@1  45.31 ( 43.18)\tAcc@5  77.34 ( 75.77)\n","Epoch: [7][390/391]\tTime  0.082 ( 0.094)\tLoss 1.8646e+00 (2.1062e+00)\tAcc@1  50.00 ( 43.29)\tAcc@5  81.25 ( 75.94)\n","==> Train Accuracy: Acc@1 43.286 || Acc@5 75.938\n","==> Test Accuracy:  Acc@1 40.600 || Acc@5 71.790\n","==> 39.85 seconds to train this epoch\n","\n","\n","----- epoch: 8, lr: 0.1 -----\n","Epoch: [8][  0/391]\tTime  0.312 ( 0.312)\tLoss 1.9822e+00 (1.9822e+00)\tAcc@1  39.06 ( 39.06)\tAcc@5  74.22 ( 74.22)\n","Epoch: [8][ 30/391]\tTime  0.096 ( 0.101)\tLoss 2.0827e+00 (1.9447e+00)\tAcc@1  45.31 ( 45.89)\tAcc@5  74.22 ( 78.73)\n","Epoch: [8][ 60/391]\tTime  0.094 ( 0.097)\tLoss 1.9061e+00 (1.9345e+00)\tAcc@1  48.44 ( 46.59)\tAcc@5  81.25 ( 79.14)\n","Epoch: [8][ 90/391]\tTime  0.094 ( 0.095)\tLoss 1.7052e+00 (1.9151e+00)\tAcc@1  53.12 ( 47.18)\tAcc@5  82.81 ( 79.61)\n","Epoch: [8][120/391]\tTime  0.107 ( 0.097)\tLoss 1.9558e+00 (1.9265e+00)\tAcc@1  39.84 ( 47.00)\tAcc@5  80.47 ( 79.35)\n","Epoch: [8][150/391]\tTime  0.100 ( 0.096)\tLoss 1.9180e+00 (1.9288e+00)\tAcc@1  47.66 ( 47.01)\tAcc@5  76.56 ( 79.18)\n","Epoch: [8][180/391]\tTime  0.096 ( 0.096)\tLoss 1.8582e+00 (1.9227e+00)\tAcc@1  50.78 ( 47.19)\tAcc@5  76.56 ( 79.37)\n","Epoch: [8][210/391]\tTime  0.094 ( 0.095)\tLoss 2.0130e+00 (1.9300e+00)\tAcc@1  46.09 ( 47.06)\tAcc@5  77.34 ( 79.14)\n","Epoch: [8][240/391]\tTime  0.092 ( 0.095)\tLoss 1.8749e+00 (1.9279e+00)\tAcc@1  47.66 ( 47.18)\tAcc@5  77.34 ( 79.15)\n","Epoch: [8][270/391]\tTime  0.090 ( 0.095)\tLoss 2.2193e+00 (1.9340e+00)\tAcc@1  41.41 ( 46.96)\tAcc@5  74.22 ( 79.00)\n","Epoch: [8][300/391]\tTime  0.095 ( 0.095)\tLoss 1.9281e+00 (1.9294e+00)\tAcc@1  42.97 ( 47.06)\tAcc@5  79.69 ( 79.06)\n","Epoch: [8][330/391]\tTime  0.092 ( 0.094)\tLoss 1.9900e+00 (1.9364e+00)\tAcc@1  43.75 ( 46.86)\tAcc@5  76.56 ( 79.02)\n","Epoch: [8][360/391]\tTime  0.088 ( 0.094)\tLoss 2.1420e+00 (1.9371e+00)\tAcc@1  42.19 ( 46.84)\tAcc@5  77.34 ( 79.02)\n","Epoch: [8][390/391]\tTime  0.089 ( 0.094)\tLoss 1.7914e+00 (1.9355e+00)\tAcc@1  46.25 ( 47.00)\tAcc@5  82.50 ( 78.99)\n","==> Train Accuracy: Acc@1 46.996 || Acc@5 78.988\n","==> Test Accuracy:  Acc@1 46.530 || Acc@5 78.100\n","==> 39.87 seconds to train this epoch\n","\n","\n","----- epoch: 9, lr: 0.1 -----\n","Epoch: [9][  0/391]\tTime  0.262 ( 0.262)\tLoss 1.8527e+00 (1.8527e+00)\tAcc@1  50.78 ( 50.78)\tAcc@5  84.38 ( 84.38)\n","Epoch: [9][ 30/391]\tTime  0.099 ( 0.099)\tLoss 1.5169e+00 (1.8066e+00)\tAcc@1  60.94 ( 50.35)\tAcc@5  86.72 ( 81.63)\n","Epoch: [9][ 60/391]\tTime  0.093 ( 0.096)\tLoss 1.8790e+00 (1.8019e+00)\tAcc@1  46.88 ( 49.71)\tAcc@5  78.91 ( 81.56)\n","Epoch: [9][ 90/391]\tTime  0.093 ( 0.095)\tLoss 1.6586e+00 (1.8090e+00)\tAcc@1  50.00 ( 49.22)\tAcc@5  85.16 ( 81.52)\n","Epoch: [9][120/391]\tTime  0.077 ( 0.096)\tLoss 1.7279e+00 (1.8217e+00)\tAcc@1  49.22 ( 49.04)\tAcc@5  81.25 ( 81.23)\n","Epoch: [9][150/391]\tTime  0.091 ( 0.096)\tLoss 1.7737e+00 (1.8161e+00)\tAcc@1  49.22 ( 49.50)\tAcc@5  81.25 ( 81.28)\n","Epoch: [9][180/391]\tTime  0.092 ( 0.095)\tLoss 1.8681e+00 (1.8116e+00)\tAcc@1  50.00 ( 49.82)\tAcc@5  78.12 ( 81.25)\n","Epoch: [9][210/391]\tTime  0.090 ( 0.095)\tLoss 1.9193e+00 (1.8071e+00)\tAcc@1  47.66 ( 49.94)\tAcc@5  82.81 ( 81.34)\n","Epoch: [9][240/391]\tTime  0.093 ( 0.094)\tLoss 1.7477e+00 (1.8094e+00)\tAcc@1  53.12 ( 49.84)\tAcc@5  78.91 ( 81.22)\n","Epoch: [9][270/391]\tTime  0.094 ( 0.094)\tLoss 1.5805e+00 (1.8132e+00)\tAcc@1  53.91 ( 49.82)\tAcc@5  87.50 ( 81.21)\n","Epoch: [9][300/391]\tTime  0.095 ( 0.094)\tLoss 1.9490e+00 (1.8090e+00)\tAcc@1  47.66 ( 49.93)\tAcc@5  78.91 ( 81.34)\n","Epoch: [9][330/391]\tTime  0.092 ( 0.094)\tLoss 1.8109e+00 (1.8109e+00)\tAcc@1  54.69 ( 49.81)\tAcc@5  80.47 ( 81.33)\n","Epoch: [9][360/391]\tTime  0.091 ( 0.094)\tLoss 1.8305e+00 (1.8124e+00)\tAcc@1  51.56 ( 49.92)\tAcc@5  82.81 ( 81.30)\n","Epoch: [9][390/391]\tTime  0.079 ( 0.094)\tLoss 1.9471e+00 (1.8111e+00)\tAcc@1  50.00 ( 49.97)\tAcc@5  75.00 ( 81.28)\n","==> Train Accuracy: Acc@1 49.972 || Acc@5 81.282\n","==> Test Accuracy:  Acc@1 48.560 || Acc@5 79.730\n","==> 39.76 seconds to train this epoch\n","\n","\n","----- epoch: 10, lr: 0.1 -----\n","Epoch: [10][  0/391]\tTime  0.296 ( 0.296)\tLoss 1.5358e+00 (1.5358e+00)\tAcc@1  53.12 ( 53.12)\tAcc@5  87.50 ( 87.50)\n","Epoch: [10][ 30/391]\tTime  0.095 ( 0.099)\tLoss 1.9504e+00 (1.7386e+00)\tAcc@1  48.44 ( 52.44)\tAcc@5  78.91 ( 82.51)\n","Epoch: [10][ 60/391]\tTime  0.092 ( 0.096)\tLoss 1.7239e+00 (1.7225e+00)\tAcc@1  49.22 ( 52.37)\tAcc@5  82.03 ( 82.94)\n","Epoch: [10][ 90/391]\tTime  0.093 ( 0.095)\tLoss 1.5524e+00 (1.7162e+00)\tAcc@1  56.25 ( 52.64)\tAcc@5  85.16 ( 82.99)\n","Epoch: [10][120/391]\tTime  0.107 ( 0.097)\tLoss 1.6105e+00 (1.7215e+00)\tAcc@1  53.12 ( 52.41)\tAcc@5  86.72 ( 83.01)\n","Epoch: [10][150/391]\tTime  0.090 ( 0.096)\tLoss 2.1137e+00 (1.7197e+00)\tAcc@1  42.97 ( 52.28)\tAcc@5  78.12 ( 82.99)\n","Epoch: [10][180/391]\tTime  0.091 ( 0.096)\tLoss 1.7227e+00 (1.7094e+00)\tAcc@1  53.91 ( 52.49)\tAcc@5  82.03 ( 83.05)\n","Epoch: [10][210/391]\tTime  0.083 ( 0.095)\tLoss 1.5627e+00 (1.7095e+00)\tAcc@1  59.38 ( 52.56)\tAcc@5  85.94 ( 82.99)\n","Epoch: [10][240/391]\tTime  0.089 ( 0.095)\tLoss 1.8330e+00 (1.7114e+00)\tAcc@1  46.88 ( 52.45)\tAcc@5  82.81 ( 83.01)\n","Epoch: [10][270/391]\tTime  0.093 ( 0.095)\tLoss 1.6159e+00 (1.7163e+00)\tAcc@1  60.16 ( 52.42)\tAcc@5  82.03 ( 82.95)\n","Epoch: [10][300/391]\tTime  0.090 ( 0.094)\tLoss 1.4987e+00 (1.7178e+00)\tAcc@1  56.25 ( 52.36)\tAcc@5  86.72 ( 82.96)\n","Epoch: [10][330/391]\tTime  0.093 ( 0.094)\tLoss 1.7262e+00 (1.7167e+00)\tAcc@1  50.00 ( 52.37)\tAcc@5  83.59 ( 83.00)\n","Epoch: [10][360/391]\tTime  0.103 ( 0.094)\tLoss 1.7362e+00 (1.7166e+00)\tAcc@1  53.91 ( 52.33)\tAcc@5  82.03 ( 82.98)\n","Epoch: [10][390/391]\tTime  0.082 ( 0.094)\tLoss 1.8809e+00 (1.7171e+00)\tAcc@1  45.00 ( 52.34)\tAcc@5  80.00 ( 82.98)\n","==> Train Accuracy: Acc@1 52.344 || Acc@5 82.980\n","==> Test Accuracy:  Acc@1 51.500 || Acc@5 80.710\n","==> 39.81 seconds to train this epoch\n","\n","\n","----- epoch: 11, lr: 0.1 -----\n","Epoch: [11][  0/391]\tTime  0.316 ( 0.316)\tLoss 1.7057e+00 (1.7057e+00)\tAcc@1  53.12 ( 53.12)\tAcc@5  85.94 ( 85.94)\n","Epoch: [11][ 30/391]\tTime  0.092 ( 0.100)\tLoss 1.7805e+00 (1.5913e+00)\tAcc@1  51.56 ( 55.67)\tAcc@5  78.91 ( 85.31)\n","Epoch: [11][ 60/391]\tTime  0.093 ( 0.096)\tLoss 1.5041e+00 (1.5672e+00)\tAcc@1  58.59 ( 55.99)\tAcc@5  87.50 ( 85.41)\n","Epoch: [11][ 90/391]\tTime  0.091 ( 0.095)\tLoss 1.6712e+00 (1.5840e+00)\tAcc@1  57.03 ( 55.71)\tAcc@5  86.72 ( 85.16)\n","Epoch: [11][120/391]\tTime  0.096 ( 0.097)\tLoss 1.8049e+00 (1.6055e+00)\tAcc@1  47.66 ( 55.17)\tAcc@5  84.38 ( 84.79)\n","Epoch: [11][150/391]\tTime  0.089 ( 0.096)\tLoss 1.5578e+00 (1.6036e+00)\tAcc@1  53.91 ( 55.11)\tAcc@5  85.16 ( 84.77)\n","Epoch: [11][180/391]\tTime  0.090 ( 0.095)\tLoss 1.6780e+00 (1.6069e+00)\tAcc@1  56.25 ( 55.03)\tAcc@5  85.94 ( 84.81)\n","Epoch: [11][210/391]\tTime  0.093 ( 0.095)\tLoss 1.5603e+00 (1.6129e+00)\tAcc@1  59.38 ( 54.90)\tAcc@5  85.94 ( 84.63)\n","Epoch: [11][240/391]\tTime  0.093 ( 0.095)\tLoss 1.6984e+00 (1.6119e+00)\tAcc@1  57.81 ( 54.94)\tAcc@5  76.56 ( 84.63)\n","Epoch: [11][270/391]\tTime  0.091 ( 0.094)\tLoss 1.7512e+00 (1.6171e+00)\tAcc@1  55.47 ( 54.82)\tAcc@5  82.81 ( 84.49)\n","Epoch: [11][300/391]\tTime  0.092 ( 0.094)\tLoss 1.5796e+00 (1.6211e+00)\tAcc@1  56.25 ( 54.79)\tAcc@5  82.03 ( 84.42)\n","Epoch: [11][330/391]\tTime  0.100 ( 0.094)\tLoss 1.6628e+00 (1.6243e+00)\tAcc@1  53.12 ( 54.72)\tAcc@5  85.16 ( 84.33)\n","Epoch: [11][360/391]\tTime  0.097 ( 0.094)\tLoss 1.8727e+00 (1.6285e+00)\tAcc@1  50.78 ( 54.57)\tAcc@5  78.91 ( 84.30)\n","Epoch: [11][390/391]\tTime  0.082 ( 0.094)\tLoss 1.7639e+00 (1.6284e+00)\tAcc@1  52.50 ( 54.59)\tAcc@5  86.25 ( 84.31)\n","==> Train Accuracy: Acc@1 54.594 || Acc@5 84.308\n","==> Test Accuracy:  Acc@1 50.700 || Acc@5 81.910\n","==> 39.86 seconds to train this epoch\n","\n","\n","----- epoch: 12, lr: 0.1 -----\n","Epoch: [12][  0/391]\tTime  0.287 ( 0.287)\tLoss 1.2551e+00 (1.2551e+00)\tAcc@1  63.28 ( 63.28)\tAcc@5  91.41 ( 91.41)\n","Epoch: [12][ 30/391]\tTime  0.096 ( 0.100)\tLoss 1.5932e+00 (1.5170e+00)\tAcc@1  60.94 ( 57.89)\tAcc@5  85.94 ( 86.19)\n","Epoch: [12][ 60/391]\tTime  0.092 ( 0.097)\tLoss 1.5238e+00 (1.5251e+00)\tAcc@1  56.25 ( 57.61)\tAcc@5  85.94 ( 86.10)\n","Epoch: [12][ 90/391]\tTime  0.090 ( 0.095)\tLoss 1.5497e+00 (1.5295e+00)\tAcc@1  58.59 ( 57.23)\tAcc@5  85.16 ( 85.92)\n","Epoch: [12][120/391]\tTime  0.090 ( 0.094)\tLoss 1.5865e+00 (1.5408e+00)\tAcc@1  52.34 ( 56.85)\tAcc@5  83.59 ( 85.66)\n","Epoch: [12][150/391]\tTime  0.091 ( 0.094)\tLoss 1.7039e+00 (1.5444e+00)\tAcc@1  53.12 ( 56.92)\tAcc@5  84.38 ( 85.55)\n","Epoch: [12][180/391]\tTime  0.093 ( 0.094)\tLoss 1.3222e+00 (1.5415e+00)\tAcc@1  62.50 ( 56.98)\tAcc@5  87.50 ( 85.57)\n","Epoch: [12][210/391]\tTime  0.093 ( 0.094)\tLoss 1.4098e+00 (1.5443e+00)\tAcc@1  60.16 ( 56.82)\tAcc@5  86.72 ( 85.66)\n","Epoch: [12][240/391]\tTime  0.096 ( 0.094)\tLoss 1.6524e+00 (1.5503e+00)\tAcc@1  55.47 ( 56.66)\tAcc@5  83.59 ( 85.49)\n","Epoch: [12][270/391]\tTime  0.091 ( 0.094)\tLoss 1.3832e+00 (1.5581e+00)\tAcc@1  63.28 ( 56.42)\tAcc@5  87.50 ( 85.41)\n","Epoch: [12][300/391]\tTime  0.107 ( 0.094)\tLoss 1.7181e+00 (1.5595e+00)\tAcc@1  53.91 ( 56.35)\tAcc@5  81.25 ( 85.43)\n","Epoch: [12][330/391]\tTime  0.093 ( 0.093)\tLoss 1.4681e+00 (1.5635e+00)\tAcc@1  57.81 ( 56.22)\tAcc@5  85.94 ( 85.39)\n","Epoch: [12][360/391]\tTime  0.095 ( 0.093)\tLoss 1.7900e+00 (1.5694e+00)\tAcc@1  55.47 ( 56.01)\tAcc@5  82.81 ( 85.27)\n","Epoch: [12][390/391]\tTime  0.082 ( 0.093)\tLoss 1.6722e+00 (1.5690e+00)\tAcc@1  53.75 ( 56.03)\tAcc@5  83.75 ( 85.31)\n","==> Train Accuracy: Acc@1 56.032 || Acc@5 85.310\n","==> Test Accuracy:  Acc@1 52.620 || Acc@5 82.190\n","==> 39.55 seconds to train this epoch\n","\n","\n","----- epoch: 13, lr: 0.1 -----\n","Epoch: [13][  0/391]\tTime  0.304 ( 0.304)\tLoss 1.4580e+00 (1.4580e+00)\tAcc@1  60.94 ( 60.94)\tAcc@5  87.50 ( 87.50)\n","Epoch: [13][ 30/391]\tTime  0.092 ( 0.100)\tLoss 1.4793e+00 (1.4696e+00)\tAcc@1  59.38 ( 58.34)\tAcc@5  87.50 ( 86.54)\n","Epoch: [13][ 60/391]\tTime  0.092 ( 0.096)\tLoss 1.5185e+00 (1.4582e+00)\tAcc@1  60.94 ( 58.88)\tAcc@5  85.94 ( 87.10)\n","Epoch: [13][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.6490e+00 (1.4688e+00)\tAcc@1  57.03 ( 58.50)\tAcc@5  84.38 ( 87.16)\n","Epoch: [13][120/391]\tTime  0.123 ( 0.097)\tLoss 1.6602e+00 (1.4799e+00)\tAcc@1  54.69 ( 58.09)\tAcc@5  85.16 ( 87.02)\n","Epoch: [13][150/391]\tTime  0.092 ( 0.096)\tLoss 1.5260e+00 (1.4856e+00)\tAcc@1  57.81 ( 57.88)\tAcc@5  84.38 ( 86.83)\n","Epoch: [13][180/391]\tTime  0.104 ( 0.096)\tLoss 1.6635e+00 (1.4903e+00)\tAcc@1  57.81 ( 57.74)\tAcc@5  85.16 ( 86.66)\n","Epoch: [13][210/391]\tTime  0.091 ( 0.095)\tLoss 1.3777e+00 (1.4971e+00)\tAcc@1  63.28 ( 57.53)\tAcc@5  89.84 ( 86.46)\n","Epoch: [13][240/391]\tTime  0.091 ( 0.095)\tLoss 1.2623e+00 (1.5027e+00)\tAcc@1  65.62 ( 57.38)\tAcc@5  89.06 ( 86.43)\n","Epoch: [13][270/391]\tTime  0.095 ( 0.095)\tLoss 1.8714e+00 (1.5056e+00)\tAcc@1  52.34 ( 57.37)\tAcc@5  82.81 ( 86.36)\n","Epoch: [13][300/391]\tTime  0.093 ( 0.095)\tLoss 1.5500e+00 (1.5039e+00)\tAcc@1  55.47 ( 57.39)\tAcc@5  85.94 ( 86.40)\n","Epoch: [13][330/391]\tTime  0.093 ( 0.094)\tLoss 1.6888e+00 (1.5081e+00)\tAcc@1  51.56 ( 57.31)\tAcc@5  88.28 ( 86.30)\n","Epoch: [13][360/391]\tTime  0.094 ( 0.094)\tLoss 1.6074e+00 (1.5127e+00)\tAcc@1  59.38 ( 57.30)\tAcc@5  85.94 ( 86.21)\n","Epoch: [13][390/391]\tTime  0.082 ( 0.094)\tLoss 1.3199e+00 (1.5144e+00)\tAcc@1  62.50 ( 57.26)\tAcc@5  91.25 ( 86.23)\n","==> Train Accuracy: Acc@1 57.260 || Acc@5 86.234\n","==> Test Accuracy:  Acc@1 54.470 || Acc@5 83.710\n","==> 39.93 seconds to train this epoch\n","\n","\n","----- epoch: 14, lr: 0.1 -----\n","Epoch: [14][  0/391]\tTime  0.289 ( 0.289)\tLoss 1.3784e+00 (1.3784e+00)\tAcc@1  56.25 ( 56.25)\tAcc@5  88.28 ( 88.28)\n","Epoch: [14][ 30/391]\tTime  0.091 ( 0.099)\tLoss 1.3538e+00 (1.4319e+00)\tAcc@1  59.38 ( 58.82)\tAcc@5  89.84 ( 87.47)\n","Epoch: [14][ 60/391]\tTime  0.100 ( 0.096)\tLoss 1.3886e+00 (1.4476e+00)\tAcc@1  60.16 ( 58.61)\tAcc@5  85.94 ( 87.06)\n","Epoch: [14][ 90/391]\tTime  0.098 ( 0.095)\tLoss 1.5777e+00 (1.4381e+00)\tAcc@1  55.47 ( 59.07)\tAcc@5  87.50 ( 87.41)\n","Epoch: [14][120/391]\tTime  0.100 ( 0.097)\tLoss 1.2511e+00 (1.4467e+00)\tAcc@1  61.72 ( 58.93)\tAcc@5  90.62 ( 87.23)\n","Epoch: [14][150/391]\tTime  0.092 ( 0.096)\tLoss 1.2729e+00 (1.4491e+00)\tAcc@1  64.06 ( 58.97)\tAcc@5  92.19 ( 87.13)\n","Epoch: [14][180/391]\tTime  0.094 ( 0.096)\tLoss 1.2888e+00 (1.4576e+00)\tAcc@1  60.94 ( 58.65)\tAcc@5  91.41 ( 87.07)\n","Epoch: [14][210/391]\tTime  0.093 ( 0.095)\tLoss 1.6678e+00 (1.4555e+00)\tAcc@1  46.88 ( 58.70)\tAcc@5  89.06 ( 87.11)\n","Epoch: [14][240/391]\tTime  0.094 ( 0.095)\tLoss 1.6858e+00 (1.4554e+00)\tAcc@1  52.34 ( 58.65)\tAcc@5  80.47 ( 87.12)\n","Epoch: [14][270/391]\tTime  0.091 ( 0.095)\tLoss 1.5036e+00 (1.4596e+00)\tAcc@1  60.94 ( 58.54)\tAcc@5  85.94 ( 87.12)\n","Epoch: [14][300/391]\tTime  0.095 ( 0.094)\tLoss 1.6851e+00 (1.4633e+00)\tAcc@1  49.22 ( 58.45)\tAcc@5  82.81 ( 87.07)\n","Epoch: [14][330/391]\tTime  0.091 ( 0.094)\tLoss 1.6066e+00 (1.4666e+00)\tAcc@1  50.78 ( 58.37)\tAcc@5  85.16 ( 87.04)\n","Epoch: [14][360/391]\tTime  0.094 ( 0.094)\tLoss 1.6320e+00 (1.4676e+00)\tAcc@1  52.34 ( 58.33)\tAcc@5  81.25 ( 86.98)\n","Epoch: [14][390/391]\tTime  0.082 ( 0.094)\tLoss 1.5675e+00 (1.4698e+00)\tAcc@1  58.75 ( 58.27)\tAcc@5  83.75 ( 86.92)\n","==> Train Accuracy: Acc@1 58.266 || Acc@5 86.922\n","==> Test Accuracy:  Acc@1 50.390 || Acc@5 79.910\n","==> 39.86 seconds to train this epoch\n","\n","\n","----- epoch: 15, lr: 0.1 -----\n","Epoch: [15][  0/391]\tTime  0.304 ( 0.304)\tLoss 1.2829e+00 (1.2829e+00)\tAcc@1  64.06 ( 64.06)\tAcc@5  86.72 ( 86.72)\n","Epoch: [15][ 30/391]\tTime  0.093 ( 0.100)\tLoss 1.4527e+00 (1.3708e+00)\tAcc@1  62.50 ( 61.04)\tAcc@5  85.94 ( 88.08)\n","Epoch: [15][ 60/391]\tTime  0.103 ( 0.097)\tLoss 1.2189e+00 (1.3638e+00)\tAcc@1  69.53 ( 61.31)\tAcc@5  89.06 ( 88.52)\n","Epoch: [15][ 90/391]\tTime  0.090 ( 0.095)\tLoss 1.4394e+00 (1.3794e+00)\tAcc@1  57.81 ( 60.68)\tAcc@5  92.19 ( 88.32)\n","Epoch: [15][120/391]\tTime  0.094 ( 0.095)\tLoss 1.3836e+00 (1.4038e+00)\tAcc@1  60.16 ( 59.99)\tAcc@5  91.41 ( 87.99)\n","Epoch: [15][150/391]\tTime  0.092 ( 0.094)\tLoss 1.7413e+00 (1.4150e+00)\tAcc@1  49.22 ( 59.56)\tAcc@5  83.59 ( 87.83)\n","Epoch: [15][180/391]\tTime  0.089 ( 0.094)\tLoss 1.4931e+00 (1.4138e+00)\tAcc@1  54.69 ( 59.61)\tAcc@5  86.72 ( 87.80)\n","Epoch: [15][210/391]\tTime  0.091 ( 0.094)\tLoss 1.5451e+00 (1.4175e+00)\tAcc@1  53.12 ( 59.48)\tAcc@5  86.72 ( 87.79)\n","Epoch: [15][240/391]\tTime  0.090 ( 0.094)\tLoss 1.3446e+00 (1.4223e+00)\tAcc@1  65.62 ( 59.43)\tAcc@5  91.41 ( 87.73)\n","Epoch: [15][270/391]\tTime  0.095 ( 0.094)\tLoss 1.4882e+00 (1.4231e+00)\tAcc@1  60.94 ( 59.48)\tAcc@5  85.94 ( 87.69)\n","Epoch: [15][300/391]\tTime  0.093 ( 0.094)\tLoss 1.3692e+00 (1.4276e+00)\tAcc@1  60.94 ( 59.47)\tAcc@5  91.41 ( 87.63)\n","Epoch: [15][330/391]\tTime  0.091 ( 0.093)\tLoss 1.5333e+00 (1.4290e+00)\tAcc@1  57.81 ( 59.51)\tAcc@5  85.94 ( 87.59)\n","Epoch: [15][360/391]\tTime  0.089 ( 0.093)\tLoss 1.3947e+00 (1.4299e+00)\tAcc@1  60.94 ( 59.50)\tAcc@5  88.28 ( 87.58)\n","Epoch: [15][390/391]\tTime  0.087 ( 0.093)\tLoss 1.4290e+00 (1.4302e+00)\tAcc@1  55.00 ( 59.48)\tAcc@5  92.50 ( 87.58)\n","==> Train Accuracy: Acc@1 59.482 || Acc@5 87.584\n","==> Test Accuracy:  Acc@1 52.720 || Acc@5 82.850\n","==> 39.56 seconds to train this epoch\n","\n","\n","----- epoch: 16, lr: 0.1 -----\n","Epoch: [16][  0/391]\tTime  0.287 ( 0.287)\tLoss 1.4250e+00 (1.4250e+00)\tAcc@1  56.25 ( 56.25)\tAcc@5  89.06 ( 89.06)\n","Epoch: [16][ 30/391]\tTime  0.096 ( 0.099)\tLoss 1.3267e+00 (1.3480e+00)\tAcc@1  62.50 ( 60.91)\tAcc@5  90.62 ( 89.24)\n","Epoch: [16][ 60/391]\tTime  0.088 ( 0.096)\tLoss 1.4736e+00 (1.3547e+00)\tAcc@1  61.72 ( 61.44)\tAcc@5  84.38 ( 88.90)\n","Epoch: [16][ 90/391]\tTime  0.082 ( 0.095)\tLoss 1.3593e+00 (1.3565e+00)\tAcc@1  63.28 ( 61.55)\tAcc@5  89.84 ( 88.75)\n","Epoch: [16][120/391]\tTime  0.093 ( 0.094)\tLoss 1.3394e+00 (1.3659e+00)\tAcc@1  55.47 ( 61.09)\tAcc@5  92.97 ( 88.63)\n","Epoch: [16][150/391]\tTime  0.091 ( 0.094)\tLoss 1.5825e+00 (1.3750e+00)\tAcc@1  53.91 ( 60.96)\tAcc@5  85.16 ( 88.46)\n","Epoch: [16][180/391]\tTime  0.090 ( 0.094)\tLoss 1.2464e+00 (1.3768e+00)\tAcc@1  64.06 ( 60.78)\tAcc@5  92.19 ( 88.50)\n","Epoch: [16][210/391]\tTime  0.092 ( 0.094)\tLoss 1.2782e+00 (1.3740e+00)\tAcc@1  61.72 ( 60.80)\tAcc@5  90.62 ( 88.60)\n","Epoch: [16][240/391]\tTime  0.094 ( 0.094)\tLoss 1.4079e+00 (1.3775e+00)\tAcc@1  62.50 ( 60.77)\tAcc@5  86.72 ( 88.52)\n","Epoch: [16][270/391]\tTime  0.092 ( 0.093)\tLoss 1.5084e+00 (1.3806e+00)\tAcc@1  57.81 ( 60.76)\tAcc@5  82.03 ( 88.38)\n","Epoch: [16][300/391]\tTime  0.091 ( 0.093)\tLoss 1.4178e+00 (1.3887e+00)\tAcc@1  60.94 ( 60.59)\tAcc@5  85.16 ( 88.21)\n","Epoch: [16][330/391]\tTime  0.097 ( 0.093)\tLoss 1.3960e+00 (1.3958e+00)\tAcc@1  60.16 ( 60.44)\tAcc@5  85.16 ( 88.09)\n","Epoch: [16][360/391]\tTime  0.092 ( 0.093)\tLoss 1.4408e+00 (1.3981e+00)\tAcc@1  57.03 ( 60.34)\tAcc@5  85.16 ( 88.08)\n","Epoch: [16][390/391]\tTime  0.083 ( 0.093)\tLoss 1.4316e+00 (1.4005e+00)\tAcc@1  57.50 ( 60.28)\tAcc@5  90.00 ( 88.02)\n","==> Train Accuracy: Acc@1 60.282 || Acc@5 88.020\n","==> Test Accuracy:  Acc@1 52.900 || Acc@5 82.760\n","==> 39.55 seconds to train this epoch\n","\n","\n","----- epoch: 17, lr: 0.1 -----\n","Epoch: [17][  0/391]\tTime  0.260 ( 0.260)\tLoss 1.1066e+00 (1.1066e+00)\tAcc@1  65.62 ( 65.62)\tAcc@5  90.62 ( 90.62)\n","Epoch: [17][ 30/391]\tTime  0.087 ( 0.100)\tLoss 1.4270e+00 (1.3078e+00)\tAcc@1  62.50 ( 62.93)\tAcc@5  83.59 ( 89.36)\n","Epoch: [17][ 60/391]\tTime  0.100 ( 0.096)\tLoss 1.5506e+00 (1.3216e+00)\tAcc@1  56.25 ( 62.35)\tAcc@5  85.94 ( 89.16)\n","Epoch: [17][ 90/391]\tTime  0.094 ( 0.095)\tLoss 1.3421e+00 (1.3337e+00)\tAcc@1  65.62 ( 62.04)\tAcc@5  88.28 ( 88.93)\n","Epoch: [17][120/391]\tTime  0.091 ( 0.095)\tLoss 1.3983e+00 (1.3330e+00)\tAcc@1  63.28 ( 62.16)\tAcc@5  88.28 ( 88.82)\n","Epoch: [17][150/391]\tTime  0.091 ( 0.094)\tLoss 1.4008e+00 (1.3368e+00)\tAcc@1  57.03 ( 61.81)\tAcc@5  90.62 ( 88.92)\n","Epoch: [17][180/391]\tTime  0.093 ( 0.094)\tLoss 1.8507e+00 (1.3521e+00)\tAcc@1  53.91 ( 61.44)\tAcc@5  77.34 ( 88.73)\n","Epoch: [17][210/391]\tTime  0.091 ( 0.094)\tLoss 1.3420e+00 (1.3492e+00)\tAcc@1  58.59 ( 61.53)\tAcc@5  92.97 ( 88.85)\n","Epoch: [17][240/391]\tTime  0.093 ( 0.094)\tLoss 1.2738e+00 (1.3495e+00)\tAcc@1  62.50 ( 61.45)\tAcc@5  89.84 ( 88.81)\n","Epoch: [17][270/391]\tTime  0.091 ( 0.094)\tLoss 1.2737e+00 (1.3511e+00)\tAcc@1  66.41 ( 61.45)\tAcc@5  89.84 ( 88.78)\n","Epoch: [17][300/391]\tTime  0.093 ( 0.094)\tLoss 1.6106e+00 (1.3568e+00)\tAcc@1  61.72 ( 61.35)\tAcc@5  81.25 ( 88.64)\n","Epoch: [17][330/391]\tTime  0.095 ( 0.093)\tLoss 1.6402e+00 (1.3606e+00)\tAcc@1  54.69 ( 61.22)\tAcc@5  80.47 ( 88.58)\n","Epoch: [17][360/391]\tTime  0.091 ( 0.093)\tLoss 1.3965e+00 (1.3631e+00)\tAcc@1  60.16 ( 61.12)\tAcc@5  89.84 ( 88.57)\n","Epoch: [17][390/391]\tTime  0.082 ( 0.093)\tLoss 1.5469e+00 (1.3680e+00)\tAcc@1  55.00 ( 61.00)\tAcc@5  91.25 ( 88.51)\n","==> Train Accuracy: Acc@1 61.004 || Acc@5 88.508\n","==> Test Accuracy:  Acc@1 55.630 || Acc@5 83.590\n","==> 39.58 seconds to train this epoch\n","\n","\n","----- epoch: 18, lr: 0.1 -----\n","Epoch: [18][  0/391]\tTime  0.309 ( 0.309)\tLoss 1.2771e+00 (1.2771e+00)\tAcc@1  64.84 ( 64.84)\tAcc@5  89.84 ( 89.84)\n","Epoch: [18][ 30/391]\tTime  0.093 ( 0.101)\tLoss 1.2381e+00 (1.3048e+00)\tAcc@1  67.19 ( 62.40)\tAcc@5  86.72 ( 89.62)\n","Epoch: [18][ 60/391]\tTime  0.093 ( 0.097)\tLoss 1.5136e+00 (1.2695e+00)\tAcc@1  57.81 ( 63.23)\tAcc@5  85.94 ( 89.92)\n","Epoch: [18][ 90/391]\tTime  0.094 ( 0.095)\tLoss 1.2372e+00 (1.2714e+00)\tAcc@1  66.41 ( 63.06)\tAcc@5  91.41 ( 89.98)\n","Epoch: [18][120/391]\tTime  0.103 ( 0.098)\tLoss 1.1726e+00 (1.2866e+00)\tAcc@1  66.41 ( 62.84)\tAcc@5  89.06 ( 89.86)\n","Epoch: [18][150/391]\tTime  0.087 ( 0.097)\tLoss 1.4473e+00 (1.3059e+00)\tAcc@1  64.06 ( 62.68)\tAcc@5  85.94 ( 89.54)\n","Epoch: [18][180/391]\tTime  0.092 ( 0.096)\tLoss 1.3683e+00 (1.3154e+00)\tAcc@1  58.59 ( 62.44)\tAcc@5  86.72 ( 89.25)\n","Epoch: [18][210/391]\tTime  0.091 ( 0.096)\tLoss 1.5190e+00 (1.3175e+00)\tAcc@1  57.03 ( 62.45)\tAcc@5  85.94 ( 89.23)\n","Epoch: [18][240/391]\tTime  0.091 ( 0.095)\tLoss 1.3363e+00 (1.3230e+00)\tAcc@1  61.72 ( 62.24)\tAcc@5  89.06 ( 89.17)\n","Epoch: [18][270/391]\tTime  0.092 ( 0.095)\tLoss 1.1030e+00 (1.3274e+00)\tAcc@1  68.75 ( 62.13)\tAcc@5  91.41 ( 89.03)\n","Epoch: [18][300/391]\tTime  0.090 ( 0.095)\tLoss 1.3917e+00 (1.3336e+00)\tAcc@1  64.06 ( 62.02)\tAcc@5  87.50 ( 88.96)\n","Epoch: [18][330/391]\tTime  0.091 ( 0.095)\tLoss 1.4591e+00 (1.3362e+00)\tAcc@1  59.38 ( 61.95)\tAcc@5  88.28 ( 88.92)\n","Epoch: [18][360/391]\tTime  0.094 ( 0.094)\tLoss 1.2591e+00 (1.3404e+00)\tAcc@1  64.84 ( 61.90)\tAcc@5  91.41 ( 88.84)\n","Epoch: [18][390/391]\tTime  0.082 ( 0.094)\tLoss 1.4520e+00 (1.3423e+00)\tAcc@1  62.50 ( 61.83)\tAcc@5  86.25 ( 88.78)\n","==> Train Accuracy: Acc@1 61.832 || Acc@5 88.784\n","==> Test Accuracy:  Acc@1 54.550 || Acc@5 83.080\n","==> 39.96 seconds to train this epoch\n","\n","\n","----- epoch: 19, lr: 0.1 -----\n","Epoch: [19][  0/391]\tTime  0.265 ( 0.265)\tLoss 1.2814e+00 (1.2814e+00)\tAcc@1  66.41 ( 66.41)\tAcc@5  89.84 ( 89.84)\n","Epoch: [19][ 30/391]\tTime  0.093 ( 0.099)\tLoss 1.1332e+00 (1.2115e+00)\tAcc@1  68.75 ( 64.54)\tAcc@5  92.19 ( 90.85)\n","Epoch: [19][ 60/391]\tTime  0.091 ( 0.096)\tLoss 1.1848e+00 (1.2266e+00)\tAcc@1  64.06 ( 64.01)\tAcc@5  90.62 ( 90.42)\n","Epoch: [19][ 90/391]\tTime  0.090 ( 0.095)\tLoss 1.1609e+00 (1.2401e+00)\tAcc@1  63.28 ( 64.02)\tAcc@5  89.84 ( 89.96)\n","Epoch: [19][120/391]\tTime  0.093 ( 0.094)\tLoss 1.4029e+00 (1.2554e+00)\tAcc@1  58.59 ( 63.74)\tAcc@5  87.50 ( 89.73)\n","Epoch: [19][150/391]\tTime  0.087 ( 0.094)\tLoss 1.2287e+00 (1.2610e+00)\tAcc@1  63.28 ( 63.60)\tAcc@5  87.50 ( 89.81)\n","Epoch: [19][180/391]\tTime  0.094 ( 0.094)\tLoss 1.4299e+00 (1.2797e+00)\tAcc@1  61.72 ( 63.18)\tAcc@5  86.72 ( 89.63)\n","Epoch: [19][210/391]\tTime  0.089 ( 0.094)\tLoss 1.1209e+00 (1.2861e+00)\tAcc@1  64.84 ( 63.13)\tAcc@5  92.19 ( 89.51)\n","Epoch: [19][240/391]\tTime  0.091 ( 0.093)\tLoss 1.5115e+00 (1.2916e+00)\tAcc@1  57.81 ( 63.03)\tAcc@5  86.72 ( 89.47)\n","Epoch: [19][270/391]\tTime  0.102 ( 0.093)\tLoss 1.3844e+00 (1.2977e+00)\tAcc@1  60.94 ( 62.81)\tAcc@5  85.94 ( 89.41)\n","Epoch: [19][300/391]\tTime  0.091 ( 0.093)\tLoss 1.4146e+00 (1.3006e+00)\tAcc@1  64.06 ( 62.82)\tAcc@5  86.72 ( 89.36)\n","Epoch: [19][330/391]\tTime  0.099 ( 0.093)\tLoss 1.2726e+00 (1.3047e+00)\tAcc@1  64.84 ( 62.70)\tAcc@5  91.41 ( 89.34)\n","Epoch: [19][360/391]\tTime  0.091 ( 0.093)\tLoss 1.2278e+00 (1.3084e+00)\tAcc@1  60.94 ( 62.61)\tAcc@5  93.75 ( 89.29)\n","Epoch: [19][390/391]\tTime  0.081 ( 0.093)\tLoss 1.3911e+00 (1.3111e+00)\tAcc@1  61.25 ( 62.54)\tAcc@5  87.50 ( 89.22)\n","==> Train Accuracy: Acc@1 62.540 || Acc@5 89.222\n","==> Test Accuracy:  Acc@1 45.380 || Acc@5 75.010\n","==> 39.51 seconds to train this epoch\n","\n","\n","----- epoch: 20, lr: 0.1 -----\n","Epoch: [20][  0/391]\tTime  0.288 ( 0.288)\tLoss 1.2419e+00 (1.2419e+00)\tAcc@1  62.50 ( 62.50)\tAcc@5  90.62 ( 90.62)\n","Epoch: [20][ 30/391]\tTime  0.092 ( 0.099)\tLoss 1.4925e+00 (1.2606e+00)\tAcc@1  55.47 ( 63.08)\tAcc@5  85.94 ( 89.87)\n","Epoch: [20][ 60/391]\tTime  0.088 ( 0.096)\tLoss 1.2619e+00 (1.2345e+00)\tAcc@1  58.59 ( 63.97)\tAcc@5  91.41 ( 90.29)\n","Epoch: [20][ 90/391]\tTime  0.093 ( 0.095)\tLoss 1.3536e+00 (1.2575e+00)\tAcc@1  57.81 ( 63.73)\tAcc@5  89.06 ( 89.91)\n","Epoch: [20][120/391]\tTime  0.093 ( 0.095)\tLoss 1.2546e+00 (1.2688e+00)\tAcc@1  66.41 ( 63.62)\tAcc@5  86.72 ( 89.79)\n","Epoch: [20][150/391]\tTime  0.090 ( 0.094)\tLoss 1.2507e+00 (1.2627e+00)\tAcc@1  64.84 ( 63.84)\tAcc@5  91.41 ( 89.87)\n","Epoch: [20][180/391]\tTime  0.090 ( 0.094)\tLoss 1.3099e+00 (1.2616e+00)\tAcc@1  63.28 ( 63.83)\tAcc@5  87.50 ( 89.90)\n","Epoch: [20][210/391]\tTime  0.096 ( 0.094)\tLoss 1.4501e+00 (1.2714e+00)\tAcc@1  59.38 ( 63.55)\tAcc@5  86.72 ( 89.70)\n","Epoch: [20][240/391]\tTime  0.100 ( 0.094)\tLoss 1.2879e+00 (1.2801e+00)\tAcc@1  61.72 ( 63.33)\tAcc@5  92.19 ( 89.58)\n","Epoch: [20][270/391]\tTime  0.093 ( 0.094)\tLoss 1.3067e+00 (1.2805e+00)\tAcc@1  64.84 ( 63.36)\tAcc@5  88.28 ( 89.60)\n","Epoch: [20][300/391]\tTime  0.094 ( 0.094)\tLoss 1.1351e+00 (1.2859e+00)\tAcc@1  67.97 ( 63.21)\tAcc@5  89.06 ( 89.53)\n","Epoch: [20][330/391]\tTime  0.093 ( 0.093)\tLoss 9.7906e-01 (1.2920e+00)\tAcc@1  73.44 ( 63.05)\tAcc@5  92.19 ( 89.45)\n","Epoch: [20][360/391]\tTime  0.092 ( 0.093)\tLoss 1.3302e+00 (1.2969e+00)\tAcc@1  63.28 ( 62.92)\tAcc@5  89.84 ( 89.41)\n","Epoch: [20][390/391]\tTime  0.082 ( 0.093)\tLoss 1.3577e+00 (1.3001e+00)\tAcc@1  62.50 ( 62.84)\tAcc@5  85.00 ( 89.34)\n","==> Train Accuracy: Acc@1 62.838 || Acc@5 89.344\n","==> Test Accuracy:  Acc@1 55.770 || Acc@5 83.790\n","==> 39.56 seconds to train this epoch\n","\n","\n","----- epoch: 21, lr: 0.1 -----\n","Epoch: [21][  0/391]\tTime  0.273 ( 0.273)\tLoss 1.4532e+00 (1.4532e+00)\tAcc@1  59.38 ( 59.38)\tAcc@5  85.94 ( 85.94)\n","Epoch: [21][ 30/391]\tTime  0.093 ( 0.099)\tLoss 1.1302e+00 (1.2465e+00)\tAcc@1  71.88 ( 64.64)\tAcc@5  90.62 ( 90.57)\n","Epoch: [21][ 60/391]\tTime  0.092 ( 0.096)\tLoss 1.4802e+00 (1.2150e+00)\tAcc@1  57.03 ( 65.30)\tAcc@5  87.50 ( 90.71)\n","Epoch: [21][ 90/391]\tTime  0.091 ( 0.095)\tLoss 1.6399e+00 (1.2256e+00)\tAcc@1  57.03 ( 64.72)\tAcc@5  85.16 ( 90.28)\n","Epoch: [21][120/391]\tTime  0.105 ( 0.097)\tLoss 1.4128e+00 (1.2356e+00)\tAcc@1  61.72 ( 64.37)\tAcc@5  85.16 ( 90.16)\n","Epoch: [21][150/391]\tTime  0.090 ( 0.096)\tLoss 1.0333e+00 (1.2452e+00)\tAcc@1  68.75 ( 64.06)\tAcc@5  93.75 ( 90.11)\n","Epoch: [21][180/391]\tTime  0.092 ( 0.096)\tLoss 1.2493e+00 (1.2571e+00)\tAcc@1  61.72 ( 63.79)\tAcc@5  91.41 ( 89.83)\n","Epoch: [21][210/391]\tTime  0.096 ( 0.095)\tLoss 1.2514e+00 (1.2658e+00)\tAcc@1  63.28 ( 63.59)\tAcc@5  90.62 ( 89.70)\n","Epoch: [21][240/391]\tTime  0.089 ( 0.095)\tLoss 1.4885e+00 (1.2654e+00)\tAcc@1  60.94 ( 63.58)\tAcc@5  85.94 ( 89.70)\n","Epoch: [21][270/391]\tTime  0.092 ( 0.095)\tLoss 1.2973e+00 (1.2682e+00)\tAcc@1  65.62 ( 63.53)\tAcc@5  89.06 ( 89.63)\n","Epoch: [21][300/391]\tTime  0.093 ( 0.094)\tLoss 1.3126e+00 (1.2682e+00)\tAcc@1  60.94 ( 63.50)\tAcc@5  88.28 ( 89.68)\n","Epoch: [21][330/391]\tTime  0.089 ( 0.094)\tLoss 1.3162e+00 (1.2723e+00)\tAcc@1  61.72 ( 63.39)\tAcc@5  90.62 ( 89.67)\n","Epoch: [21][360/391]\tTime  0.092 ( 0.094)\tLoss 1.5480e+00 (1.2766e+00)\tAcc@1  58.59 ( 63.26)\tAcc@5  86.72 ( 89.64)\n","Epoch: [21][390/391]\tTime  0.077 ( 0.094)\tLoss 1.4441e+00 (1.2820e+00)\tAcc@1  57.50 ( 63.05)\tAcc@5  88.75 ( 89.59)\n","==> Train Accuracy: Acc@1 63.048 || Acc@5 89.588\n","==> Test Accuracy:  Acc@1 54.200 || Acc@5 83.040\n","==> 39.73 seconds to train this epoch\n","\n","\n","----- epoch: 22, lr: 0.1 -----\n","Epoch: [22][  0/391]\tTime  0.288 ( 0.288)\tLoss 1.0173e+00 (1.0173e+00)\tAcc@1  64.84 ( 64.84)\tAcc@5  95.31 ( 95.31)\n","Epoch: [22][ 30/391]\tTime  0.091 ( 0.099)\tLoss 1.2238e+00 (1.1736e+00)\tAcc@1  64.84 ( 65.93)\tAcc@5  94.53 ( 91.20)\n","Epoch: [22][ 60/391]\tTime  0.080 ( 0.096)\tLoss 1.3020e+00 (1.2071e+00)\tAcc@1  64.84 ( 65.16)\tAcc@5  89.06 ( 90.71)\n","Epoch: [22][ 90/391]\tTime  0.091 ( 0.095)\tLoss 1.1223e+00 (1.2224e+00)\tAcc@1  67.97 ( 64.71)\tAcc@5  92.97 ( 90.64)\n","Epoch: [22][120/391]\tTime  0.092 ( 0.094)\tLoss 1.0232e+00 (1.2306e+00)\tAcc@1  72.66 ( 64.34)\tAcc@5  90.62 ( 90.43)\n","Epoch: [22][150/391]\tTime  0.086 ( 0.094)\tLoss 1.1549e+00 (1.2333e+00)\tAcc@1  70.31 ( 64.45)\tAcc@5  94.53 ( 90.37)\n","Epoch: [22][180/391]\tTime  0.093 ( 0.094)\tLoss 1.3516e+00 (1.2441e+00)\tAcc@1  60.16 ( 64.12)\tAcc@5  89.06 ( 90.21)\n","Epoch: [22][210/391]\tTime  0.095 ( 0.094)\tLoss 1.2169e+00 (1.2457e+00)\tAcc@1  69.53 ( 64.18)\tAcc@5  89.84 ( 90.10)\n","Epoch: [22][240/391]\tTime  0.089 ( 0.094)\tLoss 1.3051e+00 (1.2538e+00)\tAcc@1  61.72 ( 63.92)\tAcc@5  89.06 ( 89.99)\n","Epoch: [22][270/391]\tTime  0.094 ( 0.094)\tLoss 1.4230e+00 (1.2579e+00)\tAcc@1  59.38 ( 63.85)\tAcc@5  86.72 ( 89.86)\n","Epoch: [22][300/391]\tTime  0.091 ( 0.093)\tLoss 1.2887e+00 (1.2636e+00)\tAcc@1  60.94 ( 63.61)\tAcc@5  89.06 ( 89.83)\n","Epoch: [22][330/391]\tTime  0.092 ( 0.093)\tLoss 1.2771e+00 (1.2639e+00)\tAcc@1  57.81 ( 63.64)\tAcc@5  96.88 ( 89.83)\n","Epoch: [22][360/391]\tTime  0.095 ( 0.093)\tLoss 1.1019e+00 (1.2643e+00)\tAcc@1  73.44 ( 63.66)\tAcc@5  89.84 ( 89.79)\n","Epoch: [22][390/391]\tTime  0.087 ( 0.093)\tLoss 1.4520e+00 (1.2667e+00)\tAcc@1  58.75 ( 63.60)\tAcc@5  86.25 ( 89.84)\n","==> Train Accuracy: Acc@1 63.596 || Acc@5 89.836\n","==> Test Accuracy:  Acc@1 53.960 || Acc@5 83.320\n","==> 39.51 seconds to train this epoch\n","\n","\n","----- epoch: 23, lr: 0.1 -----\n","Epoch: [23][  0/391]\tTime  0.273 ( 0.273)\tLoss 1.2082e+00 (1.2082e+00)\tAcc@1  68.75 ( 68.75)\tAcc@5  91.41 ( 91.41)\n","Epoch: [23][ 30/391]\tTime  0.089 ( 0.099)\tLoss 1.1606e+00 (1.1688e+00)\tAcc@1  67.19 ( 65.85)\tAcc@5  90.62 ( 91.53)\n","Epoch: [23][ 60/391]\tTime  0.095 ( 0.096)\tLoss 1.0307e+00 (1.1725e+00)\tAcc@1  67.97 ( 65.61)\tAcc@5  95.31 ( 91.24)\n","Epoch: [23][ 90/391]\tTime  0.090 ( 0.095)\tLoss 9.8508e-01 (1.1823e+00)\tAcc@1  73.44 ( 65.39)\tAcc@5  90.62 ( 91.11)\n","Epoch: [23][120/391]\tTime  0.091 ( 0.094)\tLoss 1.4588e+00 (1.2006e+00)\tAcc@1  57.81 ( 65.10)\tAcc@5  91.41 ( 90.75)\n","Epoch: [23][150/391]\tTime  0.091 ( 0.094)\tLoss 1.0160e+00 (1.2061e+00)\tAcc@1  69.53 ( 64.96)\tAcc@5  89.84 ( 90.66)\n","Epoch: [23][180/391]\tTime  0.089 ( 0.094)\tLoss 1.0418e+00 (1.2107e+00)\tAcc@1  67.97 ( 64.96)\tAcc@5  93.75 ( 90.47)\n","Epoch: [23][210/391]\tTime  0.102 ( 0.094)\tLoss 1.3396e+00 (1.2164e+00)\tAcc@1  64.06 ( 64.78)\tAcc@5  89.06 ( 90.45)\n","Epoch: [23][240/391]\tTime  0.099 ( 0.094)\tLoss 1.1840e+00 (1.2227e+00)\tAcc@1  66.41 ( 64.69)\tAcc@5  92.19 ( 90.37)\n","Epoch: [23][270/391]\tTime  0.091 ( 0.093)\tLoss 1.3410e+00 (1.2321e+00)\tAcc@1  59.38 ( 64.55)\tAcc@5  89.06 ( 90.21)\n","Epoch: [23][300/391]\tTime  0.090 ( 0.093)\tLoss 1.1947e+00 (1.2361e+00)\tAcc@1  67.19 ( 64.46)\tAcc@5  90.62 ( 90.16)\n","Epoch: [23][330/391]\tTime  0.088 ( 0.093)\tLoss 1.2880e+00 (1.2413e+00)\tAcc@1  61.72 ( 64.32)\tAcc@5  89.06 ( 90.04)\n","Epoch: [23][360/391]\tTime  0.088 ( 0.093)\tLoss 1.4739e+00 (1.2458e+00)\tAcc@1  57.81 ( 64.26)\tAcc@5  89.06 ( 89.96)\n","Epoch: [23][390/391]\tTime  0.081 ( 0.093)\tLoss 1.3039e+00 (1.2471e+00)\tAcc@1  61.25 ( 64.26)\tAcc@5  90.00 ( 89.99)\n","==> Train Accuracy: Acc@1 64.264 || Acc@5 89.990\n","==> Test Accuracy:  Acc@1 56.600 || Acc@5 84.480\n","==> 39.46 seconds to train this epoch\n","\n","\n","----- epoch: 24, lr: 0.1 -----\n","Epoch: [24][  0/391]\tTime  0.290 ( 0.290)\tLoss 1.3168e+00 (1.3168e+00)\tAcc@1  63.28 ( 63.28)\tAcc@5  87.50 ( 87.50)\n","Epoch: [24][ 30/391]\tTime  0.092 ( 0.099)\tLoss 1.1263e+00 (1.1438e+00)\tAcc@1  67.19 ( 66.38)\tAcc@5  89.06 ( 91.26)\n","Epoch: [24][ 60/391]\tTime  0.092 ( 0.096)\tLoss 1.3004e+00 (1.1408e+00)\tAcc@1  63.28 ( 66.68)\tAcc@5  88.28 ( 91.39)\n","Epoch: [24][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.0781e+00 (1.1674e+00)\tAcc@1  67.97 ( 66.08)\tAcc@5  92.19 ( 90.98)\n","Epoch: [24][120/391]\tTime  0.095 ( 0.097)\tLoss 1.0208e+00 (1.1776e+00)\tAcc@1  69.53 ( 65.81)\tAcc@5  95.31 ( 90.73)\n","Epoch: [24][150/391]\tTime  0.092 ( 0.096)\tLoss 1.2642e+00 (1.1889e+00)\tAcc@1  60.94 ( 65.36)\tAcc@5  89.06 ( 90.65)\n","Epoch: [24][180/391]\tTime  0.094 ( 0.096)\tLoss 1.2076e+00 (1.1987e+00)\tAcc@1  65.62 ( 65.27)\tAcc@5  93.75 ( 90.59)\n","Epoch: [24][210/391]\tTime  0.091 ( 0.095)\tLoss 1.4494e+00 (1.2059e+00)\tAcc@1  59.38 ( 65.05)\tAcc@5  86.72 ( 90.63)\n","Epoch: [24][240/391]\tTime  0.091 ( 0.095)\tLoss 1.4014e+00 (1.2151e+00)\tAcc@1  56.25 ( 64.88)\tAcc@5  88.28 ( 90.45)\n","Epoch: [24][270/391]\tTime  0.092 ( 0.095)\tLoss 1.2475e+00 (1.2136e+00)\tAcc@1  60.94 ( 64.97)\tAcc@5  91.41 ( 90.39)\n","Epoch: [24][300/391]\tTime  0.093 ( 0.094)\tLoss 1.0456e+00 (1.2186e+00)\tAcc@1  71.09 ( 64.84)\tAcc@5  88.28 ( 90.32)\n","Epoch: [24][330/391]\tTime  0.092 ( 0.094)\tLoss 1.2251e+00 (1.2240e+00)\tAcc@1  62.50 ( 64.75)\tAcc@5  89.06 ( 90.24)\n","Epoch: [24][360/391]\tTime  0.093 ( 0.094)\tLoss 1.3486e+00 (1.2277e+00)\tAcc@1  60.94 ( 64.61)\tAcc@5  89.06 ( 90.22)\n","Epoch: [24][390/391]\tTime  0.081 ( 0.094)\tLoss 1.1237e+00 (1.2324e+00)\tAcc@1  65.00 ( 64.53)\tAcc@5  92.50 ( 90.18)\n","==> Train Accuracy: Acc@1 64.532 || Acc@5 90.176\n","==> Test Accuracy:  Acc@1 57.110 || Acc@5 84.950\n","==> 39.83 seconds to train this epoch\n","\n","\n","----- epoch: 25, lr: 0.1 -----\n","Epoch: [25][  0/391]\tTime  0.304 ( 0.304)\tLoss 1.1882e+00 (1.1882e+00)\tAcc@1  68.75 ( 68.75)\tAcc@5  88.28 ( 88.28)\n","Epoch: [25][ 30/391]\tTime  0.092 ( 0.100)\tLoss 1.1468e+00 (1.1817e+00)\tAcc@1  62.50 ( 65.50)\tAcc@5  92.19 ( 91.00)\n","Epoch: [25][ 60/391]\tTime  0.092 ( 0.096)\tLoss 1.1939e+00 (1.1406e+00)\tAcc@1  65.62 ( 66.42)\tAcc@5  87.50 ( 91.39)\n","Epoch: [25][ 90/391]\tTime  0.100 ( 0.095)\tLoss 1.0429e+00 (1.1469e+00)\tAcc@1  70.31 ( 66.41)\tAcc@5  94.53 ( 91.43)\n","Epoch: [25][120/391]\tTime  0.102 ( 0.097)\tLoss 1.4558e+00 (1.1557e+00)\tAcc@1  61.72 ( 66.40)\tAcc@5  82.03 ( 91.34)\n","Epoch: [25][150/391]\tTime  0.091 ( 0.096)\tLoss 1.4730e+00 (1.1725e+00)\tAcc@1  60.94 ( 65.98)\tAcc@5  85.16 ( 91.10)\n","Epoch: [25][180/391]\tTime  0.091 ( 0.096)\tLoss 1.1193e+00 (1.1812e+00)\tAcc@1  68.75 ( 65.75)\tAcc@5  92.19 ( 91.03)\n","Epoch: [25][210/391]\tTime  0.090 ( 0.095)\tLoss 1.1713e+00 (1.1868e+00)\tAcc@1  64.84 ( 65.66)\tAcc@5  93.75 ( 90.93)\n","Epoch: [25][240/391]\tTime  0.094 ( 0.095)\tLoss 1.1445e+00 (1.1951e+00)\tAcc@1  65.62 ( 65.42)\tAcc@5  89.84 ( 90.84)\n","Epoch: [25][270/391]\tTime  0.095 ( 0.095)\tLoss 1.2022e+00 (1.2007e+00)\tAcc@1  66.41 ( 65.26)\tAcc@5  92.97 ( 90.78)\n","Epoch: [25][300/391]\tTime  0.095 ( 0.094)\tLoss 1.2197e+00 (1.2102e+00)\tAcc@1  60.94 ( 64.94)\tAcc@5  92.19 ( 90.64)\n","Epoch: [25][330/391]\tTime  0.090 ( 0.094)\tLoss 1.1953e+00 (1.2198e+00)\tAcc@1  68.75 ( 64.74)\tAcc@5  91.41 ( 90.54)\n","Epoch: [25][360/391]\tTime  0.094 ( 0.094)\tLoss 1.1444e+00 (1.2203e+00)\tAcc@1  64.06 ( 64.80)\tAcc@5  92.97 ( 90.51)\n","Epoch: [25][390/391]\tTime  0.082 ( 0.094)\tLoss 1.2465e+00 (1.2214e+00)\tAcc@1  62.50 ( 64.85)\tAcc@5  91.25 ( 90.49)\n","==> Train Accuracy: Acc@1 64.848 || Acc@5 90.486\n","==> Test Accuracy:  Acc@1 55.380 || Acc@5 84.290\n","==> 39.83 seconds to train this epoch\n","\n","\n","----- epoch: 26, lr: 0.1 -----\n","Epoch: [26][  0/391]\tTime  0.296 ( 0.296)\tLoss 1.0454e+00 (1.0454e+00)\tAcc@1  67.97 ( 67.97)\tAcc@5  95.31 ( 95.31)\n","Epoch: [26][ 30/391]\tTime  0.093 ( 0.100)\tLoss 1.0894e+00 (1.0886e+00)\tAcc@1  69.53 ( 68.93)\tAcc@5  92.97 ( 92.14)\n","Epoch: [26][ 60/391]\tTime  0.101 ( 0.096)\tLoss 1.2498e+00 (1.1139e+00)\tAcc@1  64.84 ( 67.97)\tAcc@5  89.84 ( 91.96)\n","Epoch: [26][ 90/391]\tTime  0.094 ( 0.095)\tLoss 1.1818e+00 (1.1363e+00)\tAcc@1  60.16 ( 67.16)\tAcc@5  94.53 ( 91.79)\n","Epoch: [26][120/391]\tTime  0.092 ( 0.094)\tLoss 1.2817e+00 (1.1548e+00)\tAcc@1  62.50 ( 66.55)\tAcc@5  89.84 ( 91.46)\n","Epoch: [26][150/391]\tTime  0.089 ( 0.094)\tLoss 1.1876e+00 (1.1603e+00)\tAcc@1  69.53 ( 66.44)\tAcc@5  92.19 ( 91.41)\n","Epoch: [26][180/391]\tTime  0.090 ( 0.094)\tLoss 1.3555e+00 (1.1763e+00)\tAcc@1  60.94 ( 66.07)\tAcc@5  85.94 ( 91.08)\n","Epoch: [26][210/391]\tTime  0.091 ( 0.094)\tLoss 1.1664e+00 (1.1743e+00)\tAcc@1  67.19 ( 66.16)\tAcc@5  90.62 ( 91.12)\n","Epoch: [26][240/391]\tTime  0.085 ( 0.094)\tLoss 1.3476e+00 (1.1860e+00)\tAcc@1  61.72 ( 65.94)\tAcc@5  89.06 ( 90.94)\n","Epoch: [26][270/391]\tTime  0.105 ( 0.093)\tLoss 1.1761e+00 (1.1926e+00)\tAcc@1  66.41 ( 65.74)\tAcc@5  89.06 ( 90.88)\n","Epoch: [26][300/391]\tTime  0.099 ( 0.093)\tLoss 1.5858e+00 (1.1973e+00)\tAcc@1  57.81 ( 65.64)\tAcc@5  86.72 ( 90.79)\n","Epoch: [26][330/391]\tTime  0.094 ( 0.093)\tLoss 1.2295e+00 (1.2012e+00)\tAcc@1  60.94 ( 65.54)\tAcc@5  90.62 ( 90.73)\n","Epoch: [26][360/391]\tTime  0.093 ( 0.093)\tLoss 1.0305e+00 (1.2047e+00)\tAcc@1  71.88 ( 65.42)\tAcc@5  92.97 ( 90.73)\n","Epoch: [26][390/391]\tTime  0.080 ( 0.093)\tLoss 1.1497e+00 (1.2067e+00)\tAcc@1  65.00 ( 65.35)\tAcc@5  95.00 ( 90.70)\n","==> Train Accuracy: Acc@1 65.354 || Acc@5 90.700\n","==> Test Accuracy:  Acc@1 57.560 || Acc@5 85.910\n","==> 39.47 seconds to train this epoch\n","\n","\n","----- epoch: 27, lr: 0.1 -----\n","Epoch: [27][  0/391]\tTime  0.325 ( 0.325)\tLoss 1.1231e+00 (1.1231e+00)\tAcc@1  71.09 ( 71.09)\tAcc@5  92.97 ( 92.97)\n","Epoch: [27][ 30/391]\tTime  0.099 ( 0.101)\tLoss 1.2800e+00 (1.1551e+00)\tAcc@1  60.16 ( 65.95)\tAcc@5  91.41 ( 91.53)\n","Epoch: [27][ 60/391]\tTime  0.093 ( 0.097)\tLoss 9.7876e-01 (1.1525e+00)\tAcc@1  69.53 ( 66.10)\tAcc@5  94.53 ( 91.48)\n","Epoch: [27][ 90/391]\tTime  0.100 ( 0.096)\tLoss 9.6788e-01 (1.1471e+00)\tAcc@1  72.66 ( 66.49)\tAcc@5  93.75 ( 91.41)\n","Epoch: [27][120/391]\tTime  0.064 ( 0.097)\tLoss 1.2084e+00 (1.1530e+00)\tAcc@1  69.53 ( 66.61)\tAcc@5  88.28 ( 91.34)\n","Epoch: [27][150/391]\tTime  0.088 ( 0.096)\tLoss 1.2517e+00 (1.1645e+00)\tAcc@1  60.16 ( 66.24)\tAcc@5  91.41 ( 91.36)\n","Epoch: [27][180/391]\tTime  0.092 ( 0.096)\tLoss 1.2533e+00 (1.1667e+00)\tAcc@1  60.16 ( 66.26)\tAcc@5  90.62 ( 91.29)\n","Epoch: [27][210/391]\tTime  0.094 ( 0.095)\tLoss 9.8608e-01 (1.1699e+00)\tAcc@1  70.31 ( 66.10)\tAcc@5  93.75 ( 91.28)\n","Epoch: [27][240/391]\tTime  0.093 ( 0.095)\tLoss 1.2090e+00 (1.1822e+00)\tAcc@1  64.06 ( 65.84)\tAcc@5  89.06 ( 91.07)\n","Epoch: [27][270/391]\tTime  0.103 ( 0.095)\tLoss 1.3012e+00 (1.1886e+00)\tAcc@1  63.28 ( 65.76)\tAcc@5  91.41 ( 90.98)\n","Epoch: [27][300/391]\tTime  0.092 ( 0.095)\tLoss 1.3427e+00 (1.1897e+00)\tAcc@1  58.59 ( 65.70)\tAcc@5  89.84 ( 90.97)\n","Epoch: [27][330/391]\tTime  0.093 ( 0.095)\tLoss 1.2451e+00 (1.1877e+00)\tAcc@1  59.38 ( 65.70)\tAcc@5  89.84 ( 90.97)\n","Epoch: [27][360/391]\tTime  0.105 ( 0.094)\tLoss 1.3484e+00 (1.1897e+00)\tAcc@1  63.28 ( 65.69)\tAcc@5  85.94 ( 90.87)\n","Epoch: [27][390/391]\tTime  0.083 ( 0.094)\tLoss 1.0584e+00 (1.1914e+00)\tAcc@1  67.50 ( 65.60)\tAcc@5  96.25 ( 90.89)\n","==> Train Accuracy: Acc@1 65.602 || Acc@5 90.890\n","==> Test Accuracy:  Acc@1 52.710 || Acc@5 81.680\n","==> 39.90 seconds to train this epoch\n","\n","\n","----- epoch: 28, lr: 0.1 -----\n","Epoch: [28][  0/391]\tTime  0.296 ( 0.296)\tLoss 1.1553e+00 (1.1553e+00)\tAcc@1  66.41 ( 66.41)\tAcc@5  92.97 ( 92.97)\n","Epoch: [28][ 30/391]\tTime  0.091 ( 0.099)\tLoss 1.0769e+00 (1.1047e+00)\tAcc@1  67.19 ( 68.60)\tAcc@5  90.62 ( 91.76)\n","Epoch: [28][ 60/391]\tTime  0.102 ( 0.096)\tLoss 1.2585e+00 (1.1358e+00)\tAcc@1  64.06 ( 67.29)\tAcc@5  89.84 ( 91.51)\n","Epoch: [28][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.1432e+00 (1.1412e+00)\tAcc@1  66.41 ( 67.02)\tAcc@5  91.41 ( 91.41)\n","Epoch: [28][120/391]\tTime  0.090 ( 0.095)\tLoss 1.1472e+00 (1.1494e+00)\tAcc@1  64.84 ( 66.61)\tAcc@5  89.84 ( 91.23)\n","Epoch: [28][150/391]\tTime  0.090 ( 0.094)\tLoss 1.2492e+00 (1.1641e+00)\tAcc@1  63.28 ( 66.20)\tAcc@5  86.72 ( 91.14)\n","Epoch: [28][180/391]\tTime  0.091 ( 0.094)\tLoss 1.3684e+00 (1.1761e+00)\tAcc@1  60.94 ( 66.02)\tAcc@5  87.50 ( 90.94)\n","Epoch: [28][210/391]\tTime  0.092 ( 0.094)\tLoss 1.0076e+00 (1.1784e+00)\tAcc@1  68.75 ( 65.84)\tAcc@5  92.97 ( 90.84)\n","Epoch: [28][240/391]\tTime  0.102 ( 0.094)\tLoss 1.1141e+00 (1.1799e+00)\tAcc@1  71.09 ( 65.78)\tAcc@5  93.75 ( 90.76)\n","Epoch: [28][270/391]\tTime  0.091 ( 0.094)\tLoss 1.2459e+00 (1.1781e+00)\tAcc@1  64.84 ( 65.89)\tAcc@5  89.06 ( 90.88)\n","Epoch: [28][300/391]\tTime  0.095 ( 0.094)\tLoss 1.1057e+00 (1.1795e+00)\tAcc@1  67.97 ( 65.90)\tAcc@5  90.62 ( 90.85)\n","Epoch: [28][330/391]\tTime  0.087 ( 0.094)\tLoss 1.4015e+00 (1.1825e+00)\tAcc@1  64.84 ( 65.78)\tAcc@5  83.59 ( 90.78)\n","Epoch: [28][360/391]\tTime  0.090 ( 0.093)\tLoss 1.3276e+00 (1.1877e+00)\tAcc@1  61.72 ( 65.62)\tAcc@5  87.50 ( 90.75)\n","Epoch: [28][390/391]\tTime  0.082 ( 0.093)\tLoss 1.3689e+00 (1.1921e+00)\tAcc@1  57.50 ( 65.53)\tAcc@5  88.75 ( 90.70)\n","==> Train Accuracy: Acc@1 65.534 || Acc@5 90.696\n","==> Test Accuracy:  Acc@1 54.190 || Acc@5 83.000\n","==> 39.55 seconds to train this epoch\n","\n","\n","----- epoch: 29, lr: 0.1 -----\n","Epoch: [29][  0/391]\tTime  0.294 ( 0.294)\tLoss 9.9111e-01 (9.9111e-01)\tAcc@1  69.53 ( 69.53)\tAcc@5  92.19 ( 92.19)\n","Epoch: [29][ 30/391]\tTime  0.093 ( 0.100)\tLoss 1.2508e+00 (1.0989e+00)\tAcc@1  63.28 ( 68.02)\tAcc@5  90.62 ( 92.46)\n","Epoch: [29][ 60/391]\tTime  0.092 ( 0.097)\tLoss 1.4963e+00 (1.1125e+00)\tAcc@1  57.03 ( 67.25)\tAcc@5  85.16 ( 92.24)\n","Epoch: [29][ 90/391]\tTime  0.087 ( 0.095)\tLoss 1.3657e+00 (1.1264e+00)\tAcc@1  58.59 ( 66.94)\tAcc@5  85.16 ( 91.88)\n","Epoch: [29][120/391]\tTime  0.094 ( 0.095)\tLoss 1.1835e+00 (1.1360e+00)\tAcc@1  64.84 ( 66.82)\tAcc@5  89.84 ( 91.74)\n","Epoch: [29][150/391]\tTime  0.094 ( 0.094)\tLoss 1.0673e+00 (1.1344e+00)\tAcc@1  66.41 ( 67.00)\tAcc@5  93.75 ( 91.73)\n","Epoch: [29][180/391]\tTime  0.091 ( 0.094)\tLoss 1.1048e+00 (1.1313e+00)\tAcc@1  67.19 ( 67.11)\tAcc@5  95.31 ( 91.77)\n","Epoch: [29][210/391]\tTime  0.100 ( 0.094)\tLoss 1.1085e+00 (1.1367e+00)\tAcc@1  64.84 ( 66.87)\tAcc@5  91.41 ( 91.77)\n","Epoch: [29][240/391]\tTime  0.092 ( 0.094)\tLoss 1.2278e+00 (1.1444e+00)\tAcc@1  68.75 ( 66.77)\tAcc@5  91.41 ( 91.67)\n","Epoch: [29][270/391]\tTime  0.093 ( 0.094)\tLoss 1.3145e+00 (1.1508e+00)\tAcc@1  60.94 ( 66.60)\tAcc@5  88.28 ( 91.57)\n","Epoch: [29][300/391]\tTime  0.088 ( 0.093)\tLoss 1.1890e+00 (1.1553e+00)\tAcc@1  67.19 ( 66.49)\tAcc@5  89.06 ( 91.48)\n","Epoch: [29][330/391]\tTime  0.093 ( 0.093)\tLoss 1.1494e+00 (1.1624e+00)\tAcc@1  64.06 ( 66.30)\tAcc@5  94.53 ( 91.37)\n","Epoch: [29][360/391]\tTime  0.092 ( 0.093)\tLoss 1.2279e+00 (1.1686e+00)\tAcc@1  62.50 ( 66.17)\tAcc@5  88.28 ( 91.27)\n","Epoch: [29][390/391]\tTime  0.082 ( 0.093)\tLoss 1.0666e+00 (1.1726e+00)\tAcc@1  65.00 ( 66.07)\tAcc@5  91.25 ( 91.17)\n","==> Train Accuracy: Acc@1 66.070 || Acc@5 91.170\n","==> Test Accuracy:  Acc@1 57.570 || Acc@5 85.480\n","==> 39.51 seconds to train this epoch\n","\n","\n","----- epoch: 30, lr: 0.1 -----\n","Epoch: [30][  0/391]\tTime  0.272 ( 0.272)\tLoss 1.1352e+00 (1.1352e+00)\tAcc@1  67.97 ( 67.97)\tAcc@5  89.06 ( 89.06)\n","Epoch: [30][ 30/391]\tTime  0.091 ( 0.099)\tLoss 1.0483e+00 (1.0856e+00)\tAcc@1  71.09 ( 68.75)\tAcc@5  92.19 ( 92.11)\n","Epoch: [30][ 60/391]\tTime  0.102 ( 0.096)\tLoss 1.2890e+00 (1.0807e+00)\tAcc@1  64.06 ( 68.99)\tAcc@5  92.97 ( 92.14)\n","Epoch: [30][ 90/391]\tTime  0.102 ( 0.095)\tLoss 1.0159e+00 (1.0979e+00)\tAcc@1  69.53 ( 68.33)\tAcc@5  96.88 ( 92.24)\n","Epoch: [30][120/391]\tTime  0.111 ( 0.097)\tLoss 1.0512e+00 (1.1126e+00)\tAcc@1  72.66 ( 67.97)\tAcc@5  92.19 ( 91.89)\n","Epoch: [30][150/391]\tTime  0.093 ( 0.096)\tLoss 1.1015e+00 (1.1211e+00)\tAcc@1  67.97 ( 67.68)\tAcc@5  88.28 ( 91.79)\n","Epoch: [30][180/391]\tTime  0.091 ( 0.095)\tLoss 1.2232e+00 (1.1232e+00)\tAcc@1  62.50 ( 67.58)\tAcc@5  89.84 ( 91.75)\n","Epoch: [30][210/391]\tTime  0.095 ( 0.095)\tLoss 1.3326e+00 (1.1352e+00)\tAcc@1  65.62 ( 67.17)\tAcc@5  88.28 ( 91.62)\n","Epoch: [30][240/391]\tTime  0.094 ( 0.095)\tLoss 1.2353e+00 (1.1469e+00)\tAcc@1  61.72 ( 67.01)\tAcc@5  90.62 ( 91.41)\n","Epoch: [30][270/391]\tTime  0.095 ( 0.095)\tLoss 1.3048e+00 (1.1489e+00)\tAcc@1  65.62 ( 66.97)\tAcc@5  86.72 ( 91.36)\n","Epoch: [30][300/391]\tTime  0.091 ( 0.094)\tLoss 1.2107e+00 (1.1547e+00)\tAcc@1  64.84 ( 66.73)\tAcc@5  92.19 ( 91.27)\n","Epoch: [30][330/391]\tTime  0.092 ( 0.094)\tLoss 1.2799e+00 (1.1594e+00)\tAcc@1  67.19 ( 66.67)\tAcc@5  86.72 ( 91.22)\n","Epoch: [30][360/391]\tTime  0.088 ( 0.094)\tLoss 1.2226e+00 (1.1611e+00)\tAcc@1  64.06 ( 66.59)\tAcc@5  88.28 ( 91.19)\n","Epoch: [30][390/391]\tTime  0.081 ( 0.094)\tLoss 1.2146e+00 (1.1649e+00)\tAcc@1  65.00 ( 66.50)\tAcc@5  95.00 ( 91.11)\n","==> Train Accuracy: Acc@1 66.498 || Acc@5 91.108\n","==> Test Accuracy:  Acc@1 50.480 || Acc@5 79.730\n","==> 39.78 seconds to train this epoch\n","\n","\n","----- epoch: 31, lr: 0.1 -----\n","Epoch: [31][  0/391]\tTime  0.313 ( 0.313)\tLoss 1.1182e+00 (1.1182e+00)\tAcc@1  68.75 ( 68.75)\tAcc@5  93.75 ( 93.75)\n","Epoch: [31][ 30/391]\tTime  0.100 ( 0.100)\tLoss 1.2400e+00 (1.0378e+00)\tAcc@1  62.50 ( 70.26)\tAcc@5  91.41 ( 92.94)\n","Epoch: [31][ 60/391]\tTime  0.090 ( 0.096)\tLoss 1.0732e+00 (1.0625e+00)\tAcc@1  68.75 ( 69.22)\tAcc@5  92.19 ( 92.98)\n","Epoch: [31][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.0411e+00 (1.0771e+00)\tAcc@1  70.31 ( 68.92)\tAcc@5  92.97 ( 92.70)\n","Epoch: [31][120/391]\tTime  0.096 ( 0.094)\tLoss 1.1120e+00 (1.0936e+00)\tAcc@1  66.41 ( 68.18)\tAcc@5  92.19 ( 92.41)\n","Epoch: [31][150/391]\tTime  0.089 ( 0.094)\tLoss 1.0955e+00 (1.1074e+00)\tAcc@1  70.31 ( 67.81)\tAcc@5  93.75 ( 92.12)\n","Epoch: [31][180/391]\tTime  0.092 ( 0.094)\tLoss 1.2724e+00 (1.1182e+00)\tAcc@1  60.94 ( 67.55)\tAcc@5  88.28 ( 91.82)\n","Epoch: [31][210/391]\tTime  0.092 ( 0.093)\tLoss 1.1956e+00 (1.1281e+00)\tAcc@1  65.62 ( 67.25)\tAcc@5  90.62 ( 91.63)\n","Epoch: [31][240/391]\tTime  0.093 ( 0.093)\tLoss 1.0501e+00 (1.1260e+00)\tAcc@1  71.88 ( 67.32)\tAcc@5  92.97 ( 91.67)\n","Epoch: [31][270/391]\tTime  0.090 ( 0.093)\tLoss 1.0503e+00 (1.1327e+00)\tAcc@1  67.97 ( 67.04)\tAcc@5  91.41 ( 91.63)\n","Epoch: [31][300/391]\tTime  0.089 ( 0.093)\tLoss 1.2539e+00 (1.1440e+00)\tAcc@1  64.84 ( 66.87)\tAcc@5  89.06 ( 91.52)\n","Epoch: [31][330/391]\tTime  0.092 ( 0.093)\tLoss 1.3393e+00 (1.1457e+00)\tAcc@1  64.84 ( 66.86)\tAcc@5  88.28 ( 91.47)\n","Epoch: [31][360/391]\tTime  0.091 ( 0.093)\tLoss 1.3430e+00 (1.1547e+00)\tAcc@1  60.94 ( 66.68)\tAcc@5  84.38 ( 91.33)\n","Epoch: [31][390/391]\tTime  0.082 ( 0.093)\tLoss 1.0038e+00 (1.1601e+00)\tAcc@1  71.25 ( 66.57)\tAcc@5  95.00 ( 91.24)\n","==> Train Accuracy: Acc@1 66.572 || Acc@5 91.240\n","==> Test Accuracy:  Acc@1 58.230 || Acc@5 85.470\n","==> 39.54 seconds to train this epoch\n","\n","\n","----- epoch: 32, lr: 0.1 -----\n","Epoch: [32][  0/391]\tTime  0.305 ( 0.305)\tLoss 9.4931e-01 (9.4931e-01)\tAcc@1  71.88 ( 71.88)\tAcc@5  92.97 ( 92.97)\n","Epoch: [32][ 30/391]\tTime  0.081 ( 0.100)\tLoss 9.7862e-01 (1.0323e+00)\tAcc@1  73.44 ( 69.15)\tAcc@5  94.53 ( 93.42)\n","Epoch: [32][ 60/391]\tTime  0.092 ( 0.096)\tLoss 1.2963e+00 (1.0428e+00)\tAcc@1  62.50 ( 69.35)\tAcc@5  88.28 ( 93.06)\n","Epoch: [32][ 90/391]\tTime  0.124 ( 0.096)\tLoss 1.1488e+00 (1.0488e+00)\tAcc@1  63.28 ( 69.22)\tAcc@5  93.75 ( 92.99)\n","Epoch: [32][120/391]\tTime  0.096 ( 0.097)\tLoss 1.2883e+00 (1.0663e+00)\tAcc@1  66.41 ( 68.80)\tAcc@5  87.50 ( 92.62)\n","Epoch: [32][150/391]\tTime  0.093 ( 0.096)\tLoss 1.1439e+00 (1.1018e+00)\tAcc@1  67.19 ( 67.82)\tAcc@5  91.41 ( 92.18)\n","Epoch: [32][180/391]\tTime  0.095 ( 0.096)\tLoss 9.8830e-01 (1.1091e+00)\tAcc@1  75.00 ( 67.78)\tAcc@5  92.97 ( 92.01)\n","Epoch: [32][210/391]\tTime  0.093 ( 0.095)\tLoss 1.2463e+00 (1.1204e+00)\tAcc@1  63.28 ( 67.54)\tAcc@5  90.62 ( 91.86)\n","Epoch: [32][240/391]\tTime  0.090 ( 0.095)\tLoss 1.1144e+00 (1.1288e+00)\tAcc@1  67.97 ( 67.38)\tAcc@5  91.41 ( 91.68)\n","Epoch: [32][270/391]\tTime  0.092 ( 0.095)\tLoss 1.1287e+00 (1.1357e+00)\tAcc@1  67.19 ( 67.14)\tAcc@5  95.31 ( 91.68)\n","Epoch: [32][300/391]\tTime  0.096 ( 0.094)\tLoss 1.1785e+00 (1.1437e+00)\tAcc@1  69.53 ( 66.91)\tAcc@5  86.72 ( 91.54)\n","Epoch: [32][330/391]\tTime  0.092 ( 0.094)\tLoss 1.1417e+00 (1.1436e+00)\tAcc@1  68.75 ( 66.90)\tAcc@5  90.62 ( 91.58)\n","Epoch: [32][360/391]\tTime  0.092 ( 0.094)\tLoss 1.2266e+00 (1.1475e+00)\tAcc@1  65.62 ( 66.82)\tAcc@5  92.19 ( 91.49)\n","Epoch: [32][390/391]\tTime  0.083 ( 0.094)\tLoss 1.2773e+00 (1.1509e+00)\tAcc@1  60.00 ( 66.73)\tAcc@5  90.00 ( 91.44)\n","==> Train Accuracy: Acc@1 66.732 || Acc@5 91.438\n","==> Test Accuracy:  Acc@1 55.020 || Acc@5 81.770\n","==> 39.80 seconds to train this epoch\n","\n","\n","----- epoch: 33, lr: 0.1 -----\n","Epoch: [33][  0/391]\tTime  0.284 ( 0.284)\tLoss 8.0371e-01 (8.0371e-01)\tAcc@1  75.00 ( 75.00)\tAcc@5  96.09 ( 96.09)\n","Epoch: [33][ 30/391]\tTime  0.094 ( 0.099)\tLoss 1.0596e+00 (1.0774e+00)\tAcc@1  70.31 ( 67.67)\tAcc@5  91.41 ( 91.94)\n","Epoch: [33][ 60/391]\tTime  0.093 ( 0.096)\tLoss 1.0312e+00 (1.0759e+00)\tAcc@1  68.75 ( 68.56)\tAcc@5  93.75 ( 92.26)\n","Epoch: [33][ 90/391]\tTime  0.094 ( 0.095)\tLoss 1.0980e+00 (1.0767e+00)\tAcc@1  67.19 ( 68.36)\tAcc@5  90.62 ( 92.32)\n","Epoch: [33][120/391]\tTime  0.093 ( 0.095)\tLoss 8.3872e-01 (1.0884e+00)\tAcc@1  72.66 ( 68.25)\tAcc@5  96.88 ( 92.21)\n","Epoch: [33][150/391]\tTime  0.090 ( 0.094)\tLoss 1.2396e+00 (1.1048e+00)\tAcc@1  60.94 ( 67.75)\tAcc@5  89.84 ( 92.12)\n","Epoch: [33][180/391]\tTime  0.089 ( 0.094)\tLoss 9.1706e-01 (1.1146e+00)\tAcc@1  75.78 ( 67.59)\tAcc@5  93.75 ( 91.99)\n","Epoch: [33][210/391]\tTime  0.091 ( 0.094)\tLoss 1.1859e+00 (1.1252e+00)\tAcc@1  65.62 ( 67.34)\tAcc@5  92.19 ( 91.85)\n","Epoch: [33][240/391]\tTime  0.097 ( 0.094)\tLoss 1.1873e+00 (1.1336e+00)\tAcc@1  64.84 ( 67.20)\tAcc@5  93.75 ( 91.76)\n","Epoch: [33][270/391]\tTime  0.095 ( 0.094)\tLoss 1.2253e+00 (1.1368e+00)\tAcc@1  67.19 ( 67.15)\tAcc@5  90.62 ( 91.69)\n","Epoch: [33][300/391]\tTime  0.095 ( 0.093)\tLoss 1.0416e+00 (1.1408e+00)\tAcc@1  72.66 ( 67.03)\tAcc@5  90.62 ( 91.58)\n","Epoch: [33][330/391]\tTime  0.090 ( 0.093)\tLoss 1.1615e+00 (1.1401e+00)\tAcc@1  64.06 ( 66.93)\tAcc@5  89.06 ( 91.63)\n","Epoch: [33][360/391]\tTime  0.100 ( 0.093)\tLoss 1.1961e+00 (1.1426e+00)\tAcc@1  67.19 ( 66.87)\tAcc@5  91.41 ( 91.57)\n","Epoch: [33][390/391]\tTime  0.083 ( 0.093)\tLoss 1.2546e+00 (1.1496e+00)\tAcc@1  61.25 ( 66.66)\tAcc@5  91.25 ( 91.45)\n","==> Train Accuracy: Acc@1 66.662 || Acc@5 91.454\n","==> Test Accuracy:  Acc@1 59.930 || Acc@5 86.270\n","==> 39.55 seconds to train this epoch\n","\n","\n","----- epoch: 34, lr: 0.1 -----\n","Epoch: [34][  0/391]\tTime  0.301 ( 0.301)\tLoss 1.0980e+00 (1.0980e+00)\tAcc@1  67.97 ( 67.97)\tAcc@5  91.41 ( 91.41)\n","Epoch: [34][ 30/391]\tTime  0.092 ( 0.100)\tLoss 1.0533e+00 (1.0268e+00)\tAcc@1  66.41 ( 70.04)\tAcc@5  93.75 ( 93.20)\n","Epoch: [34][ 60/391]\tTime  0.094 ( 0.097)\tLoss 1.1579e+00 (1.0375e+00)\tAcc@1  64.84 ( 69.63)\tAcc@5  89.84 ( 92.79)\n","Epoch: [34][ 90/391]\tTime  0.097 ( 0.096)\tLoss 1.0456e+00 (1.0506e+00)\tAcc@1  71.09 ( 69.50)\tAcc@5  91.41 ( 92.60)\n","Epoch: [34][120/391]\tTime  0.089 ( 0.098)\tLoss 1.1251e+00 (1.0664e+00)\tAcc@1  71.88 ( 69.02)\tAcc@5  92.19 ( 92.55)\n","Epoch: [34][150/391]\tTime  0.094 ( 0.097)\tLoss 9.6163e-01 (1.0826e+00)\tAcc@1  75.00 ( 68.79)\tAcc@5  93.75 ( 92.31)\n","Epoch: [34][180/391]\tTime  0.093 ( 0.096)\tLoss 1.1682e+00 (1.0855e+00)\tAcc@1  65.62 ( 68.66)\tAcc@5  91.41 ( 92.30)\n","Epoch: [34][210/391]\tTime  0.089 ( 0.095)\tLoss 9.6462e-01 (1.0948e+00)\tAcc@1  72.66 ( 68.38)\tAcc@5  94.53 ( 92.09)\n","Epoch: [34][240/391]\tTime  0.084 ( 0.095)\tLoss 1.0130e+00 (1.1035e+00)\tAcc@1  71.09 ( 68.24)\tAcc@5  91.41 ( 91.92)\n","Epoch: [34][270/391]\tTime  0.094 ( 0.095)\tLoss 1.2269e+00 (1.1111e+00)\tAcc@1  66.41 ( 68.00)\tAcc@5  91.41 ( 91.87)\n","Epoch: [34][300/391]\tTime  0.090 ( 0.095)\tLoss 1.3444e+00 (1.1220e+00)\tAcc@1  62.50 ( 67.67)\tAcc@5  89.84 ( 91.73)\n","Epoch: [34][330/391]\tTime  0.087 ( 0.094)\tLoss 1.1378e+00 (1.1315e+00)\tAcc@1  71.88 ( 67.42)\tAcc@5  92.19 ( 91.62)\n","Epoch: [34][360/391]\tTime  0.094 ( 0.094)\tLoss 1.1013e+00 (1.1337e+00)\tAcc@1  66.41 ( 67.36)\tAcc@5  92.19 ( 91.58)\n","Epoch: [34][390/391]\tTime  0.082 ( 0.094)\tLoss 1.1004e+00 (1.1382e+00)\tAcc@1  70.00 ( 67.28)\tAcc@5  91.25 ( 91.47)\n","==> Train Accuracy: Acc@1 67.280 || Acc@5 91.470\n","==> Test Accuracy:  Acc@1 58.300 || Acc@5 85.850\n","==> 39.88 seconds to train this epoch\n","\n","\n","----- epoch: 35, lr: 0.1 -----\n","Epoch: [35][  0/391]\tTime  0.281 ( 0.281)\tLoss 9.1700e-01 (9.1700e-01)\tAcc@1  72.66 ( 72.66)\tAcc@5  96.09 ( 96.09)\n","Epoch: [35][ 30/391]\tTime  0.091 ( 0.099)\tLoss 1.0929e+00 (1.0374e+00)\tAcc@1  69.53 ( 69.38)\tAcc@5  91.41 ( 92.99)\n","Epoch: [35][ 60/391]\tTime  0.091 ( 0.096)\tLoss 1.1243e+00 (1.0406e+00)\tAcc@1  68.75 ( 69.49)\tAcc@5  91.41 ( 92.78)\n","Epoch: [35][ 90/391]\tTime  0.105 ( 0.095)\tLoss 1.0675e+00 (1.0628e+00)\tAcc@1  65.62 ( 68.90)\tAcc@5  91.41 ( 92.44)\n","Epoch: [35][120/391]\tTime  0.091 ( 0.094)\tLoss 1.0934e+00 (1.0656e+00)\tAcc@1  67.19 ( 68.65)\tAcc@5  93.75 ( 92.43)\n","Epoch: [35][150/391]\tTime  0.089 ( 0.094)\tLoss 1.2154e+00 (1.0833e+00)\tAcc@1  63.28 ( 68.30)\tAcc@5  92.19 ( 92.17)\n","Epoch: [35][180/391]\tTime  0.092 ( 0.094)\tLoss 1.0519e+00 (1.0877e+00)\tAcc@1  66.41 ( 68.19)\tAcc@5  94.53 ( 92.18)\n","Epoch: [35][210/391]\tTime  0.088 ( 0.094)\tLoss 1.2688e+00 (1.0947e+00)\tAcc@1  61.72 ( 67.99)\tAcc@5  89.06 ( 92.09)\n","Epoch: [35][240/391]\tTime  0.091 ( 0.093)\tLoss 1.0360e+00 (1.0977e+00)\tAcc@1  71.09 ( 67.96)\tAcc@5  94.53 ( 92.04)\n","Epoch: [35][270/391]\tTime  0.093 ( 0.093)\tLoss 1.2450e+00 (1.1065e+00)\tAcc@1  64.84 ( 67.68)\tAcc@5  88.28 ( 91.95)\n","Epoch: [35][300/391]\tTime  0.091 ( 0.093)\tLoss 1.0327e+00 (1.1102e+00)\tAcc@1  69.53 ( 67.56)\tAcc@5  92.97 ( 91.88)\n","Epoch: [35][330/391]\tTime  0.090 ( 0.093)\tLoss 1.1166e+00 (1.1173e+00)\tAcc@1  68.75 ( 67.47)\tAcc@5  95.31 ( 91.82)\n","Epoch: [35][360/391]\tTime  0.092 ( 0.093)\tLoss 1.1983e+00 (1.1219e+00)\tAcc@1  63.28 ( 67.42)\tAcc@5  92.97 ( 91.76)\n","Epoch: [35][390/391]\tTime  0.083 ( 0.093)\tLoss 1.2989e+00 (1.1251e+00)\tAcc@1  67.50 ( 67.37)\tAcc@5  88.75 ( 91.72)\n","==> Train Accuracy: Acc@1 67.368 || Acc@5 91.720\n","==> Test Accuracy:  Acc@1 61.140 || Acc@5 87.360\n","==> 39.50 seconds to train this epoch\n","\n","\n","----- epoch: 36, lr: 0.1 -----\n","Epoch: [36][  0/391]\tTime  0.288 ( 0.288)\tLoss 9.3395e-01 (9.3395e-01)\tAcc@1  71.09 ( 71.09)\tAcc@5  94.53 ( 94.53)\n","Epoch: [36][ 30/391]\tTime  0.096 ( 0.099)\tLoss 8.6172e-01 (9.9791e-01)\tAcc@1  75.00 ( 70.87)\tAcc@5  93.75 ( 93.15)\n","Epoch: [36][ 60/391]\tTime  0.091 ( 0.096)\tLoss 1.1638e+00 (1.0327e+00)\tAcc@1  65.62 ( 70.25)\tAcc@5  90.62 ( 92.92)\n","Epoch: [36][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.1337e+00 (1.0521e+00)\tAcc@1  67.19 ( 69.51)\tAcc@5  91.41 ( 92.89)\n","Epoch: [36][120/391]\tTime  0.090 ( 0.097)\tLoss 8.3131e-01 (1.0579e+00)\tAcc@1  76.56 ( 69.31)\tAcc@5  98.44 ( 92.75)\n","Epoch: [36][150/391]\tTime  0.092 ( 0.096)\tLoss 1.2498e+00 (1.0712e+00)\tAcc@1  56.25 ( 68.89)\tAcc@5  92.19 ( 92.54)\n","Epoch: [36][180/391]\tTime  0.094 ( 0.096)\tLoss 1.2340e+00 (1.0903e+00)\tAcc@1  64.84 ( 68.34)\tAcc@5  90.62 ( 92.34)\n","Epoch: [36][210/391]\tTime  0.092 ( 0.095)\tLoss 1.2061e+00 (1.1001e+00)\tAcc@1  67.19 ( 68.08)\tAcc@5  90.62 ( 92.17)\n","Epoch: [36][240/391]\tTime  0.090 ( 0.095)\tLoss 9.9278e-01 (1.1064e+00)\tAcc@1  67.19 ( 67.85)\tAcc@5  93.75 ( 92.13)\n","Epoch: [36][270/391]\tTime  0.090 ( 0.095)\tLoss 1.1408e+00 (1.1182e+00)\tAcc@1  66.41 ( 67.56)\tAcc@5  89.06 ( 91.96)\n","Epoch: [36][300/391]\tTime  0.091 ( 0.095)\tLoss 1.2062e+00 (1.1247e+00)\tAcc@1  66.41 ( 67.47)\tAcc@5  89.84 ( 91.90)\n","Epoch: [36][330/391]\tTime  0.088 ( 0.094)\tLoss 9.5756e-01 (1.1288e+00)\tAcc@1  78.12 ( 67.35)\tAcc@5  92.19 ( 91.82)\n","Epoch: [36][360/391]\tTime  0.107 ( 0.094)\tLoss 1.0911e+00 (1.1312e+00)\tAcc@1  69.53 ( 67.27)\tAcc@5  92.97 ( 91.81)\n","Epoch: [36][390/391]\tTime  0.082 ( 0.094)\tLoss 1.2370e+00 (1.1342e+00)\tAcc@1  65.00 ( 67.23)\tAcc@5  87.50 ( 91.73)\n","==> Train Accuracy: Acc@1 67.228 || Acc@5 91.734\n","==> Test Accuracy:  Acc@1 58.840 || Acc@5 86.640\n","==> 39.86 seconds to train this epoch\n","\n","\n","----- epoch: 37, lr: 0.1 -----\n","Epoch: [37][  0/391]\tTime  0.278 ( 0.278)\tLoss 1.0865e+00 (1.0865e+00)\tAcc@1  66.41 ( 66.41)\tAcc@5  93.75 ( 93.75)\n","Epoch: [37][ 30/391]\tTime  0.097 ( 0.099)\tLoss 8.6450e-01 (9.9256e-01)\tAcc@1  70.31 ( 70.97)\tAcc@5  94.53 ( 93.72)\n","Epoch: [37][ 60/391]\tTime  0.098 ( 0.096)\tLoss 1.1768e+00 (1.0120e+00)\tAcc@1  72.66 ( 70.30)\tAcc@5  89.84 ( 93.31)\n","Epoch: [37][ 90/391]\tTime  0.091 ( 0.095)\tLoss 9.0471e-01 (1.0241e+00)\tAcc@1  75.00 ( 70.20)\tAcc@5  93.75 ( 92.94)\n","Epoch: [37][120/391]\tTime  0.092 ( 0.094)\tLoss 1.0944e+00 (1.0548e+00)\tAcc@1  71.09 ( 69.58)\tAcc@5  89.84 ( 92.51)\n","Epoch: [37][150/391]\tTime  0.092 ( 0.094)\tLoss 1.3314e+00 (1.0621e+00)\tAcc@1  64.06 ( 69.36)\tAcc@5  89.06 ( 92.58)\n","Epoch: [37][180/391]\tTime  0.091 ( 0.094)\tLoss 1.2779e+00 (1.0760e+00)\tAcc@1  64.06 ( 68.88)\tAcc@5  89.06 ( 92.46)\n","Epoch: [37][210/391]\tTime  0.097 ( 0.094)\tLoss 9.5451e-01 (1.0903e+00)\tAcc@1  67.97 ( 68.42)\tAcc@5  96.88 ( 92.30)\n","Epoch: [37][240/391]\tTime  0.083 ( 0.093)\tLoss 1.2120e+00 (1.0974e+00)\tAcc@1  63.28 ( 68.14)\tAcc@5  88.28 ( 92.19)\n","Epoch: [37][270/391]\tTime  0.092 ( 0.093)\tLoss 1.0600e+00 (1.1050e+00)\tAcc@1  69.53 ( 67.93)\tAcc@5  92.97 ( 92.04)\n","Epoch: [37][300/391]\tTime  0.091 ( 0.093)\tLoss 9.9805e-01 (1.1054e+00)\tAcc@1  68.75 ( 67.99)\tAcc@5  94.53 ( 92.00)\n","Epoch: [37][330/391]\tTime  0.091 ( 0.093)\tLoss 9.5879e-01 (1.1071e+00)\tAcc@1  69.53 ( 67.97)\tAcc@5  94.53 ( 91.97)\n","Epoch: [37][360/391]\tTime  0.092 ( 0.093)\tLoss 1.4613e+00 (1.1136e+00)\tAcc@1  61.72 ( 67.81)\tAcc@5  88.28 ( 91.85)\n","Epoch: [37][390/391]\tTime  0.083 ( 0.093)\tLoss 1.2017e+00 (1.1170e+00)\tAcc@1  68.75 ( 67.72)\tAcc@5  93.75 ( 91.78)\n","==> Train Accuracy: Acc@1 67.720 || Acc@5 91.778\n","==> Test Accuracy:  Acc@1 53.790 || Acc@5 83.600\n","==> 39.46 seconds to train this epoch\n","\n","\n","----- epoch: 38, lr: 0.1 -----\n","Epoch: [38][  0/391]\tTime  0.275 ( 0.275)\tLoss 9.6527e-01 (9.6527e-01)\tAcc@1  71.88 ( 71.88)\tAcc@5  93.75 ( 93.75)\n","Epoch: [38][ 30/391]\tTime  0.091 ( 0.100)\tLoss 1.0099e+00 (1.0252e+00)\tAcc@1  69.53 ( 69.23)\tAcc@5  95.31 ( 93.40)\n","Epoch: [38][ 60/391]\tTime  0.093 ( 0.097)\tLoss 8.8856e-01 (1.0293e+00)\tAcc@1  77.34 ( 69.57)\tAcc@5  92.97 ( 92.93)\n","Epoch: [38][ 90/391]\tTime  0.086 ( 0.095)\tLoss 1.4601e+00 (1.0517e+00)\tAcc@1  57.03 ( 68.93)\tAcc@5  86.72 ( 92.72)\n","Epoch: [38][120/391]\tTime  0.093 ( 0.095)\tLoss 1.1826e+00 (1.0649e+00)\tAcc@1  65.62 ( 68.44)\tAcc@5  92.97 ( 92.63)\n","Epoch: [38][150/391]\tTime  0.093 ( 0.094)\tLoss 1.1336e+00 (1.0722e+00)\tAcc@1  65.62 ( 68.28)\tAcc@5  93.75 ( 92.59)\n","Epoch: [38][180/391]\tTime  0.095 ( 0.094)\tLoss 1.1831e+00 (1.0818e+00)\tAcc@1  67.97 ( 67.94)\tAcc@5  90.62 ( 92.41)\n","Epoch: [38][210/391]\tTime  0.097 ( 0.094)\tLoss 1.3095e+00 (1.0911e+00)\tAcc@1  63.28 ( 67.76)\tAcc@5  91.41 ( 92.30)\n","Epoch: [38][240/391]\tTime  0.093 ( 0.094)\tLoss 9.5129e-01 (1.0945e+00)\tAcc@1  72.66 ( 67.87)\tAcc@5  94.53 ( 92.22)\n","Epoch: [38][270/391]\tTime  0.087 ( 0.094)\tLoss 1.0943e+00 (1.1019e+00)\tAcc@1  73.44 ( 67.74)\tAcc@5  92.19 ( 92.12)\n","Epoch: [38][300/391]\tTime  0.090 ( 0.094)\tLoss 1.1738e+00 (1.1072e+00)\tAcc@1  68.75 ( 67.62)\tAcc@5  90.62 ( 92.04)\n","Epoch: [38][330/391]\tTime  0.093 ( 0.093)\tLoss 1.0804e+00 (1.1092e+00)\tAcc@1  67.19 ( 67.69)\tAcc@5  92.97 ( 91.98)\n","Epoch: [38][360/391]\tTime  0.091 ( 0.093)\tLoss 1.1225e+00 (1.1124e+00)\tAcc@1  68.75 ( 67.61)\tAcc@5  92.19 ( 91.98)\n","Epoch: [38][390/391]\tTime  0.082 ( 0.093)\tLoss 1.0001e+00 (1.1155e+00)\tAcc@1  71.25 ( 67.57)\tAcc@5  93.75 ( 91.95)\n","==> Train Accuracy: Acc@1 67.566 || Acc@5 91.952\n","==> Test Accuracy:  Acc@1 59.760 || Acc@5 86.560\n","==> 39.56 seconds to train this epoch\n","\n","\n","----- epoch: 39, lr: 0.1 -----\n","Epoch: [39][  0/391]\tTime  0.278 ( 0.278)\tLoss 1.1454e+00 (1.1454e+00)\tAcc@1  65.62 ( 65.62)\tAcc@5  92.97 ( 92.97)\n","Epoch: [39][ 30/391]\tTime  0.094 ( 0.100)\tLoss 1.0014e+00 (1.0364e+00)\tAcc@1  73.44 ( 69.78)\tAcc@5  93.75 ( 93.42)\n","Epoch: [39][ 60/391]\tTime  0.090 ( 0.096)\tLoss 9.6064e-01 (1.0411e+00)\tAcc@1  73.44 ( 69.86)\tAcc@5  96.88 ( 93.17)\n","Epoch: [39][ 90/391]\tTime  0.089 ( 0.095)\tLoss 1.0450e+00 (1.0530e+00)\tAcc@1  72.66 ( 69.63)\tAcc@5  90.62 ( 92.99)\n","Epoch: [39][120/391]\tTime  0.095 ( 0.095)\tLoss 1.0567e+00 (1.0699e+00)\tAcc@1  72.66 ( 69.27)\tAcc@5  93.75 ( 92.75)\n","Epoch: [39][150/391]\tTime  0.090 ( 0.094)\tLoss 1.0675e+00 (1.0756e+00)\tAcc@1  68.75 ( 68.91)\tAcc@5  92.97 ( 92.63)\n","Epoch: [39][180/391]\tTime  0.093 ( 0.094)\tLoss 1.2815e+00 (1.0880e+00)\tAcc@1  65.62 ( 68.55)\tAcc@5  87.50 ( 92.46)\n","Epoch: [39][210/391]\tTime  0.093 ( 0.094)\tLoss 9.2397e-01 (1.0954e+00)\tAcc@1  73.44 ( 68.28)\tAcc@5  93.75 ( 92.28)\n","Epoch: [39][240/391]\tTime  0.092 ( 0.094)\tLoss 1.1143e+00 (1.1012e+00)\tAcc@1  67.19 ( 67.99)\tAcc@5  92.97 ( 92.21)\n","Epoch: [39][270/391]\tTime  0.094 ( 0.094)\tLoss 1.0911e+00 (1.1049e+00)\tAcc@1  68.75 ( 67.91)\tAcc@5  92.19 ( 92.18)\n","Epoch: [39][300/391]\tTime  0.092 ( 0.093)\tLoss 1.0817e+00 (1.1097e+00)\tAcc@1  70.31 ( 67.76)\tAcc@5  95.31 ( 92.18)\n","Epoch: [39][330/391]\tTime  0.092 ( 0.093)\tLoss 1.1705e+00 (1.1124e+00)\tAcc@1  69.53 ( 67.72)\tAcc@5  88.28 ( 92.07)\n","Epoch: [39][360/391]\tTime  0.092 ( 0.093)\tLoss 1.2272e+00 (1.1159e+00)\tAcc@1  67.19 ( 67.64)\tAcc@5  89.84 ( 92.00)\n","Epoch: [39][390/391]\tTime  0.082 ( 0.093)\tLoss 1.0594e+00 (1.1189e+00)\tAcc@1  67.50 ( 67.52)\tAcc@5  91.25 ( 91.95)\n","==> Train Accuracy: Acc@1 67.524 || Acc@5 91.950\n","==> Test Accuracy:  Acc@1 54.550 || Acc@5 84.260\n","==> 39.57 seconds to train this epoch\n","\n","\n","----- epoch: 40, lr: 0.1 -----\n","Epoch: [40][  0/391]\tTime  0.296 ( 0.296)\tLoss 1.1589e+00 (1.1589e+00)\tAcc@1  68.75 ( 68.75)\tAcc@5  89.84 ( 89.84)\n","Epoch: [40][ 30/391]\tTime  0.093 ( 0.100)\tLoss 1.0766e+00 (1.0265e+00)\tAcc@1  71.09 ( 70.97)\tAcc@5  89.84 ( 92.82)\n","Epoch: [40][ 60/391]\tTime  0.096 ( 0.096)\tLoss 1.0673e+00 (1.0449e+00)\tAcc@1  70.31 ( 69.79)\tAcc@5  93.75 ( 92.62)\n","Epoch: [40][ 90/391]\tTime  0.094 ( 0.095)\tLoss 1.0208e+00 (1.0491e+00)\tAcc@1  68.75 ( 69.63)\tAcc@5  92.97 ( 92.47)\n","Epoch: [40][120/391]\tTime  0.090 ( 0.094)\tLoss 9.4069e-01 (1.0528e+00)\tAcc@1  75.00 ( 69.38)\tAcc@5  92.97 ( 92.47)\n","Epoch: [40][150/391]\tTime  0.098 ( 0.094)\tLoss 9.0456e-01 (1.0570e+00)\tAcc@1  73.44 ( 69.24)\tAcc@5  93.75 ( 92.47)\n","Epoch: [40][180/391]\tTime  0.092 ( 0.094)\tLoss 1.2274e+00 (1.0612e+00)\tAcc@1  62.50 ( 69.01)\tAcc@5  92.19 ( 92.38)\n","Epoch: [40][210/391]\tTime  0.093 ( 0.094)\tLoss 1.1210e+00 (1.0661e+00)\tAcc@1  65.62 ( 68.84)\tAcc@5  92.19 ( 92.39)\n","Epoch: [40][240/391]\tTime  0.094 ( 0.093)\tLoss 1.3130e+00 (1.0737e+00)\tAcc@1  60.94 ( 68.69)\tAcc@5  91.41 ( 92.36)\n","Epoch: [40][270/391]\tTime  0.090 ( 0.093)\tLoss 1.0951e+00 (1.0864e+00)\tAcc@1  65.62 ( 68.43)\tAcc@5  92.97 ( 92.18)\n","Epoch: [40][300/391]\tTime  0.094 ( 0.093)\tLoss 1.0761e+00 (1.0944e+00)\tAcc@1  75.00 ( 68.29)\tAcc@5  91.41 ( 92.05)\n","Epoch: [40][330/391]\tTime  0.106 ( 0.093)\tLoss 1.1986e+00 (1.0988e+00)\tAcc@1  57.03 ( 68.17)\tAcc@5  93.75 ( 91.99)\n","Epoch: [40][360/391]\tTime  0.100 ( 0.093)\tLoss 1.0417e+00 (1.1048e+00)\tAcc@1  65.62 ( 68.01)\tAcc@5  91.41 ( 91.93)\n","Epoch: [40][390/391]\tTime  0.082 ( 0.093)\tLoss 1.0491e+00 (1.1079e+00)\tAcc@1  71.25 ( 67.96)\tAcc@5  95.00 ( 91.91)\n","==> Train Accuracy: Acc@1 67.960 || Acc@5 91.908\n","==> Test Accuracy:  Acc@1 60.960 || Acc@5 86.850\n","==> 39.43 seconds to train this epoch\n","\n","\n","----- epoch: 41, lr: 0.1 -----\n","Epoch: [41][  0/391]\tTime  0.296 ( 0.296)\tLoss 9.1705e-01 (9.1705e-01)\tAcc@1  74.22 ( 74.22)\tAcc@5  95.31 ( 95.31)\n","Epoch: [41][ 30/391]\tTime  0.090 ( 0.100)\tLoss 1.1369e+00 (1.0314e+00)\tAcc@1  72.66 ( 70.54)\tAcc@5  90.62 ( 93.32)\n","Epoch: [41][ 60/391]\tTime  0.090 ( 0.096)\tLoss 9.9262e-01 (1.0322e+00)\tAcc@1  67.97 ( 70.27)\tAcc@5  92.97 ( 93.07)\n","Epoch: [41][ 90/391]\tTime  0.091 ( 0.095)\tLoss 1.1062e+00 (1.0358e+00)\tAcc@1  69.53 ( 69.79)\tAcc@5  89.84 ( 92.94)\n","Epoch: [41][120/391]\tTime  0.092 ( 0.095)\tLoss 1.1259e+00 (1.0471e+00)\tAcc@1  67.97 ( 69.39)\tAcc@5  88.28 ( 92.76)\n","Epoch: [41][150/391]\tTime  0.092 ( 0.094)\tLoss 1.1593e+00 (1.0611e+00)\tAcc@1  67.19 ( 68.80)\tAcc@5  91.41 ( 92.69)\n","Epoch: [41][180/391]\tTime  0.090 ( 0.094)\tLoss 1.1169e+00 (1.0661e+00)\tAcc@1  69.53 ( 68.72)\tAcc@5  92.97 ( 92.70)\n","Epoch: [41][210/391]\tTime  0.095 ( 0.094)\tLoss 1.1094e+00 (1.0741e+00)\tAcc@1  68.75 ( 68.55)\tAcc@5  90.62 ( 92.59)\n","Epoch: [41][240/391]\tTime  0.090 ( 0.094)\tLoss 1.2075e+00 (1.0790e+00)\tAcc@1  64.84 ( 68.44)\tAcc@5  89.06 ( 92.50)\n","Epoch: [41][270/391]\tTime  0.093 ( 0.094)\tLoss 1.0177e+00 (1.0854e+00)\tAcc@1  73.44 ( 68.24)\tAcc@5  92.97 ( 92.45)\n","Epoch: [41][300/391]\tTime  0.094 ( 0.093)\tLoss 1.2504e+00 (1.0930e+00)\tAcc@1  66.41 ( 68.11)\tAcc@5  89.06 ( 92.33)\n","Epoch: [41][330/391]\tTime  0.093 ( 0.093)\tLoss 1.0335e+00 (1.0958e+00)\tAcc@1  70.31 ( 68.02)\tAcc@5  91.41 ( 92.26)\n","Epoch: [41][360/391]\tTime  0.092 ( 0.093)\tLoss 1.1135e+00 (1.0995e+00)\tAcc@1  68.75 ( 67.92)\tAcc@5  91.41 ( 92.22)\n","Epoch: [41][390/391]\tTime  0.082 ( 0.093)\tLoss 1.3417e+00 (1.1037e+00)\tAcc@1  67.50 ( 67.88)\tAcc@5  85.00 ( 92.12)\n","==> Train Accuracy: Acc@1 67.876 || Acc@5 92.122\n","==> Test Accuracy:  Acc@1 58.670 || Acc@5 86.350\n","==> 39.53 seconds to train this epoch\n","\n","\n","----- epoch: 42, lr: 0.1 -----\n","Epoch: [42][  0/391]\tTime  0.258 ( 0.258)\tLoss 1.0108e+00 (1.0108e+00)\tAcc@1  75.00 ( 75.00)\tAcc@5  92.19 ( 92.19)\n","Epoch: [42][ 30/391]\tTime  0.094 ( 0.099)\tLoss 1.1176e+00 (1.0046e+00)\tAcc@1  67.19 ( 71.47)\tAcc@5  90.62 ( 93.32)\n","Epoch: [42][ 60/391]\tTime  0.098 ( 0.096)\tLoss 9.7595e-01 (1.0062e+00)\tAcc@1  66.41 ( 70.70)\tAcc@5  96.09 ( 93.61)\n","Epoch: [42][ 90/391]\tTime  0.093 ( 0.095)\tLoss 1.0315e+00 (1.0404e+00)\tAcc@1  71.88 ( 69.93)\tAcc@5  91.41 ( 93.11)\n","Epoch: [42][120/391]\tTime  0.092 ( 0.094)\tLoss 8.1669e-01 (1.0444e+00)\tAcc@1  76.56 ( 69.81)\tAcc@5  95.31 ( 93.12)\n","Epoch: [42][150/391]\tTime  0.089 ( 0.094)\tLoss 9.6515e-01 (1.0631e+00)\tAcc@1  71.09 ( 69.39)\tAcc@5  94.53 ( 92.75)\n","Epoch: [42][180/391]\tTime  0.101 ( 0.094)\tLoss 1.3352e+00 (1.0760e+00)\tAcc@1  60.16 ( 69.04)\tAcc@5  89.06 ( 92.58)\n","Epoch: [42][210/391]\tTime  0.091 ( 0.094)\tLoss 9.9365e-01 (1.0791e+00)\tAcc@1  72.66 ( 68.88)\tAcc@5  93.75 ( 92.55)\n","Epoch: [42][240/391]\tTime  0.091 ( 0.094)\tLoss 1.1434e+00 (1.0825e+00)\tAcc@1  67.97 ( 68.69)\tAcc@5  92.19 ( 92.45)\n","Epoch: [42][270/391]\tTime  0.092 ( 0.093)\tLoss 1.1161e+00 (1.0875e+00)\tAcc@1  67.97 ( 68.49)\tAcc@5  90.62 ( 92.33)\n","Epoch: [42][300/391]\tTime  0.094 ( 0.093)\tLoss 1.0476e+00 (1.0928e+00)\tAcc@1  70.31 ( 68.40)\tAcc@5  92.97 ( 92.23)\n","Epoch: [42][330/391]\tTime  0.093 ( 0.093)\tLoss 1.2117e+00 (1.0967e+00)\tAcc@1  63.28 ( 68.33)\tAcc@5  90.62 ( 92.17)\n","Epoch: [42][360/391]\tTime  0.090 ( 0.093)\tLoss 9.5923e-01 (1.1016e+00)\tAcc@1  71.88 ( 68.15)\tAcc@5  95.31 ( 92.17)\n","Epoch: [42][390/391]\tTime  0.082 ( 0.093)\tLoss 1.4414e+00 (1.1022e+00)\tAcc@1  61.25 ( 68.14)\tAcc@5  88.75 ( 92.17)\n","==> Train Accuracy: Acc@1 68.138 || Acc@5 92.166\n","==> Test Accuracy:  Acc@1 57.170 || Acc@5 85.850\n","==> 39.48 seconds to train this epoch\n","\n","\n","----- epoch: 43, lr: 0.1 -----\n","Epoch: [43][  0/391]\tTime  0.291 ( 0.291)\tLoss 1.3537e+00 (1.3537e+00)\tAcc@1  63.28 ( 63.28)\tAcc@5  89.06 ( 89.06)\n","Epoch: [43][ 30/391]\tTime  0.103 ( 0.099)\tLoss 9.7671e-01 (1.0324e+00)\tAcc@1  67.97 ( 70.04)\tAcc@5  93.75 ( 92.92)\n","Epoch: [43][ 60/391]\tTime  0.104 ( 0.096)\tLoss 1.0437e+00 (1.0269e+00)\tAcc@1  66.41 ( 70.02)\tAcc@5  92.97 ( 92.99)\n","Epoch: [43][ 90/391]\tTime  0.089 ( 0.095)\tLoss 9.8145e-01 (1.0288e+00)\tAcc@1  72.66 ( 70.19)\tAcc@5  92.19 ( 92.82)\n","Epoch: [43][120/391]\tTime  0.093 ( 0.094)\tLoss 1.1343e+00 (1.0511e+00)\tAcc@1  68.75 ( 69.47)\tAcc@5  91.41 ( 92.49)\n","Epoch: [43][150/391]\tTime  0.091 ( 0.094)\tLoss 8.6168e-01 (1.0569e+00)\tAcc@1  75.78 ( 69.47)\tAcc@5  96.88 ( 92.41)\n","Epoch: [43][180/391]\tTime  0.095 ( 0.094)\tLoss 1.0979e+00 (1.0617e+00)\tAcc@1  67.97 ( 69.33)\tAcc@5  93.75 ( 92.42)\n","Epoch: [43][210/391]\tTime  0.092 ( 0.094)\tLoss 1.0427e+00 (1.0696e+00)\tAcc@1  71.88 ( 69.09)\tAcc@5  90.62 ( 92.34)\n","Epoch: [43][240/391]\tTime  0.092 ( 0.094)\tLoss 1.1862e+00 (1.0757e+00)\tAcc@1  64.06 ( 68.89)\tAcc@5  92.19 ( 92.32)\n","Epoch: [43][270/391]\tTime  0.102 ( 0.093)\tLoss 1.1450e+00 (1.0817e+00)\tAcc@1  71.88 ( 68.63)\tAcc@5  92.19 ( 92.34)\n","Epoch: [43][300/391]\tTime  0.101 ( 0.093)\tLoss 1.1601e+00 (1.0894e+00)\tAcc@1  63.28 ( 68.35)\tAcc@5  92.97 ( 92.31)\n","Epoch: [43][330/391]\tTime  0.092 ( 0.093)\tLoss 1.2201e+00 (1.0928e+00)\tAcc@1  63.28 ( 68.29)\tAcc@5  90.62 ( 92.27)\n","Epoch: [43][360/391]\tTime  0.095 ( 0.093)\tLoss 1.4392e+00 (1.0957e+00)\tAcc@1  59.38 ( 68.25)\tAcc@5  88.28 ( 92.19)\n","Epoch: [43][390/391]\tTime  0.082 ( 0.093)\tLoss 1.1953e+00 (1.1007e+00)\tAcc@1  67.50 ( 68.12)\tAcc@5  91.25 ( 92.10)\n","==> Train Accuracy: Acc@1 68.116 || Acc@5 92.102\n","==> Test Accuracy:  Acc@1 59.150 || Acc@5 85.770\n","==> 39.48 seconds to train this epoch\n","\n","\n","----- epoch: 44, lr: 0.1 -----\n","Epoch: [44][  0/391]\tTime  0.290 ( 0.290)\tLoss 1.0726e+00 (1.0726e+00)\tAcc@1  64.06 ( 64.06)\tAcc@5  92.97 ( 92.97)\n","Epoch: [44][ 30/391]\tTime  0.093 ( 0.100)\tLoss 9.4667e-01 (1.0491e+00)\tAcc@1  72.66 ( 69.13)\tAcc@5  96.88 ( 92.84)\n","Epoch: [44][ 60/391]\tTime  0.100 ( 0.097)\tLoss 1.1677e+00 (1.0397e+00)\tAcc@1  63.28 ( 69.66)\tAcc@5  92.97 ( 92.89)\n","Epoch: [44][ 90/391]\tTime  0.088 ( 0.095)\tLoss 8.2446e-01 (1.0346e+00)\tAcc@1  72.66 ( 69.68)\tAcc@5  93.75 ( 92.87)\n","Epoch: [44][120/391]\tTime  0.091 ( 0.094)\tLoss 1.2075e+00 (1.0393e+00)\tAcc@1  61.72 ( 69.63)\tAcc@5  91.41 ( 92.90)\n","Epoch: [44][150/391]\tTime  0.091 ( 0.094)\tLoss 1.1402e+00 (1.0463e+00)\tAcc@1  60.94 ( 69.42)\tAcc@5  92.97 ( 92.92)\n","Epoch: [44][180/391]\tTime  0.089 ( 0.094)\tLoss 9.2196e-01 (1.0453e+00)\tAcc@1  71.88 ( 69.39)\tAcc@5  92.19 ( 92.93)\n","Epoch: [44][210/391]\tTime  0.096 ( 0.094)\tLoss 1.1626e+00 (1.0467e+00)\tAcc@1  63.28 ( 69.33)\tAcc@5  93.75 ( 92.88)\n","Epoch: [44][240/391]\tTime  0.088 ( 0.094)\tLoss 1.0780e+00 (1.0638e+00)\tAcc@1  70.31 ( 68.88)\tAcc@5  90.62 ( 92.63)\n","Epoch: [44][270/391]\tTime  0.094 ( 0.094)\tLoss 1.4185e+00 (1.0721e+00)\tAcc@1  57.81 ( 68.72)\tAcc@5  89.84 ( 92.54)\n","Epoch: [44][300/391]\tTime  0.088 ( 0.094)\tLoss 1.0082e+00 (1.0815e+00)\tAcc@1  71.09 ( 68.52)\tAcc@5  94.53 ( 92.42)\n","Epoch: [44][330/391]\tTime  0.096 ( 0.094)\tLoss 9.7062e-01 (1.0870e+00)\tAcc@1  70.31 ( 68.35)\tAcc@5  94.53 ( 92.33)\n","Epoch: [44][360/391]\tTime  0.093 ( 0.093)\tLoss 1.1772e+00 (1.0920e+00)\tAcc@1  62.50 ( 68.18)\tAcc@5  91.41 ( 92.25)\n","Epoch: [44][390/391]\tTime  0.087 ( 0.093)\tLoss 9.5779e-01 (1.0944e+00)\tAcc@1  71.25 ( 68.11)\tAcc@5  93.75 ( 92.23)\n","==> Train Accuracy: Acc@1 68.114 || Acc@5 92.232\n","==> Test Accuracy:  Acc@1 58.250 || Acc@5 86.130\n","==> 39.64 seconds to train this epoch\n","\n","\n","----- epoch: 45, lr: 0.1 -----\n","Epoch: [45][  0/391]\tTime  0.292 ( 0.292)\tLoss 9.6988e-01 (9.6988e-01)\tAcc@1  72.66 ( 72.66)\tAcc@5  95.31 ( 95.31)\n","Epoch: [45][ 30/391]\tTime  0.091 ( 0.100)\tLoss 7.7672e-01 (9.9214e-01)\tAcc@1  75.00 ( 70.74)\tAcc@5  98.44 ( 93.70)\n","Epoch: [45][ 60/391]\tTime  0.094 ( 0.096)\tLoss 1.1327e+00 (9.9047e-01)\tAcc@1  70.31 ( 70.95)\tAcc@5  90.62 ( 93.81)\n","Epoch: [45][ 90/391]\tTime  0.096 ( 0.095)\tLoss 1.1027e+00 (1.0268e+00)\tAcc@1  67.97 ( 70.06)\tAcc@5  92.19 ( 93.24)\n","Epoch: [45][120/391]\tTime  0.089 ( 0.095)\tLoss 9.8659e-01 (1.0358e+00)\tAcc@1  72.66 ( 69.78)\tAcc@5  92.97 ( 93.08)\n","Epoch: [45][150/391]\tTime  0.092 ( 0.094)\tLoss 9.6879e-01 (1.0415e+00)\tAcc@1  70.31 ( 69.56)\tAcc@5  91.41 ( 92.98)\n","Epoch: [45][180/391]\tTime  0.089 ( 0.094)\tLoss 1.0184e+00 (1.0479e+00)\tAcc@1  67.19 ( 69.35)\tAcc@5  91.41 ( 92.87)\n","Epoch: [45][210/391]\tTime  0.097 ( 0.094)\tLoss 9.7495e-01 (1.0570e+00)\tAcc@1  72.66 ( 69.04)\tAcc@5  92.97 ( 92.78)\n","Epoch: [45][240/391]\tTime  0.091 ( 0.093)\tLoss 1.0764e+00 (1.0643e+00)\tAcc@1  67.19 ( 68.87)\tAcc@5  92.97 ( 92.71)\n","Epoch: [45][270/391]\tTime  0.094 ( 0.093)\tLoss 1.3003e+00 (1.0690e+00)\tAcc@1  62.50 ( 68.80)\tAcc@5  87.50 ( 92.65)\n","Epoch: [45][300/391]\tTime  0.082 ( 0.093)\tLoss 1.0852e+00 (1.0745e+00)\tAcc@1  67.19 ( 68.76)\tAcc@5  93.75 ( 92.58)\n","Epoch: [45][330/391]\tTime  0.098 ( 0.093)\tLoss 1.1496e+00 (1.0792e+00)\tAcc@1  66.41 ( 68.56)\tAcc@5  90.62 ( 92.51)\n","Epoch: [45][360/391]\tTime  0.091 ( 0.093)\tLoss 1.0446e+00 (1.0840e+00)\tAcc@1  73.44 ( 68.38)\tAcc@5  90.62 ( 92.49)\n","Epoch: [45][390/391]\tTime  0.081 ( 0.093)\tLoss 1.3035e+00 (1.0873e+00)\tAcc@1  65.00 ( 68.28)\tAcc@5  91.25 ( 92.45)\n","==> Train Accuracy: Acc@1 68.278 || Acc@5 92.452\n","==> Test Accuracy:  Acc@1 58.900 || Acc@5 86.500\n","==> 39.50 seconds to train this epoch\n","\n","\n","----- epoch: 46, lr: 0.1 -----\n","Epoch: [46][  0/391]\tTime  0.288 ( 0.288)\tLoss 9.0281e-01 (9.0281e-01)\tAcc@1  76.56 ( 76.56)\tAcc@5  95.31 ( 95.31)\n","Epoch: [46][ 30/391]\tTime  0.091 ( 0.099)\tLoss 8.7736e-01 (9.7544e-01)\tAcc@1  78.12 ( 71.50)\tAcc@5  93.75 ( 93.70)\n","Epoch: [46][ 60/391]\tTime  0.101 ( 0.096)\tLoss 9.0096e-01 (9.6826e-01)\tAcc@1  69.53 ( 71.75)\tAcc@5  93.75 ( 93.70)\n","Epoch: [46][ 90/391]\tTime  0.094 ( 0.095)\tLoss 1.1525e+00 (9.9809e-01)\tAcc@1  67.97 ( 70.93)\tAcc@5  89.06 ( 93.34)\n","Epoch: [46][120/391]\tTime  0.092 ( 0.095)\tLoss 1.0301e+00 (1.0222e+00)\tAcc@1  64.84 ( 70.22)\tAcc@5  96.88 ( 93.13)\n","Epoch: [46][150/391]\tTime  0.093 ( 0.094)\tLoss 9.4140e-01 (1.0263e+00)\tAcc@1  73.44 ( 70.20)\tAcc@5  93.75 ( 93.05)\n","Epoch: [46][180/391]\tTime  0.093 ( 0.094)\tLoss 1.0899e+00 (1.0381e+00)\tAcc@1  68.75 ( 69.87)\tAcc@5  91.41 ( 92.93)\n","Epoch: [46][210/391]\tTime  0.096 ( 0.094)\tLoss 1.0124e+00 (1.0444e+00)\tAcc@1  68.75 ( 69.71)\tAcc@5  96.09 ( 92.84)\n","Epoch: [46][240/391]\tTime  0.088 ( 0.094)\tLoss 9.8365e-01 (1.0486e+00)\tAcc@1  74.22 ( 69.60)\tAcc@5  95.31 ( 92.77)\n","Epoch: [46][270/391]\tTime  0.102 ( 0.093)\tLoss 1.1763e+00 (1.0589e+00)\tAcc@1  62.50 ( 69.45)\tAcc@5  90.62 ( 92.62)\n","Epoch: [46][300/391]\tTime  0.089 ( 0.093)\tLoss 8.8362e-01 (1.0645e+00)\tAcc@1  75.00 ( 69.25)\tAcc@5  93.75 ( 92.54)\n","Epoch: [46][330/391]\tTime  0.096 ( 0.093)\tLoss 9.1846e-01 (1.0668e+00)\tAcc@1  78.12 ( 69.18)\tAcc@5  93.75 ( 92.50)\n","Epoch: [46][360/391]\tTime  0.092 ( 0.093)\tLoss 9.7640e-01 (1.0744e+00)\tAcc@1  71.88 ( 68.94)\tAcc@5  92.97 ( 92.40)\n","Epoch: [46][390/391]\tTime  0.082 ( 0.093)\tLoss 1.2756e+00 (1.0805e+00)\tAcc@1  61.25 ( 68.75)\tAcc@5  90.00 ( 92.33)\n","==> Train Accuracy: Acc@1 68.754 || Acc@5 92.326\n","==> Test Accuracy:  Acc@1 62.130 || Acc@5 88.150\n","==> 39.51 seconds to train this epoch\n","\n","\n","----- epoch: 47, lr: 0.1 -----\n","Epoch: [47][  0/391]\tTime  0.278 ( 0.278)\tLoss 8.3223e-01 (8.3223e-01)\tAcc@1  73.44 ( 73.44)\tAcc@5  96.09 ( 96.09)\n","Epoch: [47][ 30/391]\tTime  0.092 ( 0.099)\tLoss 1.0715e+00 (9.4539e-01)\tAcc@1  69.53 ( 71.72)\tAcc@5  89.84 ( 93.75)\n","Epoch: [47][ 60/391]\tTime  0.102 ( 0.096)\tLoss 1.1378e+00 (9.6811e-01)\tAcc@1  66.41 ( 71.09)\tAcc@5  91.41 ( 93.85)\n","Epoch: [47][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.1374e+00 (9.8211e-01)\tAcc@1  65.62 ( 70.92)\tAcc@5  91.41 ( 93.54)\n","Epoch: [47][120/391]\tTime  0.096 ( 0.097)\tLoss 1.2669e+00 (1.0089e+00)\tAcc@1  62.50 ( 70.38)\tAcc@5  90.62 ( 93.21)\n","Epoch: [47][150/391]\tTime  0.096 ( 0.096)\tLoss 1.1753e+00 (1.0214e+00)\tAcc@1  63.28 ( 70.09)\tAcc@5  90.62 ( 93.04)\n","Epoch: [47][180/391]\tTime  0.093 ( 0.096)\tLoss 9.9346e-01 (1.0385e+00)\tAcc@1  67.19 ( 69.66)\tAcc@5  90.62 ( 92.83)\n","Epoch: [47][210/391]\tTime  0.089 ( 0.095)\tLoss 1.1352e+00 (1.0463e+00)\tAcc@1  64.06 ( 69.35)\tAcc@5  92.97 ( 92.74)\n","Epoch: [47][240/391]\tTime  0.090 ( 0.095)\tLoss 1.1785e+00 (1.0562e+00)\tAcc@1  67.19 ( 69.08)\tAcc@5  91.41 ( 92.62)\n","Epoch: [47][270/391]\tTime  0.093 ( 0.095)\tLoss 1.1850e+00 (1.0622e+00)\tAcc@1  66.41 ( 68.90)\tAcc@5  92.19 ( 92.60)\n","Epoch: [47][300/391]\tTime  0.099 ( 0.094)\tLoss 1.2277e+00 (1.0743e+00)\tAcc@1  59.38 ( 68.62)\tAcc@5  91.41 ( 92.47)\n","Epoch: [47][330/391]\tTime  0.092 ( 0.094)\tLoss 1.1238e+00 (1.0812e+00)\tAcc@1  67.19 ( 68.48)\tAcc@5  94.53 ( 92.38)\n","Epoch: [47][360/391]\tTime  0.090 ( 0.094)\tLoss 8.9280e-01 (1.0793e+00)\tAcc@1  75.78 ( 68.54)\tAcc@5  95.31 ( 92.41)\n","Epoch: [47][390/391]\tTime  0.081 ( 0.094)\tLoss 1.4190e+00 (1.0844e+00)\tAcc@1  61.25 ( 68.45)\tAcc@5  87.50 ( 92.30)\n","==> Train Accuracy: Acc@1 68.450 || Acc@5 92.302\n","==> Test Accuracy:  Acc@1 56.890 || Acc@5 84.290\n","==> 39.82 seconds to train this epoch\n","\n","\n","----- epoch: 48, lr: 0.1 -----\n","Epoch: [48][  0/391]\tTime  0.266 ( 0.266)\tLoss 9.8129e-01 (9.8129e-01)\tAcc@1  70.31 ( 70.31)\tAcc@5  93.75 ( 93.75)\n","Epoch: [48][ 30/391]\tTime  0.102 ( 0.100)\tLoss 7.2412e-01 (9.6594e-01)\tAcc@1  77.34 ( 71.82)\tAcc@5  97.66 ( 93.85)\n","Epoch: [48][ 60/391]\tTime  0.094 ( 0.096)\tLoss 1.1320e+00 (9.8511e-01)\tAcc@1  66.41 ( 71.31)\tAcc@5  93.75 ( 93.79)\n","Epoch: [48][ 90/391]\tTime  0.091 ( 0.095)\tLoss 1.1564e+00 (9.9626e-01)\tAcc@1  73.44 ( 71.16)\tAcc@5  89.06 ( 93.49)\n","Epoch: [48][120/391]\tTime  0.092 ( 0.094)\tLoss 1.2390e+00 (1.0061e+00)\tAcc@1  67.97 ( 70.86)\tAcc@5  89.84 ( 93.30)\n","Epoch: [48][150/391]\tTime  0.090 ( 0.094)\tLoss 1.0375e+00 (1.0292e+00)\tAcc@1  67.19 ( 70.25)\tAcc@5  96.09 ( 93.03)\n","Epoch: [48][180/391]\tTime  0.091 ( 0.094)\tLoss 1.0589e+00 (1.0443e+00)\tAcc@1  71.88 ( 69.65)\tAcc@5  96.88 ( 92.83)\n","Epoch: [48][210/391]\tTime  0.092 ( 0.094)\tLoss 1.1525e+00 (1.0530e+00)\tAcc@1  67.19 ( 69.43)\tAcc@5  90.62 ( 92.72)\n","Epoch: [48][240/391]\tTime  0.093 ( 0.094)\tLoss 1.1772e+00 (1.0553e+00)\tAcc@1  62.50 ( 69.29)\tAcc@5  92.97 ( 92.74)\n","Epoch: [48][270/391]\tTime  0.104 ( 0.094)\tLoss 1.0312e+00 (1.0596e+00)\tAcc@1  73.44 ( 69.20)\tAcc@5  89.84 ( 92.65)\n","Epoch: [48][300/391]\tTime  0.097 ( 0.094)\tLoss 1.1723e+00 (1.0660e+00)\tAcc@1  64.84 ( 68.99)\tAcc@5  89.84 ( 92.58)\n","Epoch: [48][330/391]\tTime  0.091 ( 0.093)\tLoss 1.3890e+00 (1.0693e+00)\tAcc@1  60.94 ( 68.87)\tAcc@5  91.41 ( 92.53)\n","Epoch: [48][360/391]\tTime  0.093 ( 0.093)\tLoss 1.0410e+00 (1.0721e+00)\tAcc@1  67.19 ( 68.80)\tAcc@5  94.53 ( 92.50)\n","Epoch: [48][390/391]\tTime  0.082 ( 0.093)\tLoss 1.2022e+00 (1.0775e+00)\tAcc@1  68.75 ( 68.59)\tAcc@5  88.75 ( 92.44)\n","==> Train Accuracy: Acc@1 68.594 || Acc@5 92.436\n","==> Test Accuracy:  Acc@1 60.110 || Acc@5 86.950\n","==> 39.54 seconds to train this epoch\n","\n","\n","----- epoch: 49, lr: 0.1 -----\n","Epoch: [49][  0/391]\tTime  0.301 ( 0.301)\tLoss 1.1380e+00 (1.1380e+00)\tAcc@1  64.84 ( 64.84)\tAcc@5  93.75 ( 93.75)\n","Epoch: [49][ 30/391]\tTime  0.091 ( 0.100)\tLoss 8.5376e-01 (9.7971e-01)\tAcc@1  71.88 ( 71.09)\tAcc@5  96.88 ( 93.62)\n","Epoch: [49][ 60/391]\tTime  0.096 ( 0.097)\tLoss 9.9361e-01 (9.8649e-01)\tAcc@1  67.97 ( 70.88)\tAcc@5  95.31 ( 93.40)\n","Epoch: [49][ 90/391]\tTime  0.093 ( 0.095)\tLoss 1.1968e+00 (1.0000e+00)\tAcc@1  66.41 ( 70.40)\tAcc@5  89.84 ( 93.43)\n","Epoch: [49][120/391]\tTime  0.088 ( 0.095)\tLoss 9.9203e-01 (1.0083e+00)\tAcc@1  71.09 ( 70.33)\tAcc@5  93.75 ( 93.38)\n","Epoch: [49][150/391]\tTime  0.091 ( 0.094)\tLoss 1.2154e+00 (1.0140e+00)\tAcc@1  67.19 ( 70.25)\tAcc@5  89.84 ( 93.33)\n","Epoch: [49][180/391]\tTime  0.095 ( 0.094)\tLoss 1.1092e+00 (1.0176e+00)\tAcc@1  67.97 ( 70.20)\tAcc@5  92.19 ( 93.26)\n","Epoch: [49][210/391]\tTime  0.090 ( 0.094)\tLoss 1.0837e+00 (1.0296e+00)\tAcc@1  71.88 ( 69.89)\tAcc@5  90.62 ( 93.09)\n","Epoch: [49][240/391]\tTime  0.105 ( 0.094)\tLoss 1.0179e+00 (1.0377e+00)\tAcc@1  70.31 ( 69.66)\tAcc@5  95.31 ( 92.99)\n","Epoch: [49][270/391]\tTime  0.091 ( 0.094)\tLoss 9.2804e-01 (1.0419e+00)\tAcc@1  72.66 ( 69.60)\tAcc@5  92.97 ( 92.89)\n","Epoch: [49][300/391]\tTime  0.090 ( 0.093)\tLoss 1.2802e+00 (1.0474e+00)\tAcc@1  61.72 ( 69.43)\tAcc@5  89.84 ( 92.84)\n","Epoch: [49][330/391]\tTime  0.088 ( 0.093)\tLoss 1.0668e+00 (1.0538e+00)\tAcc@1  71.09 ( 69.28)\tAcc@5  91.41 ( 92.77)\n","Epoch: [49][360/391]\tTime  0.090 ( 0.093)\tLoss 1.0447e+00 (1.0598e+00)\tAcc@1  71.09 ( 69.17)\tAcc@5  90.62 ( 92.73)\n","Epoch: [49][390/391]\tTime  0.089 ( 0.093)\tLoss 1.1628e+00 (1.0695e+00)\tAcc@1  56.25 ( 68.92)\tAcc@5  93.75 ( 92.61)\n","==> Train Accuracy: Acc@1 68.924 || Acc@5 92.614\n","==> Test Accuracy:  Acc@1 52.380 || Acc@5 80.830\n","==> 39.57 seconds to train this epoch\n","\n","\n","----- epoch: 50, lr: 0.1 -----\n","Epoch: [50][  0/391]\tTime  0.309 ( 0.309)\tLoss 9.7453e-01 (9.7453e-01)\tAcc@1  69.53 ( 69.53)\tAcc@5  96.09 ( 96.09)\n","Epoch: [50][ 30/391]\tTime  0.088 ( 0.100)\tLoss 1.1527e+00 (1.0413e+00)\tAcc@1  69.53 ( 69.25)\tAcc@5  92.19 ( 92.82)\n","Epoch: [50][ 60/391]\tTime  0.095 ( 0.097)\tLoss 8.7182e-01 (1.0125e+00)\tAcc@1  72.66 ( 70.12)\tAcc@5  95.31 ( 92.94)\n","Epoch: [50][ 90/391]\tTime  0.093 ( 0.095)\tLoss 1.1207e+00 (1.0062e+00)\tAcc@1  68.75 ( 70.39)\tAcc@5  90.62 ( 93.03)\n","Epoch: [50][120/391]\tTime  0.091 ( 0.095)\tLoss 9.7046e-01 (9.9548e-01)\tAcc@1  71.09 ( 70.64)\tAcc@5  92.19 ( 93.10)\n","Epoch: [50][150/391]\tTime  0.092 ( 0.094)\tLoss 1.0938e+00 (1.0027e+00)\tAcc@1  67.97 ( 70.41)\tAcc@5  92.97 ( 93.12)\n","Epoch: [50][180/391]\tTime  0.092 ( 0.094)\tLoss 1.4470e+00 (1.0216e+00)\tAcc@1  59.38 ( 69.96)\tAcc@5  86.72 ( 93.03)\n","Epoch: [50][210/391]\tTime  0.091 ( 0.094)\tLoss 1.0138e+00 (1.0336e+00)\tAcc@1  71.88 ( 69.75)\tAcc@5  93.75 ( 92.82)\n","Epoch: [50][240/391]\tTime  0.091 ( 0.094)\tLoss 1.0480e+00 (1.0373e+00)\tAcc@1  68.75 ( 69.59)\tAcc@5  94.53 ( 92.82)\n","Epoch: [50][270/391]\tTime  0.096 ( 0.094)\tLoss 1.0597e+00 (1.0465e+00)\tAcc@1  69.53 ( 69.35)\tAcc@5  92.97 ( 92.70)\n","Epoch: [50][300/391]\tTime  0.087 ( 0.093)\tLoss 1.0924e+00 (1.0534e+00)\tAcc@1  70.31 ( 69.08)\tAcc@5  89.84 ( 92.65)\n","Epoch: [50][330/391]\tTime  0.088 ( 0.093)\tLoss 1.3357e+00 (1.0608e+00)\tAcc@1  67.97 ( 68.97)\tAcc@5  90.62 ( 92.59)\n","Epoch: [50][360/391]\tTime  0.092 ( 0.093)\tLoss 1.1894e+00 (1.0671e+00)\tAcc@1  65.62 ( 68.88)\tAcc@5  91.41 ( 92.53)\n","Epoch: [50][390/391]\tTime  0.082 ( 0.093)\tLoss 1.1626e+00 (1.0696e+00)\tAcc@1  75.00 ( 68.86)\tAcc@5  90.00 ( 92.54)\n","==> Train Accuracy: Acc@1 68.862 || Acc@5 92.542\n","==> Test Accuracy:  Acc@1 57.910 || Acc@5 84.910\n","==> 39.57 seconds to train this epoch\n","\n","\n","----- epoch: 51, lr: 0.1 -----\n","Epoch: [51][  0/391]\tTime  0.284 ( 0.284)\tLoss 7.5356e-01 (7.5356e-01)\tAcc@1  82.03 ( 82.03)\tAcc@5  94.53 ( 94.53)\n","Epoch: [51][ 30/391]\tTime  0.093 ( 0.100)\tLoss 8.4300e-01 (9.6958e-01)\tAcc@1  75.78 ( 72.40)\tAcc@5  95.31 ( 93.04)\n","Epoch: [51][ 60/391]\tTime  0.086 ( 0.096)\tLoss 1.0893e+00 (9.7241e-01)\tAcc@1  73.44 ( 71.72)\tAcc@5  92.19 ( 93.16)\n","Epoch: [51][ 90/391]\tTime  0.093 ( 0.095)\tLoss 1.2442e+00 (9.8799e-01)\tAcc@1  64.06 ( 71.00)\tAcc@5  91.41 ( 93.23)\n","Epoch: [51][120/391]\tTime  0.095 ( 0.095)\tLoss 1.1287e+00 (9.9058e-01)\tAcc@1  68.75 ( 71.03)\tAcc@5  92.19 ( 93.34)\n","Epoch: [51][150/391]\tTime  0.087 ( 0.094)\tLoss 1.2520e+00 (1.0021e+00)\tAcc@1  61.72 ( 70.71)\tAcc@5  91.41 ( 93.19)\n","Epoch: [51][180/391]\tTime  0.092 ( 0.094)\tLoss 1.0734e+00 (1.0125e+00)\tAcc@1  71.09 ( 70.29)\tAcc@5  92.19 ( 93.14)\n","Epoch: [51][210/391]\tTime  0.095 ( 0.094)\tLoss 1.1768e+00 (1.0256e+00)\tAcc@1  67.19 ( 69.91)\tAcc@5  92.97 ( 93.06)\n","Epoch: [51][240/391]\tTime  0.091 ( 0.094)\tLoss 1.2206e+00 (1.0366e+00)\tAcc@1  61.72 ( 69.55)\tAcc@5  95.31 ( 92.97)\n","Epoch: [51][270/391]\tTime  0.089 ( 0.093)\tLoss 1.0753e+00 (1.0457e+00)\tAcc@1  64.84 ( 69.29)\tAcc@5  91.41 ( 92.87)\n","Epoch: [51][300/391]\tTime  0.097 ( 0.093)\tLoss 1.3273e+00 (1.0541e+00)\tAcc@1  58.59 ( 69.12)\tAcc@5  91.41 ( 92.72)\n","Epoch: [51][330/391]\tTime  0.094 ( 0.093)\tLoss 1.2869e+00 (1.0624e+00)\tAcc@1  64.84 ( 68.91)\tAcc@5  87.50 ( 92.58)\n","Epoch: [51][360/391]\tTime  0.093 ( 0.093)\tLoss 9.0440e-01 (1.0687e+00)\tAcc@1  71.88 ( 68.75)\tAcc@5  92.97 ( 92.48)\n","Epoch: [51][390/391]\tTime  0.082 ( 0.093)\tLoss 1.5449e+00 (1.0710e+00)\tAcc@1  57.50 ( 68.71)\tAcc@5  86.25 ( 92.45)\n","==> Train Accuracy: Acc@1 68.710 || Acc@5 92.452\n","==> Test Accuracy:  Acc@1 58.860 || Acc@5 86.280\n","==> 39.49 seconds to train this epoch\n","\n","\n","----- epoch: 52, lr: 0.1 -----\n","Epoch: [52][  0/391]\tTime  0.274 ( 0.274)\tLoss 7.3468e-01 (7.3468e-01)\tAcc@1  75.00 ( 75.00)\tAcc@5  97.66 ( 97.66)\n","Epoch: [52][ 30/391]\tTime  0.090 ( 0.098)\tLoss 9.9355e-01 (9.9590e-01)\tAcc@1  64.84 ( 70.44)\tAcc@5  92.19 ( 92.94)\n","Epoch: [52][ 60/391]\tTime  0.090 ( 0.095)\tLoss 8.6926e-01 (1.0037e+00)\tAcc@1  75.78 ( 70.52)\tAcc@5  95.31 ( 92.82)\n","Epoch: [52][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.1894e+00 (9.9470e-01)\tAcc@1  67.19 ( 70.87)\tAcc@5  90.62 ( 93.04)\n","Epoch: [52][120/391]\tTime  0.093 ( 0.094)\tLoss 1.0753e+00 (1.0043e+00)\tAcc@1  70.31 ( 70.62)\tAcc@5  93.75 ( 93.03)\n","Epoch: [52][150/391]\tTime  0.095 ( 0.094)\tLoss 1.2374e+00 (1.0104e+00)\tAcc@1  65.62 ( 70.47)\tAcc@5  92.19 ( 92.96)\n","Epoch: [52][180/391]\tTime  0.087 ( 0.094)\tLoss 1.1212e+00 (1.0229e+00)\tAcc@1  64.84 ( 70.07)\tAcc@5  90.62 ( 92.83)\n","Epoch: [52][210/391]\tTime  0.089 ( 0.094)\tLoss 9.6279e-01 (1.0282e+00)\tAcc@1  71.09 ( 69.88)\tAcc@5  90.62 ( 92.77)\n","Epoch: [52][240/391]\tTime  0.095 ( 0.093)\tLoss 1.2566e+00 (1.0365e+00)\tAcc@1  65.62 ( 69.63)\tAcc@5  89.84 ( 92.69)\n","Epoch: [52][270/391]\tTime  0.090 ( 0.093)\tLoss 1.0033e+00 (1.0388e+00)\tAcc@1  73.44 ( 69.63)\tAcc@5  92.19 ( 92.67)\n","Epoch: [52][300/391]\tTime  0.093 ( 0.093)\tLoss 1.3450e+00 (1.0461e+00)\tAcc@1  59.38 ( 69.42)\tAcc@5  85.16 ( 92.61)\n","Epoch: [52][330/391]\tTime  0.090 ( 0.093)\tLoss 1.0101e+00 (1.0507e+00)\tAcc@1  68.75 ( 69.36)\tAcc@5  92.19 ( 92.53)\n","Epoch: [52][360/391]\tTime  0.089 ( 0.093)\tLoss 9.8254e-01 (1.0567e+00)\tAcc@1  65.62 ( 69.21)\tAcc@5  92.19 ( 92.45)\n","Epoch: [52][390/391]\tTime  0.088 ( 0.093)\tLoss 1.0734e+00 (1.0601e+00)\tAcc@1  67.50 ( 69.12)\tAcc@5  95.00 ( 92.46)\n","==> Train Accuracy: Acc@1 69.116 || Acc@5 92.456\n","==> Test Accuracy:  Acc@1 59.710 || Acc@5 87.050\n","==> 39.48 seconds to train this epoch\n","\n","\n","----- epoch: 53, lr: 0.1 -----\n","Epoch: [53][  0/391]\tTime  0.287 ( 0.287)\tLoss 1.0127e+00 (1.0127e+00)\tAcc@1  66.41 ( 66.41)\tAcc@5  92.97 ( 92.97)\n","Epoch: [53][ 30/391]\tTime  0.091 ( 0.099)\tLoss 9.7458e-01 (9.7974e-01)\tAcc@1  71.09 ( 70.89)\tAcc@5  92.97 ( 93.52)\n","Epoch: [53][ 60/391]\tTime  0.090 ( 0.096)\tLoss 7.2117e-01 (9.6166e-01)\tAcc@1  78.91 ( 71.66)\tAcc@5  97.66 ( 93.55)\n","Epoch: [53][ 90/391]\tTime  0.094 ( 0.095)\tLoss 8.2865e-01 (9.7030e-01)\tAcc@1  72.66 ( 71.40)\tAcc@5  95.31 ( 93.65)\n","Epoch: [53][120/391]\tTime  0.088 ( 0.094)\tLoss 9.4266e-01 (9.8700e-01)\tAcc@1  73.44 ( 70.98)\tAcc@5  90.62 ( 93.29)\n","Epoch: [53][150/391]\tTime  0.088 ( 0.094)\tLoss 1.1128e+00 (9.9446e-01)\tAcc@1  68.75 ( 70.82)\tAcc@5  92.97 ( 93.27)\n","Epoch: [53][180/391]\tTime  0.093 ( 0.094)\tLoss 9.5219e-01 (1.0069e+00)\tAcc@1  71.09 ( 70.49)\tAcc@5  93.75 ( 93.18)\n","Epoch: [53][210/391]\tTime  0.090 ( 0.094)\tLoss 9.0640e-01 (1.0209e+00)\tAcc@1  71.88 ( 70.24)\tAcc@5  93.75 ( 92.99)\n","Epoch: [53][240/391]\tTime  0.091 ( 0.094)\tLoss 8.8748e-01 (1.0267e+00)\tAcc@1  78.12 ( 70.14)\tAcc@5  91.41 ( 92.94)\n","Epoch: [53][270/391]\tTime  0.092 ( 0.093)\tLoss 1.3014e+00 (1.0359e+00)\tAcc@1  62.50 ( 70.05)\tAcc@5  92.19 ( 92.77)\n","Epoch: [53][300/391]\tTime  0.094 ( 0.093)\tLoss 1.2911e+00 (1.0454e+00)\tAcc@1  66.41 ( 69.75)\tAcc@5  91.41 ( 92.69)\n","Epoch: [53][330/391]\tTime  0.081 ( 0.093)\tLoss 1.0817e+00 (1.0518e+00)\tAcc@1  68.75 ( 69.59)\tAcc@5  93.75 ( 92.61)\n","Epoch: [53][360/391]\tTime  0.087 ( 0.093)\tLoss 9.2113e-01 (1.0568e+00)\tAcc@1  70.31 ( 69.41)\tAcc@5  95.31 ( 92.54)\n","Epoch: [53][390/391]\tTime  0.082 ( 0.093)\tLoss 1.1086e+00 (1.0596e+00)\tAcc@1  71.25 ( 69.23)\tAcc@5  90.00 ( 92.48)\n","==> Train Accuracy: Acc@1 69.234 || Acc@5 92.482\n","==> Test Accuracy:  Acc@1 50.750 || Acc@5 80.650\n","==> 39.56 seconds to train this epoch\n","\n","\n","----- epoch: 54, lr: 0.1 -----\n","Epoch: [54][  0/391]\tTime  0.281 ( 0.281)\tLoss 9.1930e-01 (9.1930e-01)\tAcc@1  71.88 ( 71.88)\tAcc@5  97.66 ( 97.66)\n","Epoch: [54][ 30/391]\tTime  0.092 ( 0.100)\tLoss 9.2180e-01 (9.4378e-01)\tAcc@1  73.44 ( 72.05)\tAcc@5  93.75 ( 94.56)\n","Epoch: [54][ 60/391]\tTime  0.090 ( 0.096)\tLoss 9.1119e-01 (9.6213e-01)\tAcc@1  70.31 ( 71.98)\tAcc@5  98.44 ( 93.94)\n","Epoch: [54][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.1242e+00 (9.7967e-01)\tAcc@1  64.84 ( 71.33)\tAcc@5  92.19 ( 93.84)\n","Epoch: [54][120/391]\tTime  0.091 ( 0.094)\tLoss 7.6264e-01 (9.9253e-01)\tAcc@1  79.69 ( 70.93)\tAcc@5  96.09 ( 93.65)\n","Epoch: [54][150/391]\tTime  0.091 ( 0.094)\tLoss 1.1148e+00 (9.9680e-01)\tAcc@1  66.41 ( 70.75)\tAcc@5  91.41 ( 93.42)\n","Epoch: [54][180/391]\tTime  0.092 ( 0.094)\tLoss 1.1563e+00 (1.0097e+00)\tAcc@1  67.97 ( 70.30)\tAcc@5  90.62 ( 93.28)\n","Epoch: [54][210/391]\tTime  0.092 ( 0.093)\tLoss 1.1939e+00 (1.0206e+00)\tAcc@1  70.31 ( 70.05)\tAcc@5  89.06 ( 93.08)\n","Epoch: [54][240/391]\tTime  0.090 ( 0.093)\tLoss 1.2384e+00 (1.0303e+00)\tAcc@1  64.84 ( 69.86)\tAcc@5  89.84 ( 92.93)\n","Epoch: [54][270/391]\tTime  0.091 ( 0.093)\tLoss 9.7918e-01 (1.0396e+00)\tAcc@1  80.47 ( 69.60)\tAcc@5  91.41 ( 92.76)\n","Epoch: [54][300/391]\tTime  0.102 ( 0.093)\tLoss 1.1380e+00 (1.0544e+00)\tAcc@1  66.41 ( 69.17)\tAcc@5  92.19 ( 92.58)\n","Epoch: [54][330/391]\tTime  0.093 ( 0.093)\tLoss 9.7528e-01 (1.0541e+00)\tAcc@1  72.66 ( 69.23)\tAcc@5  93.75 ( 92.61)\n","Epoch: [54][360/391]\tTime  0.090 ( 0.093)\tLoss 1.1158e+00 (1.0615e+00)\tAcc@1  66.41 ( 69.07)\tAcc@5  92.19 ( 92.49)\n","Epoch: [54][390/391]\tTime  0.082 ( 0.093)\tLoss 1.2648e+00 (1.0665e+00)\tAcc@1  66.25 ( 68.95)\tAcc@5  85.00 ( 92.41)\n","==> Train Accuracy: Acc@1 68.950 || Acc@5 92.406\n","==> Test Accuracy:  Acc@1 55.660 || Acc@5 83.030\n","==> 39.46 seconds to train this epoch\n","\n","\n","----- epoch: 55, lr: 0.1 -----\n","Epoch: [55][  0/391]\tTime  0.266 ( 0.266)\tLoss 1.0319e+00 (1.0319e+00)\tAcc@1  67.97 ( 67.97)\tAcc@5  95.31 ( 95.31)\n","Epoch: [55][ 30/391]\tTime  0.092 ( 0.099)\tLoss 9.8723e-01 (9.9445e-01)\tAcc@1  69.53 ( 70.89)\tAcc@5  92.19 ( 93.45)\n","Epoch: [55][ 60/391]\tTime  0.096 ( 0.096)\tLoss 1.1011e+00 (9.9301e-01)\tAcc@1  68.75 ( 71.11)\tAcc@5  91.41 ( 93.21)\n","Epoch: [55][ 90/391]\tTime  0.089 ( 0.095)\tLoss 1.0032e+00 (9.9825e-01)\tAcc@1  75.78 ( 71.20)\tAcc@5  92.97 ( 93.17)\n","Epoch: [55][120/391]\tTime  0.089 ( 0.094)\tLoss 1.0545e+00 (1.0126e+00)\tAcc@1  71.09 ( 70.63)\tAcc@5  88.28 ( 93.08)\n","Epoch: [55][150/391]\tTime  0.093 ( 0.094)\tLoss 1.1935e+00 (1.0232e+00)\tAcc@1  62.50 ( 70.28)\tAcc@5  91.41 ( 92.99)\n","Epoch: [55][180/391]\tTime  0.094 ( 0.094)\tLoss 1.1226e+00 (1.0324e+00)\tAcc@1  72.66 ( 70.03)\tAcc@5  88.28 ( 93.00)\n","Epoch: [55][210/391]\tTime  0.094 ( 0.094)\tLoss 1.0136e+00 (1.0406e+00)\tAcc@1  71.09 ( 69.68)\tAcc@5  92.19 ( 92.86)\n","Epoch: [55][240/391]\tTime  0.089 ( 0.093)\tLoss 1.1522e+00 (1.0516e+00)\tAcc@1  67.97 ( 69.47)\tAcc@5  92.97 ( 92.71)\n","Epoch: [55][270/391]\tTime  0.090 ( 0.093)\tLoss 1.1988e+00 (1.0578e+00)\tAcc@1  60.94 ( 69.26)\tAcc@5  92.97 ( 92.63)\n","Epoch: [55][300/391]\tTime  0.091 ( 0.093)\tLoss 1.1281e+00 (1.0612e+00)\tAcc@1  69.53 ( 69.15)\tAcc@5  90.62 ( 92.52)\n","Epoch: [55][330/391]\tTime  0.093 ( 0.093)\tLoss 9.5348e-01 (1.0605e+00)\tAcc@1  72.66 ( 69.17)\tAcc@5  93.75 ( 92.56)\n","Epoch: [55][360/391]\tTime  0.090 ( 0.093)\tLoss 1.2098e+00 (1.0620e+00)\tAcc@1  64.06 ( 69.14)\tAcc@5  91.41 ( 92.57)\n","Epoch: [55][390/391]\tTime  0.080 ( 0.093)\tLoss 8.4431e-01 (1.0704e+00)\tAcc@1  70.00 ( 68.93)\tAcc@5  98.75 ( 92.49)\n","==> Train Accuracy: Acc@1 68.928 || Acc@5 92.486\n","==> Test Accuracy:  Acc@1 59.270 || Acc@5 86.300\n","==> 39.47 seconds to train this epoch\n","\n","\n","----- epoch: 56, lr: 0.1 -----\n","Epoch: [56][  0/391]\tTime  0.276 ( 0.276)\tLoss 1.0786e+00 (1.0786e+00)\tAcc@1  67.19 ( 67.19)\tAcc@5  93.75 ( 93.75)\n","Epoch: [56][ 30/391]\tTime  0.093 ( 0.099)\tLoss 8.1477e-01 (9.5018e-01)\tAcc@1  73.44 ( 71.72)\tAcc@5  96.88 ( 94.03)\n","Epoch: [56][ 60/391]\tTime  0.096 ( 0.096)\tLoss 9.8343e-01 (9.7789e-01)\tAcc@1  70.31 ( 71.38)\tAcc@5  96.09 ( 93.38)\n","Epoch: [56][ 90/391]\tTime  0.090 ( 0.095)\tLoss 8.1221e-01 (9.9430e-01)\tAcc@1  72.66 ( 70.79)\tAcc@5  97.66 ( 93.29)\n","Epoch: [56][120/391]\tTime  0.099 ( 0.094)\tLoss 9.5573e-01 (1.0044e+00)\tAcc@1  77.34 ( 70.71)\tAcc@5  92.97 ( 93.14)\n","Epoch: [56][150/391]\tTime  0.093 ( 0.094)\tLoss 1.0962e+00 (1.0075e+00)\tAcc@1  66.41 ( 70.65)\tAcc@5  92.19 ( 93.15)\n","Epoch: [56][180/391]\tTime  0.090 ( 0.094)\tLoss 1.0584e+00 (1.0132e+00)\tAcc@1  69.53 ( 70.38)\tAcc@5  92.19 ( 93.11)\n","Epoch: [56][210/391]\tTime  0.088 ( 0.093)\tLoss 1.1001e+00 (1.0227e+00)\tAcc@1  67.19 ( 70.07)\tAcc@5  92.19 ( 93.01)\n","Epoch: [56][240/391]\tTime  0.091 ( 0.093)\tLoss 1.1568e+00 (1.0303e+00)\tAcc@1  64.06 ( 69.87)\tAcc@5  89.84 ( 92.91)\n","Epoch: [56][270/391]\tTime  0.092 ( 0.093)\tLoss 1.0454e+00 (1.0377e+00)\tAcc@1  68.75 ( 69.73)\tAcc@5  90.62 ( 92.82)\n","Epoch: [56][300/391]\tTime  0.090 ( 0.093)\tLoss 1.0690e+00 (1.0419e+00)\tAcc@1  66.41 ( 69.68)\tAcc@5  95.31 ( 92.76)\n","Epoch: [56][330/391]\tTime  0.103 ( 0.093)\tLoss 1.0731e+00 (1.0472e+00)\tAcc@1  67.97 ( 69.57)\tAcc@5  89.84 ( 92.66)\n","Epoch: [56][360/391]\tTime  0.091 ( 0.093)\tLoss 9.7674e-01 (1.0518e+00)\tAcc@1  71.09 ( 69.41)\tAcc@5  92.97 ( 92.60)\n","Epoch: [56][390/391]\tTime  0.082 ( 0.093)\tLoss 8.1759e-01 (1.0546e+00)\tAcc@1  66.25 ( 69.34)\tAcc@5  97.50 ( 92.56)\n","==> Train Accuracy: Acc@1 69.340 || Acc@5 92.556\n","==> Test Accuracy:  Acc@1 61.440 || Acc@5 88.230\n","==> 39.48 seconds to train this epoch\n","\n","\n","----- epoch: 57, lr: 0.1 -----\n","Epoch: [57][  0/391]\tTime  0.298 ( 0.298)\tLoss 8.1907e-01 (8.1907e-01)\tAcc@1  72.66 ( 72.66)\tAcc@5  96.88 ( 96.88)\n","Epoch: [57][ 30/391]\tTime  0.093 ( 0.101)\tLoss 1.0881e+00 (9.6142e-01)\tAcc@1  66.41 ( 70.97)\tAcc@5  91.41 ( 93.65)\n","Epoch: [57][ 60/391]\tTime  0.091 ( 0.097)\tLoss 9.5520e-01 (9.5456e-01)\tAcc@1  68.75 ( 71.14)\tAcc@5  95.31 ( 93.80)\n","Epoch: [57][ 90/391]\tTime  0.097 ( 0.096)\tLoss 1.1050e+00 (9.7697e-01)\tAcc@1  69.53 ( 70.84)\tAcc@5  89.06 ( 93.64)\n","Epoch: [57][120/391]\tTime  0.089 ( 0.095)\tLoss 1.1306e+00 (9.9091e-01)\tAcc@1  68.75 ( 70.75)\tAcc@5  90.62 ( 93.45)\n","Epoch: [57][150/391]\tTime  0.101 ( 0.095)\tLoss 1.3921e+00 (1.0098e+00)\tAcc@1  60.94 ( 70.22)\tAcc@5  85.94 ( 93.32)\n","Epoch: [57][180/391]\tTime  0.091 ( 0.094)\tLoss 1.1071e+00 (1.0193e+00)\tAcc@1  66.41 ( 70.00)\tAcc@5  92.19 ( 93.37)\n","Epoch: [57][210/391]\tTime  0.089 ( 0.094)\tLoss 9.0933e-01 (1.0277e+00)\tAcc@1  68.75 ( 69.78)\tAcc@5  93.75 ( 93.21)\n","Epoch: [57][240/391]\tTime  0.092 ( 0.094)\tLoss 1.0764e+00 (1.0350e+00)\tAcc@1  66.41 ( 69.54)\tAcc@5  90.62 ( 93.19)\n","Epoch: [57][270/391]\tTime  0.094 ( 0.094)\tLoss 1.2478e+00 (1.0393e+00)\tAcc@1  64.84 ( 69.45)\tAcc@5  89.84 ( 93.08)\n","Epoch: [57][300/391]\tTime  0.094 ( 0.094)\tLoss 8.9118e-01 (1.0411e+00)\tAcc@1  75.78 ( 69.44)\tAcc@5  93.75 ( 93.00)\n","Epoch: [57][330/391]\tTime  0.090 ( 0.093)\tLoss 9.8303e-01 (1.0438e+00)\tAcc@1  73.44 ( 69.39)\tAcc@5  92.97 ( 92.95)\n","Epoch: [57][360/391]\tTime  0.092 ( 0.093)\tLoss 1.2207e+00 (1.0481e+00)\tAcc@1  67.19 ( 69.31)\tAcc@5  90.62 ( 92.91)\n","Epoch: [57][390/391]\tTime  0.082 ( 0.093)\tLoss 9.4152e-01 (1.0540e+00)\tAcc@1  72.50 ( 69.19)\tAcc@5  96.25 ( 92.82)\n","==> Train Accuracy: Acc@1 69.194 || Acc@5 92.822\n","==> Test Accuracy:  Acc@1 57.590 || Acc@5 85.300\n","==> 39.74 seconds to train this epoch\n","\n","\n","----- epoch: 58, lr: 0.1 -----\n","Epoch: [58][  0/391]\tTime  0.398 ( 0.398)\tLoss 9.4367e-01 (9.4367e-01)\tAcc@1  71.88 ( 71.88)\tAcc@5  96.09 ( 96.09)\n","Epoch: [58][ 30/391]\tTime  0.091 ( 0.103)\tLoss 1.1390e+00 (9.9848e-01)\tAcc@1  66.41 ( 71.45)\tAcc@5  92.19 ( 93.15)\n","Epoch: [58][ 60/391]\tTime  0.100 ( 0.098)\tLoss 1.0475e+00 (9.9052e-01)\tAcc@1  72.66 ( 71.64)\tAcc@5  92.97 ( 93.30)\n","Epoch: [58][ 90/391]\tTime  0.093 ( 0.096)\tLoss 8.6930e-01 (9.9591e-01)\tAcc@1  76.56 ( 71.36)\tAcc@5  92.97 ( 93.29)\n","Epoch: [58][120/391]\tTime  0.095 ( 0.095)\tLoss 1.0194e+00 (1.0112e+00)\tAcc@1  66.41 ( 70.72)\tAcc@5  92.97 ( 93.15)\n","Epoch: [58][150/391]\tTime  0.090 ( 0.095)\tLoss 1.0366e+00 (1.0184e+00)\tAcc@1  66.41 ( 70.43)\tAcc@5  92.97 ( 93.08)\n","Epoch: [58][180/391]\tTime  0.092 ( 0.095)\tLoss 1.0563e+00 (1.0242e+00)\tAcc@1  66.41 ( 70.33)\tAcc@5  93.75 ( 93.13)\n","Epoch: [58][210/391]\tTime  0.090 ( 0.094)\tLoss 1.2657e+00 (1.0307e+00)\tAcc@1  60.16 ( 70.00)\tAcc@5  88.28 ( 93.05)\n","Epoch: [58][240/391]\tTime  0.090 ( 0.094)\tLoss 9.1266e-01 (1.0379e+00)\tAcc@1  75.00 ( 69.83)\tAcc@5  90.62 ( 92.98)\n","Epoch: [58][270/391]\tTime  0.089 ( 0.094)\tLoss 1.0165e+00 (1.0422e+00)\tAcc@1  69.53 ( 69.73)\tAcc@5  93.75 ( 92.94)\n","Epoch: [58][300/391]\tTime  0.090 ( 0.094)\tLoss 9.1668e-01 (1.0458e+00)\tAcc@1  75.78 ( 69.79)\tAcc@5  91.41 ( 92.86)\n","Epoch: [58][330/391]\tTime  0.091 ( 0.094)\tLoss 1.0399e+00 (1.0485e+00)\tAcc@1  66.41 ( 69.69)\tAcc@5  95.31 ( 92.82)\n","Epoch: [58][360/391]\tTime  0.092 ( 0.094)\tLoss 1.0836e+00 (1.0514e+00)\tAcc@1  64.06 ( 69.60)\tAcc@5  92.19 ( 92.78)\n","Epoch: [58][390/391]\tTime  0.083 ( 0.093)\tLoss 1.1969e+00 (1.0572e+00)\tAcc@1  66.25 ( 69.43)\tAcc@5  92.50 ( 92.73)\n","==> Train Accuracy: Acc@1 69.434 || Acc@5 92.728\n","==> Test Accuracy:  Acc@1 57.650 || Acc@5 85.010\n","==> 39.75 seconds to train this epoch\n","\n","\n","----- epoch: 59, lr: 0.1 -----\n","Epoch: [59][  0/391]\tTime  0.279 ( 0.279)\tLoss 1.0290e+00 (1.0290e+00)\tAcc@1  75.00 ( 75.00)\tAcc@5  92.97 ( 92.97)\n","Epoch: [59][ 30/391]\tTime  0.086 ( 0.102)\tLoss 9.5503e-01 (9.7554e-01)\tAcc@1  71.88 ( 71.75)\tAcc@5  92.19 ( 93.47)\n","Epoch: [59][ 60/391]\tTime  0.090 ( 0.097)\tLoss 9.7805e-01 (9.8005e-01)\tAcc@1  70.31 ( 71.22)\tAcc@5  94.53 ( 93.58)\n","Epoch: [59][ 90/391]\tTime  0.091 ( 0.096)\tLoss 9.8132e-01 (1.0047e+00)\tAcc@1  71.09 ( 70.58)\tAcc@5  99.22 ( 93.26)\n","Epoch: [59][120/391]\tTime  0.091 ( 0.095)\tLoss 9.7283e-01 (1.0111e+00)\tAcc@1  73.44 ( 70.47)\tAcc@5  94.53 ( 93.13)\n","Epoch: [59][150/391]\tTime  0.093 ( 0.095)\tLoss 1.0423e+00 (1.0227e+00)\tAcc@1  70.31 ( 70.16)\tAcc@5  90.62 ( 93.02)\n","Epoch: [59][180/391]\tTime  0.101 ( 0.094)\tLoss 8.8442e-01 (1.0251e+00)\tAcc@1  74.22 ( 70.21)\tAcc@5  94.53 ( 93.00)\n","Epoch: [59][210/391]\tTime  0.095 ( 0.094)\tLoss 9.9006e-01 (1.0332e+00)\tAcc@1  70.31 ( 69.95)\tAcc@5  92.19 ( 92.95)\n","Epoch: [59][240/391]\tTime  0.093 ( 0.094)\tLoss 1.0096e+00 (1.0340e+00)\tAcc@1  71.88 ( 70.00)\tAcc@5  92.97 ( 92.89)\n","Epoch: [59][270/391]\tTime  0.091 ( 0.094)\tLoss 1.0191e+00 (1.0415e+00)\tAcc@1  71.09 ( 69.75)\tAcc@5  92.97 ( 92.79)\n","Epoch: [59][300/391]\tTime  0.088 ( 0.094)\tLoss 1.0822e+00 (1.0398e+00)\tAcc@1  67.97 ( 69.81)\tAcc@5  92.97 ( 92.88)\n","Epoch: [59][330/391]\tTime  0.098 ( 0.093)\tLoss 1.0404e+00 (1.0439e+00)\tAcc@1  65.62 ( 69.68)\tAcc@5  92.19 ( 92.87)\n","Epoch: [59][360/391]\tTime  0.090 ( 0.093)\tLoss 1.3023e+00 (1.0453e+00)\tAcc@1  65.62 ( 69.68)\tAcc@5  91.41 ( 92.84)\n","Epoch: [59][390/391]\tTime  0.091 ( 0.093)\tLoss 1.0816e+00 (1.0497e+00)\tAcc@1  71.25 ( 69.53)\tAcc@5  88.75 ( 92.74)\n","==> Train Accuracy: Acc@1 69.532 || Acc@5 92.744\n","==> Test Accuracy:  Acc@1 58.670 || Acc@5 86.010\n","==> 39.57 seconds to train this epoch\n","\n","\n","----- epoch: 60, lr: 0.020000000000000004 -----\n","Epoch: [60][  0/391]\tTime  0.288 ( 0.288)\tLoss 1.0859e+00 (1.0859e+00)\tAcc@1  68.75 ( 68.75)\tAcc@5  91.41 ( 91.41)\n","Epoch: [60][ 30/391]\tTime  0.099 ( 0.099)\tLoss 6.7296e-01 (8.3261e-01)\tAcc@1  77.34 ( 75.25)\tAcc@5  97.66 ( 95.21)\n","Epoch: [60][ 60/391]\tTime  0.101 ( 0.096)\tLoss 5.3600e-01 (7.6048e-01)\tAcc@1  83.59 ( 77.41)\tAcc@5  97.66 ( 95.91)\n","Epoch: [60][ 90/391]\tTime  0.093 ( 0.095)\tLoss 5.8039e-01 (7.2300e-01)\tAcc@1  84.38 ( 78.79)\tAcc@5  96.88 ( 96.09)\n","Epoch: [60][120/391]\tTime  0.095 ( 0.094)\tLoss 6.7097e-01 (6.9629e-01)\tAcc@1  78.91 ( 79.49)\tAcc@5  96.09 ( 96.33)\n","Epoch: [60][150/391]\tTime  0.091 ( 0.094)\tLoss 6.5574e-01 (6.7966e-01)\tAcc@1  80.47 ( 80.01)\tAcc@5  96.88 ( 96.45)\n","Epoch: [60][180/391]\tTime  0.087 ( 0.094)\tLoss 5.7928e-01 (6.6508e-01)\tAcc@1  82.03 ( 80.43)\tAcc@5  99.22 ( 96.57)\n","Epoch: [60][210/391]\tTime  0.094 ( 0.094)\tLoss 6.8203e-01 (6.5404e-01)\tAcc@1  83.59 ( 80.67)\tAcc@5  93.75 ( 96.70)\n","Epoch: [60][240/391]\tTime  0.095 ( 0.093)\tLoss 6.0184e-01 (6.4170e-01)\tAcc@1  79.69 ( 81.11)\tAcc@5  96.88 ( 96.76)\n","Epoch: [60][270/391]\tTime  0.095 ( 0.093)\tLoss 5.7605e-01 (6.3463e-01)\tAcc@1  81.25 ( 81.25)\tAcc@5  96.88 ( 96.80)\n","Epoch: [60][300/391]\tTime  0.095 ( 0.093)\tLoss 5.7127e-01 (6.2646e-01)\tAcc@1  78.12 ( 81.46)\tAcc@5  99.22 ( 96.90)\n","Epoch: [60][330/391]\tTime  0.087 ( 0.093)\tLoss 6.7376e-01 (6.2316e-01)\tAcc@1  80.47 ( 81.52)\tAcc@5  94.53 ( 96.88)\n","Epoch: [60][360/391]\tTime  0.101 ( 0.093)\tLoss 5.2716e-01 (6.2098e-01)\tAcc@1  80.47 ( 81.55)\tAcc@5  98.44 ( 96.90)\n","Epoch: [60][390/391]\tTime  0.082 ( 0.093)\tLoss 6.1601e-01 (6.1437e-01)\tAcc@1  83.75 ( 81.77)\tAcc@5  95.00 ( 96.95)\n","==> Train Accuracy: Acc@1 81.766 || Acc@5 96.948\n","==> Test Accuracy:  Acc@1 74.540 || Acc@5 93.660\n","==> 39.47 seconds to train this epoch\n","\n","\n","----- epoch: 61, lr: 0.020000000000000004 -----\n","Epoch: [61][  0/391]\tTime  0.306 ( 0.306)\tLoss 4.2549e-01 (4.2549e-01)\tAcc@1  89.06 ( 89.06)\tAcc@5  96.88 ( 96.88)\n","Epoch: [61][ 30/391]\tTime  0.091 ( 0.100)\tLoss 4.0965e-01 (4.6409e-01)\tAcc@1  85.16 ( 86.06)\tAcc@5  98.44 ( 98.24)\n","Epoch: [61][ 60/391]\tTime  0.088 ( 0.096)\tLoss 4.2387e-01 (4.4819e-01)\tAcc@1  85.94 ( 86.46)\tAcc@5  99.22 ( 98.28)\n","Epoch: [61][ 90/391]\tTime  0.097 ( 0.095)\tLoss 5.2824e-01 (4.4845e-01)\tAcc@1  82.03 ( 86.44)\tAcc@5  98.44 ( 98.15)\n","Epoch: [61][120/391]\tTime  0.125 ( 0.097)\tLoss 6.2381e-01 (4.4841e-01)\tAcc@1  77.34 ( 86.44)\tAcc@5  98.44 ( 98.27)\n","Epoch: [61][150/391]\tTime  0.095 ( 0.096)\tLoss 4.5658e-01 (4.5427e-01)\tAcc@1  86.72 ( 86.33)\tAcc@5  97.66 ( 98.22)\n","Epoch: [61][180/391]\tTime  0.092 ( 0.096)\tLoss 4.8482e-01 (4.5562e-01)\tAcc@1  85.16 ( 86.35)\tAcc@5  96.88 ( 98.22)\n","Epoch: [61][210/391]\tTime  0.087 ( 0.095)\tLoss 4.5910e-01 (4.5578e-01)\tAcc@1  84.38 ( 86.33)\tAcc@5  97.66 ( 98.23)\n","Epoch: [61][240/391]\tTime  0.090 ( 0.095)\tLoss 3.5548e-01 (4.5526e-01)\tAcc@1  86.72 ( 86.32)\tAcc@5  98.44 ( 98.23)\n","Epoch: [61][270/391]\tTime  0.092 ( 0.095)\tLoss 5.0027e-01 (4.5603e-01)\tAcc@1  84.38 ( 86.28)\tAcc@5  97.66 ( 98.24)\n","Epoch: [61][300/391]\tTime  0.091 ( 0.094)\tLoss 4.1112e-01 (4.5572e-01)\tAcc@1  85.94 ( 86.27)\tAcc@5  98.44 ( 98.27)\n","Epoch: [61][330/391]\tTime  0.100 ( 0.094)\tLoss 3.9603e-01 (4.5678e-01)\tAcc@1  86.72 ( 86.24)\tAcc@5  98.44 ( 98.24)\n","Epoch: [61][360/391]\tTime  0.095 ( 0.094)\tLoss 3.2693e-01 (4.5882e-01)\tAcc@1  89.84 ( 86.19)\tAcc@5  98.44 ( 98.20)\n","Epoch: [61][390/391]\tTime  0.082 ( 0.094)\tLoss 5.6299e-01 (4.6059e-01)\tAcc@1  86.25 ( 86.17)\tAcc@5  96.25 ( 98.18)\n","==> Train Accuracy: Acc@1 86.166 || Acc@5 98.178\n","==> Test Accuracy:  Acc@1 74.300 || Acc@5 93.410\n","==> 39.81 seconds to train this epoch\n","\n","\n","----- epoch: 62, lr: 0.020000000000000004 -----\n","Epoch: [62][  0/391]\tTime  0.289 ( 0.289)\tLoss 3.6494e-01 (3.6494e-01)\tAcc@1  88.28 ( 88.28)\tAcc@5  99.22 ( 99.22)\n","Epoch: [62][ 30/391]\tTime  0.094 ( 0.098)\tLoss 3.7732e-01 (3.8401e-01)\tAcc@1  88.28 ( 88.81)\tAcc@5  99.22 ( 98.51)\n","Epoch: [62][ 60/391]\tTime  0.092 ( 0.095)\tLoss 3.3550e-01 (3.7121e-01)\tAcc@1  87.50 ( 89.09)\tAcc@5 100.00 ( 98.66)\n","Epoch: [62][ 90/391]\tTime  0.094 ( 0.095)\tLoss 3.2167e-01 (3.8177e-01)\tAcc@1  89.06 ( 88.62)\tAcc@5  99.22 ( 98.64)\n","Epoch: [62][120/391]\tTime  0.090 ( 0.094)\tLoss 3.6281e-01 (3.8196e-01)\tAcc@1  90.62 ( 88.63)\tAcc@5  99.22 ( 98.66)\n","Epoch: [62][150/391]\tTime  0.089 ( 0.094)\tLoss 2.2096e-01 (3.8243e-01)\tAcc@1  94.53 ( 88.56)\tAcc@5 100.00 ( 98.74)\n","Epoch: [62][180/391]\tTime  0.091 ( 0.094)\tLoss 3.7428e-01 (3.8245e-01)\tAcc@1  89.06 ( 88.55)\tAcc@5  99.22 ( 98.75)\n","Epoch: [62][210/391]\tTime  0.089 ( 0.093)\tLoss 3.0609e-01 (3.8627e-01)\tAcc@1  92.97 ( 88.39)\tAcc@5  98.44 ( 98.76)\n","Epoch: [62][240/391]\tTime  0.093 ( 0.093)\tLoss 2.4695e-01 (3.9011e-01)\tAcc@1  92.97 ( 88.32)\tAcc@5 100.00 ( 98.70)\n","Epoch: [62][270/391]\tTime  0.093 ( 0.093)\tLoss 3.8633e-01 (3.9063e-01)\tAcc@1  86.72 ( 88.33)\tAcc@5  99.22 ( 98.65)\n","Epoch: [62][300/391]\tTime  0.092 ( 0.093)\tLoss 4.7021e-01 (3.9355e-01)\tAcc@1  86.72 ( 88.21)\tAcc@5  99.22 ( 98.65)\n","Epoch: [62][330/391]\tTime  0.098 ( 0.093)\tLoss 3.5105e-01 (3.9527e-01)\tAcc@1  89.84 ( 88.18)\tAcc@5  99.22 ( 98.62)\n","Epoch: [62][360/391]\tTime  0.092 ( 0.093)\tLoss 4.5292e-01 (3.9686e-01)\tAcc@1  91.41 ( 88.10)\tAcc@5  97.66 ( 98.61)\n","Epoch: [62][390/391]\tTime  0.088 ( 0.093)\tLoss 3.7432e-01 (3.9845e-01)\tAcc@1  87.50 ( 88.05)\tAcc@5  97.50 ( 98.61)\n","==> Train Accuracy: Acc@1 88.046 || Acc@5 98.612\n","==> Test Accuracy:  Acc@1 75.020 || Acc@5 93.500\n","==> 39.45 seconds to train this epoch\n","\n","\n","----- epoch: 63, lr: 0.020000000000000004 -----\n","Epoch: [63][  0/391]\tTime  0.297 ( 0.297)\tLoss 4.0073e-01 (4.0073e-01)\tAcc@1  84.38 ( 84.38)\tAcc@5  99.22 ( 99.22)\n","Epoch: [63][ 30/391]\tTime  0.099 ( 0.100)\tLoss 3.2270e-01 (3.3583e-01)\tAcc@1  88.28 ( 90.83)\tAcc@5  99.22 ( 98.77)\n","Epoch: [63][ 60/391]\tTime  0.093 ( 0.096)\tLoss 3.9655e-01 (3.4459e-01)\tAcc@1  91.41 ( 90.45)\tAcc@5  97.66 ( 98.86)\n","Epoch: [63][ 90/391]\tTime  0.102 ( 0.098)\tLoss 3.7615e-01 (3.4840e-01)\tAcc@1  89.06 ( 90.02)\tAcc@5  99.22 ( 98.82)\n","Epoch: [63][120/391]\tTime  0.093 ( 0.097)\tLoss 2.8469e-01 (3.4843e-01)\tAcc@1  90.62 ( 89.79)\tAcc@5 100.00 ( 98.87)\n","Epoch: [63][150/391]\tTime  0.100 ( 0.096)\tLoss 4.1371e-01 (3.5410e-01)\tAcc@1  89.06 ( 89.59)\tAcc@5  98.44 ( 98.84)\n","Epoch: [63][180/391]\tTime  0.091 ( 0.095)\tLoss 4.0468e-01 (3.5201e-01)\tAcc@1  85.16 ( 89.57)\tAcc@5  97.66 ( 98.87)\n","Epoch: [63][210/391]\tTime  0.091 ( 0.095)\tLoss 2.4024e-01 (3.5286e-01)\tAcc@1  92.97 ( 89.47)\tAcc@5  99.22 ( 98.87)\n","Epoch: [63][240/391]\tTime  0.096 ( 0.095)\tLoss 3.0668e-01 (3.5347e-01)\tAcc@1  90.62 ( 89.45)\tAcc@5  99.22 ( 98.84)\n","Epoch: [63][270/391]\tTime  0.097 ( 0.094)\tLoss 3.1112e-01 (3.5582e-01)\tAcc@1  90.62 ( 89.39)\tAcc@5 100.00 ( 98.79)\n","Epoch: [63][300/391]\tTime  0.092 ( 0.094)\tLoss 4.6177e-01 (3.5713e-01)\tAcc@1  89.84 ( 89.37)\tAcc@5  97.66 ( 98.81)\n","Epoch: [63][330/391]\tTime  0.088 ( 0.094)\tLoss 4.3364e-01 (3.5945e-01)\tAcc@1  87.50 ( 89.28)\tAcc@5  96.88 ( 98.80)\n","Epoch: [63][360/391]\tTime  0.102 ( 0.094)\tLoss 3.5786e-01 (3.6108e-01)\tAcc@1  85.16 ( 89.19)\tAcc@5  98.44 ( 98.78)\n","Epoch: [63][390/391]\tTime  0.081 ( 0.094)\tLoss 2.9209e-01 (3.5878e-01)\tAcc@1  88.75 ( 89.21)\tAcc@5  98.75 ( 98.80)\n","==> Train Accuracy: Acc@1 89.210 || Acc@5 98.798\n","==> Test Accuracy:  Acc@1 74.880 || Acc@5 93.790\n","==> 39.75 seconds to train this epoch\n","\n","\n","----- epoch: 64, lr: 0.020000000000000004 -----\n","Epoch: [64][  0/391]\tTime  0.294 ( 0.294)\tLoss 2.8253e-01 (2.8253e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5 100.00 (100.00)\n","Epoch: [64][ 30/391]\tTime  0.094 ( 0.100)\tLoss 2.9190e-01 (3.2380e-01)\tAcc@1  92.97 ( 91.00)\tAcc@5  99.22 ( 99.07)\n","Epoch: [64][ 60/391]\tTime  0.105 ( 0.097)\tLoss 3.0304e-01 (3.1176e-01)\tAcc@1  90.62 ( 90.97)\tAcc@5 100.00 ( 99.22)\n","Epoch: [64][ 90/391]\tTime  0.089 ( 0.095)\tLoss 3.4395e-01 (3.0746e-01)\tAcc@1  90.62 ( 91.07)\tAcc@5  99.22 ( 99.24)\n","Epoch: [64][120/391]\tTime  0.090 ( 0.095)\tLoss 3.4285e-01 (3.0809e-01)\tAcc@1  90.62 ( 91.18)\tAcc@5  98.44 ( 99.19)\n","Epoch: [64][150/391]\tTime  0.101 ( 0.094)\tLoss 2.8012e-01 (3.0766e-01)\tAcc@1  89.84 ( 91.13)\tAcc@5  99.22 ( 99.20)\n","Epoch: [64][180/391]\tTime  0.089 ( 0.094)\tLoss 2.9494e-01 (3.1212e-01)\tAcc@1  89.84 ( 90.92)\tAcc@5  99.22 ( 99.16)\n","Epoch: [64][210/391]\tTime  0.095 ( 0.094)\tLoss 1.7410e-01 (3.1572e-01)\tAcc@1  96.09 ( 90.78)\tAcc@5 100.00 ( 99.12)\n","Epoch: [64][240/391]\tTime  0.088 ( 0.094)\tLoss 2.5540e-01 (3.1525e-01)\tAcc@1  93.75 ( 90.80)\tAcc@5 100.00 ( 99.10)\n","Epoch: [64][270/391]\tTime  0.091 ( 0.094)\tLoss 2.8800e-01 (3.1933e-01)\tAcc@1  88.28 ( 90.64)\tAcc@5 100.00 ( 99.08)\n","Epoch: [64][300/391]\tTime  0.091 ( 0.093)\tLoss 2.4883e-01 (3.2092e-01)\tAcc@1  91.41 ( 90.52)\tAcc@5 100.00 ( 99.08)\n","Epoch: [64][330/391]\tTime  0.096 ( 0.093)\tLoss 4.0167e-01 (3.2338e-01)\tAcc@1  88.28 ( 90.47)\tAcc@5  98.44 ( 99.07)\n","Epoch: [64][360/391]\tTime  0.092 ( 0.093)\tLoss 3.6472e-01 (3.2520e-01)\tAcc@1  87.50 ( 90.41)\tAcc@5 100.00 ( 99.07)\n","Epoch: [64][390/391]\tTime  0.082 ( 0.093)\tLoss 4.9551e-01 (3.2886e-01)\tAcc@1  85.00 ( 90.30)\tAcc@5  96.25 ( 99.03)\n","==> Train Accuracy: Acc@1 90.300 || Acc@5 99.026\n","==> Test Accuracy:  Acc@1 74.390 || Acc@5 93.420\n","==> 39.49 seconds to train this epoch\n","\n","\n","----- epoch: 65, lr: 0.020000000000000004 -----\n","Epoch: [65][  0/391]\tTime  0.284 ( 0.284)\tLoss 2.3494e-01 (2.3494e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5  98.44 ( 98.44)\n","Epoch: [65][ 30/391]\tTime  0.090 ( 0.099)\tLoss 2.1491e-01 (2.7141e-01)\tAcc@1  93.75 ( 92.09)\tAcc@5 100.00 ( 99.45)\n","Epoch: [65][ 60/391]\tTime  0.092 ( 0.096)\tLoss 2.5533e-01 (2.7486e-01)\tAcc@1  94.53 ( 92.01)\tAcc@5  98.44 ( 99.37)\n","Epoch: [65][ 90/391]\tTime  0.095 ( 0.095)\tLoss 4.2401e-01 (2.7612e-01)\tAcc@1  86.72 ( 91.86)\tAcc@5  98.44 ( 99.36)\n","Epoch: [65][120/391]\tTime  0.090 ( 0.095)\tLoss 2.4202e-01 (2.7636e-01)\tAcc@1  93.75 ( 91.77)\tAcc@5  99.22 ( 99.37)\n","Epoch: [65][150/391]\tTime  0.092 ( 0.094)\tLoss 2.3990e-01 (2.8312e-01)\tAcc@1  93.75 ( 91.65)\tAcc@5  99.22 ( 99.33)\n","Epoch: [65][180/391]\tTime  0.098 ( 0.094)\tLoss 2.3451e-01 (2.9034e-01)\tAcc@1  94.53 ( 91.51)\tAcc@5  99.22 ( 99.22)\n","Epoch: [65][210/391]\tTime  0.089 ( 0.094)\tLoss 2.1613e-01 (2.9284e-01)\tAcc@1  94.53 ( 91.44)\tAcc@5 100.00 ( 99.19)\n","Epoch: [65][240/391]\tTime  0.089 ( 0.094)\tLoss 4.1219e-01 (3.0042e-01)\tAcc@1  87.50 ( 91.18)\tAcc@5  98.44 ( 99.14)\n","Epoch: [65][270/391]\tTime  0.092 ( 0.093)\tLoss 1.7707e-01 (3.0389e-01)\tAcc@1  95.31 ( 91.09)\tAcc@5 100.00 ( 99.13)\n","Epoch: [65][300/391]\tTime  0.091 ( 0.093)\tLoss 2.9120e-01 (3.0673e-01)\tAcc@1  91.41 ( 90.97)\tAcc@5  98.44 ( 99.12)\n","Epoch: [65][330/391]\tTime  0.092 ( 0.093)\tLoss 2.0948e-01 (3.0806e-01)\tAcc@1  92.19 ( 90.87)\tAcc@5  99.22 ( 99.13)\n","Epoch: [65][360/391]\tTime  0.079 ( 0.093)\tLoss 2.8823e-01 (3.1099e-01)\tAcc@1  89.84 ( 90.78)\tAcc@5  99.22 ( 99.12)\n","Epoch: [65][390/391]\tTime  0.084 ( 0.093)\tLoss 3.6511e-01 (3.1251e-01)\tAcc@1  90.00 ( 90.71)\tAcc@5  98.75 ( 99.12)\n","==> Train Accuracy: Acc@1 90.710 || Acc@5 99.118\n","==> Test Accuracy:  Acc@1 73.750 || Acc@5 93.360\n","==> 39.48 seconds to train this epoch\n","\n","\n","----- epoch: 66, lr: 0.020000000000000004 -----\n","Epoch: [66][  0/391]\tTime  0.293 ( 0.293)\tLoss 2.8135e-01 (2.8135e-01)\tAcc@1  89.84 ( 89.84)\tAcc@5  99.22 ( 99.22)\n","Epoch: [66][ 30/391]\tTime  0.087 ( 0.099)\tLoss 2.8498e-01 (2.8091e-01)\tAcc@1  89.06 ( 91.94)\tAcc@5  99.22 ( 99.37)\n","Epoch: [66][ 60/391]\tTime  0.094 ( 0.096)\tLoss 2.7999e-01 (2.7618e-01)\tAcc@1  92.97 ( 91.97)\tAcc@5  99.22 ( 99.44)\n","Epoch: [66][ 90/391]\tTime  0.089 ( 0.095)\tLoss 2.1658e-01 (2.7335e-01)\tAcc@1  94.53 ( 92.03)\tAcc@5 100.00 ( 99.40)\n","Epoch: [66][120/391]\tTime  0.090 ( 0.095)\tLoss 1.4425e-01 (2.7656e-01)\tAcc@1  96.09 ( 91.92)\tAcc@5 100.00 ( 99.41)\n","Epoch: [66][150/391]\tTime  0.093 ( 0.094)\tLoss 3.1902e-01 (2.8551e-01)\tAcc@1  89.06 ( 91.69)\tAcc@5 100.00 ( 99.32)\n","Epoch: [66][180/391]\tTime  0.092 ( 0.094)\tLoss 3.3301e-01 (2.8645e-01)\tAcc@1  90.62 ( 91.56)\tAcc@5  98.44 ( 99.28)\n","Epoch: [66][210/391]\tTime  0.091 ( 0.094)\tLoss 2.5980e-01 (2.8867e-01)\tAcc@1  92.19 ( 91.48)\tAcc@5  99.22 ( 99.24)\n","Epoch: [66][240/391]\tTime  0.090 ( 0.094)\tLoss 3.3218e-01 (2.8820e-01)\tAcc@1  89.06 ( 91.45)\tAcc@5  99.22 ( 99.27)\n","Epoch: [66][270/391]\tTime  0.089 ( 0.093)\tLoss 4.1194e-01 (2.8989e-01)\tAcc@1  85.16 ( 91.39)\tAcc@5  99.22 ( 99.26)\n","Epoch: [66][300/391]\tTime  0.098 ( 0.093)\tLoss 3.8405e-01 (2.9112e-01)\tAcc@1  86.72 ( 91.32)\tAcc@5  98.44 ( 99.24)\n","Epoch: [66][330/391]\tTime  0.094 ( 0.093)\tLoss 3.3219e-01 (2.9377e-01)\tAcc@1  91.41 ( 91.23)\tAcc@5  99.22 ( 99.23)\n","Epoch: [66][360/391]\tTime  0.103 ( 0.093)\tLoss 3.8533e-01 (2.9598e-01)\tAcc@1  86.72 ( 91.21)\tAcc@5  99.22 ( 99.21)\n","Epoch: [66][390/391]\tTime  0.083 ( 0.093)\tLoss 4.3104e-01 (2.9767e-01)\tAcc@1  88.75 ( 91.11)\tAcc@5  97.50 ( 99.20)\n","==> Train Accuracy: Acc@1 91.106 || Acc@5 99.204\n","==> Test Accuracy:  Acc@1 73.210 || Acc@5 92.940\n","==> 39.52 seconds to train this epoch\n","\n","\n","----- epoch: 67, lr: 0.020000000000000004 -----\n","Epoch: [67][  0/391]\tTime  0.280 ( 0.280)\tLoss 2.6207e-01 (2.6207e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  97.66 ( 97.66)\n","Epoch: [67][ 30/391]\tTime  0.092 ( 0.099)\tLoss 3.7019e-01 (2.6330e-01)\tAcc@1  89.06 ( 92.09)\tAcc@5  98.44 ( 99.27)\n","Epoch: [67][ 60/391]\tTime  0.094 ( 0.096)\tLoss 2.2557e-01 (2.5307e-01)\tAcc@1  94.53 ( 92.57)\tAcc@5  99.22 ( 99.35)\n","Epoch: [67][ 90/391]\tTime  0.089 ( 0.095)\tLoss 2.1210e-01 (2.6179e-01)\tAcc@1  96.09 ( 92.44)\tAcc@5  99.22 ( 99.29)\n","Epoch: [67][120/391]\tTime  0.091 ( 0.094)\tLoss 3.4175e-01 (2.6393e-01)\tAcc@1  92.19 ( 92.28)\tAcc@5  97.66 ( 99.28)\n","Epoch: [67][150/391]\tTime  0.092 ( 0.094)\tLoss 2.5341e-01 (2.6569e-01)\tAcc@1  91.41 ( 92.22)\tAcc@5 100.00 ( 99.33)\n","Epoch: [67][180/391]\tTime  0.090 ( 0.094)\tLoss 3.3606e-01 (2.7030e-01)\tAcc@1  89.06 ( 92.05)\tAcc@5  99.22 ( 99.31)\n","Epoch: [67][210/391]\tTime  0.092 ( 0.093)\tLoss 2.2605e-01 (2.7299e-01)\tAcc@1  91.41 ( 91.92)\tAcc@5 100.00 ( 99.30)\n","Epoch: [67][240/391]\tTime  0.091 ( 0.093)\tLoss 2.7977e-01 (2.7600e-01)\tAcc@1  91.41 ( 91.77)\tAcc@5 100.00 ( 99.29)\n","Epoch: [67][270/391]\tTime  0.088 ( 0.093)\tLoss 3.1869e-01 (2.8179e-01)\tAcc@1  89.84 ( 91.58)\tAcc@5  99.22 ( 99.27)\n","Epoch: [67][300/391]\tTime  0.089 ( 0.093)\tLoss 4.0065e-01 (2.8547e-01)\tAcc@1  86.72 ( 91.48)\tAcc@5  98.44 ( 99.27)\n","Epoch: [67][330/391]\tTime  0.094 ( 0.093)\tLoss 3.1378e-01 (2.8903e-01)\tAcc@1  88.28 ( 91.35)\tAcc@5  98.44 ( 99.26)\n","Epoch: [67][360/391]\tTime  0.102 ( 0.093)\tLoss 2.9102e-01 (2.9251e-01)\tAcc@1  92.19 ( 91.27)\tAcc@5  98.44 ( 99.23)\n","Epoch: [67][390/391]\tTime  0.082 ( 0.093)\tLoss 3.4102e-01 (2.9275e-01)\tAcc@1  88.75 ( 91.26)\tAcc@5 100.00 ( 99.24)\n","==> Train Accuracy: Acc@1 91.262 || Acc@5 99.242\n","==> Test Accuracy:  Acc@1 73.810 || Acc@5 93.090\n","==> 39.49 seconds to train this epoch\n","\n","\n","----- epoch: 68, lr: 0.020000000000000004 -----\n","Epoch: [68][  0/391]\tTime  0.279 ( 0.279)\tLoss 3.1584e-01 (3.1584e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5  99.22 ( 99.22)\n","Epoch: [68][ 30/391]\tTime  0.096 ( 0.099)\tLoss 2.1325e-01 (2.5081e-01)\tAcc@1  92.97 ( 92.62)\tAcc@5 100.00 ( 99.45)\n","Epoch: [68][ 60/391]\tTime  0.090 ( 0.096)\tLoss 2.4385e-01 (2.5380e-01)\tAcc@1  94.53 ( 92.51)\tAcc@5 100.00 ( 99.39)\n","Epoch: [68][ 90/391]\tTime  0.092 ( 0.095)\tLoss 3.0451e-01 (2.5726e-01)\tAcc@1  89.06 ( 92.36)\tAcc@5 100.00 ( 99.47)\n","Epoch: [68][120/391]\tTime  0.092 ( 0.094)\tLoss 3.0333e-01 (2.6452e-01)\tAcc@1  92.19 ( 92.14)\tAcc@5  98.44 ( 99.43)\n","Epoch: [68][150/391]\tTime  0.087 ( 0.094)\tLoss 2.3490e-01 (2.7237e-01)\tAcc@1  90.62 ( 91.96)\tAcc@5  99.22 ( 99.39)\n","Epoch: [68][180/391]\tTime  0.096 ( 0.094)\tLoss 2.7698e-01 (2.7571e-01)\tAcc@1  92.19 ( 91.80)\tAcc@5  99.22 ( 99.38)\n","Epoch: [68][210/391]\tTime  0.093 ( 0.094)\tLoss 3.6843e-01 (2.8017e-01)\tAcc@1  89.06 ( 91.62)\tAcc@5  99.22 ( 99.34)\n","Epoch: [68][240/391]\tTime  0.090 ( 0.093)\tLoss 3.4353e-01 (2.7966e-01)\tAcc@1  91.41 ( 91.63)\tAcc@5  97.66 ( 99.32)\n","Epoch: [68][270/391]\tTime  0.091 ( 0.093)\tLoss 3.8748e-01 (2.8163e-01)\tAcc@1  87.50 ( 91.56)\tAcc@5  99.22 ( 99.29)\n","Epoch: [68][300/391]\tTime  0.094 ( 0.093)\tLoss 1.9708e-01 (2.8391e-01)\tAcc@1  93.75 ( 91.45)\tAcc@5 100.00 ( 99.30)\n","Epoch: [68][330/391]\tTime  0.091 ( 0.093)\tLoss 3.2211e-01 (2.8572e-01)\tAcc@1  89.06 ( 91.37)\tAcc@5  99.22 ( 99.29)\n","Epoch: [68][360/391]\tTime  0.089 ( 0.093)\tLoss 3.2115e-01 (2.8843e-01)\tAcc@1  92.19 ( 91.30)\tAcc@5  98.44 ( 99.28)\n","Epoch: [68][390/391]\tTime  0.084 ( 0.093)\tLoss 3.6885e-01 (2.9021e-01)\tAcc@1  87.50 ( 91.24)\tAcc@5 100.00 ( 99.26)\n","==> Train Accuracy: Acc@1 91.236 || Acc@5 99.264\n","==> Test Accuracy:  Acc@1 72.080 || Acc@5 92.140\n","==> 39.46 seconds to train this epoch\n","\n","\n","----- epoch: 69, lr: 0.020000000000000004 -----\n","Epoch: [69][  0/391]\tTime  0.285 ( 0.285)\tLoss 3.6438e-01 (3.6438e-01)\tAcc@1  87.50 ( 87.50)\tAcc@5  97.66 ( 97.66)\n","Epoch: [69][ 30/391]\tTime  0.101 ( 0.099)\tLoss 1.8551e-01 (2.4241e-01)\tAcc@1  96.09 ( 93.32)\tAcc@5  98.44 ( 99.42)\n","Epoch: [69][ 60/391]\tTime  0.090 ( 0.096)\tLoss 3.4426e-01 (2.5673e-01)\tAcc@1  89.06 ( 92.78)\tAcc@5  98.44 ( 99.27)\n","Epoch: [69][ 90/391]\tTime  0.093 ( 0.095)\tLoss 3.8448e-01 (2.5301e-01)\tAcc@1  89.06 ( 92.80)\tAcc@5  98.44 ( 99.36)\n","Epoch: [69][120/391]\tTime  0.094 ( 0.094)\tLoss 2.9158e-01 (2.5488e-01)\tAcc@1  90.62 ( 92.69)\tAcc@5  99.22 ( 99.35)\n","Epoch: [69][150/391]\tTime  0.090 ( 0.094)\tLoss 2.9127e-01 (2.5670e-01)\tAcc@1  89.06 ( 92.53)\tAcc@5 100.00 ( 99.38)\n","Epoch: [69][180/391]\tTime  0.091 ( 0.094)\tLoss 3.3164e-01 (2.6122e-01)\tAcc@1  89.06 ( 92.42)\tAcc@5 100.00 ( 99.38)\n","Epoch: [69][210/391]\tTime  0.086 ( 0.094)\tLoss 1.6309e-01 (2.6470e-01)\tAcc@1  96.09 ( 92.29)\tAcc@5 100.00 ( 99.40)\n","Epoch: [69][240/391]\tTime  0.093 ( 0.093)\tLoss 2.5650e-01 (2.6896e-01)\tAcc@1  92.19 ( 92.12)\tAcc@5 100.00 ( 99.37)\n","Epoch: [69][270/391]\tTime  0.093 ( 0.093)\tLoss 3.2193e-01 (2.7478e-01)\tAcc@1  92.19 ( 91.82)\tAcc@5  98.44 ( 99.38)\n","Epoch: [69][300/391]\tTime  0.091 ( 0.093)\tLoss 2.1475e-01 (2.7933e-01)\tAcc@1  94.53 ( 91.73)\tAcc@5 100.00 ( 99.35)\n","Epoch: [69][330/391]\tTime  0.096 ( 0.093)\tLoss 2.0534e-01 (2.8286e-01)\tAcc@1  93.75 ( 91.63)\tAcc@5 100.00 ( 99.34)\n","Epoch: [69][360/391]\tTime  0.098 ( 0.093)\tLoss 2.3485e-01 (2.8720e-01)\tAcc@1  93.75 ( 91.47)\tAcc@5  99.22 ( 99.32)\n","Epoch: [69][390/391]\tTime  0.082 ( 0.093)\tLoss 5.8667e-01 (2.9028e-01)\tAcc@1  86.25 ( 91.37)\tAcc@5  98.75 ( 99.33)\n","==> Train Accuracy: Acc@1 91.368 || Acc@5 99.330\n","==> Test Accuracy:  Acc@1 72.590 || Acc@5 92.530\n","==> 39.41 seconds to train this epoch\n","\n","\n","----- epoch: 70, lr: 0.020000000000000004 -----\n","Epoch: [70][  0/391]\tTime  0.270 ( 0.270)\tLoss 1.9426e-01 (1.9426e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n","Epoch: [70][ 30/391]\tTime  0.092 ( 0.099)\tLoss 2.0308e-01 (2.6546e-01)\tAcc@1  94.53 ( 92.14)\tAcc@5 100.00 ( 99.52)\n","Epoch: [70][ 60/391]\tTime  0.089 ( 0.096)\tLoss 2.3213e-01 (2.5508e-01)\tAcc@1  92.97 ( 92.56)\tAcc@5 100.00 ( 99.53)\n","Epoch: [70][ 90/391]\tTime  0.089 ( 0.095)\tLoss 2.3162e-01 (2.5570e-01)\tAcc@1  92.97 ( 92.51)\tAcc@5 100.00 ( 99.49)\n","Epoch: [70][120/391]\tTime  0.092 ( 0.094)\tLoss 3.0662e-01 (2.6082e-01)\tAcc@1  92.19 ( 92.43)\tAcc@5  98.44 ( 99.43)\n","Epoch: [70][150/391]\tTime  0.090 ( 0.094)\tLoss 3.5565e-01 (2.6381e-01)\tAcc@1  89.84 ( 92.35)\tAcc@5  97.66 ( 99.39)\n","Epoch: [70][180/391]\tTime  0.091 ( 0.094)\tLoss 4.2452e-01 (2.6875e-01)\tAcc@1  89.06 ( 92.15)\tAcc@5 100.00 ( 99.42)\n","Epoch: [70][210/391]\tTime  0.099 ( 0.094)\tLoss 3.4658e-01 (2.7223e-01)\tAcc@1  89.06 ( 91.97)\tAcc@5 100.00 ( 99.39)\n","Epoch: [70][240/391]\tTime  0.092 ( 0.094)\tLoss 3.6848e-01 (2.7740e-01)\tAcc@1  89.06 ( 91.77)\tAcc@5  97.66 ( 99.36)\n","Epoch: [70][270/391]\tTime  0.104 ( 0.094)\tLoss 2.9073e-01 (2.8061e-01)\tAcc@1  91.41 ( 91.61)\tAcc@5  98.44 ( 99.36)\n","Epoch: [70][300/391]\tTime  0.092 ( 0.093)\tLoss 2.6976e-01 (2.8323e-01)\tAcc@1  92.19 ( 91.53)\tAcc@5  98.44 ( 99.35)\n","Epoch: [70][330/391]\tTime  0.090 ( 0.093)\tLoss 4.0174e-01 (2.8845e-01)\tAcc@1  84.38 ( 91.31)\tAcc@5 100.00 ( 99.34)\n","Epoch: [70][360/391]\tTime  0.093 ( 0.093)\tLoss 2.5626e-01 (2.9176e-01)\tAcc@1  93.75 ( 91.23)\tAcc@5  99.22 ( 99.33)\n","Epoch: [70][390/391]\tTime  0.082 ( 0.093)\tLoss 3.0231e-01 (2.9569e-01)\tAcc@1  91.25 ( 91.12)\tAcc@5  98.75 ( 99.31)\n","==> Train Accuracy: Acc@1 91.118 || Acc@5 99.308\n","==> Test Accuracy:  Acc@1 72.000 || Acc@5 92.550\n","==> 39.51 seconds to train this epoch\n","\n","\n","----- epoch: 71, lr: 0.020000000000000004 -----\n","Epoch: [71][  0/391]\tTime  0.264 ( 0.264)\tLoss 2.4002e-01 (2.4002e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.22 ( 99.22)\n","Epoch: [71][ 30/391]\tTime  0.090 ( 0.100)\tLoss 2.0895e-01 (2.7090e-01)\tAcc@1  93.75 ( 91.76)\tAcc@5 100.00 ( 99.55)\n","Epoch: [71][ 60/391]\tTime  0.092 ( 0.096)\tLoss 2.4366e-01 (2.6182e-01)\tAcc@1  92.19 ( 92.03)\tAcc@5  99.22 ( 99.62)\n","Epoch: [71][ 90/391]\tTime  0.099 ( 0.095)\tLoss 2.2865e-01 (2.6710e-01)\tAcc@1  93.75 ( 91.84)\tAcc@5  99.22 ( 99.54)\n","Epoch: [71][120/391]\tTime  0.094 ( 0.094)\tLoss 2.6919e-01 (2.6877e-01)\tAcc@1  92.19 ( 91.85)\tAcc@5  99.22 ( 99.47)\n","Epoch: [71][150/391]\tTime  0.091 ( 0.094)\tLoss 2.7327e-01 (2.6718e-01)\tAcc@1  91.41 ( 91.99)\tAcc@5  98.44 ( 99.48)\n","Epoch: [71][180/391]\tTime  0.093 ( 0.094)\tLoss 2.7848e-01 (2.7210e-01)\tAcc@1  90.62 ( 91.90)\tAcc@5 100.00 ( 99.40)\n","Epoch: [71][210/391]\tTime  0.084 ( 0.094)\tLoss 3.1858e-01 (2.7195e-01)\tAcc@1  87.50 ( 91.93)\tAcc@5 100.00 ( 99.39)\n","Epoch: [71][240/391]\tTime  0.093 ( 0.093)\tLoss 2.3175e-01 (2.7286e-01)\tAcc@1  92.19 ( 91.91)\tAcc@5  99.22 ( 99.39)\n","Epoch: [71][270/391]\tTime  0.093 ( 0.093)\tLoss 2.6189e-01 (2.7622e-01)\tAcc@1  92.97 ( 91.75)\tAcc@5 100.00 ( 99.38)\n","Epoch: [71][300/391]\tTime  0.092 ( 0.093)\tLoss 4.0262e-01 (2.7929e-01)\tAcc@1  83.59 ( 91.65)\tAcc@5  98.44 ( 99.36)\n","Epoch: [71][330/391]\tTime  0.090 ( 0.093)\tLoss 2.8578e-01 (2.7933e-01)\tAcc@1  91.41 ( 91.64)\tAcc@5 100.00 ( 99.37)\n","Epoch: [71][360/391]\tTime  0.101 ( 0.093)\tLoss 4.1456e-01 (2.8218e-01)\tAcc@1  88.28 ( 91.57)\tAcc@5  99.22 ( 99.37)\n","Epoch: [71][390/391]\tTime  0.084 ( 0.093)\tLoss 4.3262e-01 (2.8686e-01)\tAcc@1  87.50 ( 91.40)\tAcc@5  98.75 ( 99.34)\n","==> Train Accuracy: Acc@1 91.400 || Acc@5 99.336\n","==> Test Accuracy:  Acc@1 72.130 || Acc@5 92.550\n","==> 39.46 seconds to train this epoch\n","\n","\n","----- epoch: 72, lr: 0.020000000000000004 -----\n","Epoch: [72][  0/391]\tTime  0.286 ( 0.286)\tLoss 2.5238e-01 (2.5238e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  98.44 ( 98.44)\n","Epoch: [72][ 30/391]\tTime  0.100 ( 0.099)\tLoss 2.3295e-01 (2.5900e-01)\tAcc@1  91.41 ( 92.44)\tAcc@5 100.00 ( 99.32)\n","Epoch: [72][ 60/391]\tTime  0.090 ( 0.096)\tLoss 1.9862e-01 (2.6246e-01)\tAcc@1  96.09 ( 92.32)\tAcc@5 100.00 ( 99.37)\n","Epoch: [72][ 90/391]\tTime  0.092 ( 0.095)\tLoss 3.7980e-01 (2.6834e-01)\tAcc@1  89.06 ( 92.16)\tAcc@5  98.44 ( 99.34)\n","Epoch: [72][120/391]\tTime  0.089 ( 0.094)\tLoss 2.7402e-01 (2.6467e-01)\tAcc@1  92.19 ( 92.28)\tAcc@5  99.22 ( 99.39)\n","Epoch: [72][150/391]\tTime  0.090 ( 0.094)\tLoss 2.8868e-01 (2.6384e-01)\tAcc@1  90.62 ( 92.24)\tAcc@5 100.00 ( 99.39)\n","Epoch: [72][180/391]\tTime  0.091 ( 0.094)\tLoss 2.4997e-01 (2.6480e-01)\tAcc@1  93.75 ( 92.22)\tAcc@5  99.22 ( 99.38)\n","Epoch: [72][210/391]\tTime  0.089 ( 0.094)\tLoss 2.3519e-01 (2.6684e-01)\tAcc@1  92.19 ( 92.14)\tAcc@5 100.00 ( 99.38)\n","Epoch: [72][240/391]\tTime  0.094 ( 0.093)\tLoss 3.5410e-01 (2.7119e-01)\tAcc@1  87.50 ( 91.92)\tAcc@5  99.22 ( 99.36)\n","Epoch: [72][270/391]\tTime  0.092 ( 0.093)\tLoss 2.9340e-01 (2.7538e-01)\tAcc@1  91.41 ( 91.79)\tAcc@5  99.22 ( 99.35)\n","Epoch: [72][300/391]\tTime  0.091 ( 0.093)\tLoss 3.5567e-01 (2.8066e-01)\tAcc@1  90.62 ( 91.64)\tAcc@5  99.22 ( 99.34)\n","Epoch: [72][330/391]\tTime  0.095 ( 0.093)\tLoss 3.1464e-01 (2.8475e-01)\tAcc@1  92.19 ( 91.48)\tAcc@5  99.22 ( 99.33)\n","Epoch: [72][360/391]\tTime  0.093 ( 0.093)\tLoss 4.2063e-01 (2.8891e-01)\tAcc@1  87.50 ( 91.31)\tAcc@5  97.66 ( 99.31)\n","Epoch: [72][390/391]\tTime  0.081 ( 0.093)\tLoss 2.6854e-01 (2.9265e-01)\tAcc@1  90.00 ( 91.16)\tAcc@5 100.00 ( 99.29)\n","==> Train Accuracy: Acc@1 91.162 || Acc@5 99.286\n","==> Test Accuracy:  Acc@1 71.070 || Acc@5 91.930\n","==> 39.45 seconds to train this epoch\n","\n","\n","----- epoch: 73, lr: 0.020000000000000004 -----\n","Epoch: [73][  0/391]\tTime  0.296 ( 0.296)\tLoss 3.1266e-01 (3.1266e-01)\tAcc@1  89.06 ( 89.06)\tAcc@5  99.22 ( 99.22)\n","Epoch: [73][ 30/391]\tTime  0.091 ( 0.099)\tLoss 1.8481e-01 (2.5596e-01)\tAcc@1  94.53 ( 93.09)\tAcc@5 100.00 ( 99.40)\n","Epoch: [73][ 60/391]\tTime  0.096 ( 0.096)\tLoss 2.9971e-01 (2.5051e-01)\tAcc@1  91.41 ( 92.97)\tAcc@5  98.44 ( 99.42)\n","Epoch: [73][ 90/391]\tTime  0.085 ( 0.095)\tLoss 2.5629e-01 (2.5867e-01)\tAcc@1  92.19 ( 92.54)\tAcc@5 100.00 ( 99.42)\n","Epoch: [73][120/391]\tTime  0.089 ( 0.094)\tLoss 2.7342e-01 (2.6396e-01)\tAcc@1  91.41 ( 92.33)\tAcc@5 100.00 ( 99.41)\n","Epoch: [73][150/391]\tTime  0.090 ( 0.094)\tLoss 3.7745e-01 (2.6601e-01)\tAcc@1  87.50 ( 92.25)\tAcc@5  98.44 ( 99.36)\n","Epoch: [73][180/391]\tTime  0.088 ( 0.094)\tLoss 3.0534e-01 (2.6840e-01)\tAcc@1  90.62 ( 92.17)\tAcc@5  99.22 ( 99.37)\n","Epoch: [73][210/391]\tTime  0.093 ( 0.093)\tLoss 3.1213e-01 (2.7169e-01)\tAcc@1  87.50 ( 92.04)\tAcc@5 100.00 ( 99.37)\n","Epoch: [73][240/391]\tTime  0.094 ( 0.093)\tLoss 3.3229e-01 (2.7474e-01)\tAcc@1  89.84 ( 91.92)\tAcc@5 100.00 ( 99.39)\n","Epoch: [73][270/391]\tTime  0.092 ( 0.093)\tLoss 3.3272e-01 (2.7873e-01)\tAcc@1  88.28 ( 91.77)\tAcc@5  98.44 ( 99.39)\n","Epoch: [73][300/391]\tTime  0.089 ( 0.093)\tLoss 5.0841e-01 (2.8292e-01)\tAcc@1  85.16 ( 91.63)\tAcc@5  98.44 ( 99.35)\n","Epoch: [73][330/391]\tTime  0.090 ( 0.093)\tLoss 2.7188e-01 (2.8774e-01)\tAcc@1  92.97 ( 91.46)\tAcc@5  99.22 ( 99.32)\n","Epoch: [73][360/391]\tTime  0.093 ( 0.093)\tLoss 2.2544e-01 (2.8983e-01)\tAcc@1  94.53 ( 91.42)\tAcc@5 100.00 ( 99.32)\n","Epoch: [73][390/391]\tTime  0.083 ( 0.093)\tLoss 3.8235e-01 (2.9033e-01)\tAcc@1  85.00 ( 91.39)\tAcc@5 100.00 ( 99.31)\n","==> Train Accuracy: Acc@1 91.392 || Acc@5 99.308\n","==> Test Accuracy:  Acc@1 72.010 || Acc@5 92.200\n","==> 39.46 seconds to train this epoch\n","\n","\n","----- epoch: 74, lr: 0.020000000000000004 -----\n","Epoch: [74][  0/391]\tTime  0.268 ( 0.268)\tLoss 2.8100e-01 (2.8100e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5  99.22 ( 99.22)\n","Epoch: [74][ 30/391]\tTime  0.092 ( 0.099)\tLoss 2.4899e-01 (2.8201e-01)\tAcc@1  92.97 ( 91.66)\tAcc@5 100.00 ( 99.32)\n","Epoch: [74][ 60/391]\tTime  0.094 ( 0.096)\tLoss 1.4329e-01 (2.6626e-01)\tAcc@1  96.09 ( 92.30)\tAcc@5 100.00 ( 99.35)\n","Epoch: [74][ 90/391]\tTime  0.092 ( 0.095)\tLoss 2.8219e-01 (2.6195e-01)\tAcc@1  92.19 ( 92.21)\tAcc@5  98.44 ( 99.37)\n","Epoch: [74][120/391]\tTime  0.092 ( 0.095)\tLoss 3.0503e-01 (2.6136e-01)\tAcc@1  89.06 ( 92.16)\tAcc@5  98.44 ( 99.39)\n","Epoch: [74][150/391]\tTime  0.092 ( 0.094)\tLoss 2.9993e-01 (2.6643e-01)\tAcc@1  91.41 ( 91.97)\tAcc@5  99.22 ( 99.39)\n","Epoch: [74][180/391]\tTime  0.090 ( 0.094)\tLoss 3.9835e-01 (2.7541e-01)\tAcc@1  89.06 ( 91.75)\tAcc@5 100.00 ( 99.35)\n","Epoch: [74][210/391]\tTime  0.096 ( 0.094)\tLoss 2.4780e-01 (2.7855e-01)\tAcc@1  90.62 ( 91.70)\tAcc@5 100.00 ( 99.33)\n","Epoch: [74][240/391]\tTime  0.091 ( 0.094)\tLoss 3.4577e-01 (2.8320e-01)\tAcc@1  87.50 ( 91.56)\tAcc@5 100.00 ( 99.31)\n","Epoch: [74][270/391]\tTime  0.089 ( 0.094)\tLoss 3.3713e-01 (2.8747e-01)\tAcc@1  88.28 ( 91.44)\tAcc@5  99.22 ( 99.29)\n","Epoch: [74][300/391]\tTime  0.090 ( 0.093)\tLoss 3.6142e-01 (2.9065e-01)\tAcc@1  89.84 ( 91.29)\tAcc@5  98.44 ( 99.28)\n","Epoch: [74][330/391]\tTime  0.094 ( 0.093)\tLoss 2.3057e-01 (2.9390e-01)\tAcc@1  94.53 ( 91.17)\tAcc@5 100.00 ( 99.28)\n","Epoch: [74][360/391]\tTime  0.093 ( 0.093)\tLoss 5.5187e-01 (2.9944e-01)\tAcc@1  83.59 ( 90.96)\tAcc@5  99.22 ( 99.25)\n","Epoch: [74][390/391]\tTime  0.082 ( 0.093)\tLoss 6.6513e-01 (3.0627e-01)\tAcc@1  81.25 ( 90.74)\tAcc@5  96.25 ( 99.24)\n","==> Train Accuracy: Acc@1 90.744 || Acc@5 99.244\n","==> Test Accuracy:  Acc@1 70.980 || Acc@5 91.590\n","==> 39.50 seconds to train this epoch\n","\n","\n","----- epoch: 75, lr: 0.020000000000000004 -----\n","Epoch: [75][  0/391]\tTime  0.296 ( 0.296)\tLoss 2.5097e-01 (2.5097e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n","Epoch: [75][ 30/391]\tTime  0.094 ( 0.100)\tLoss 1.8397e-01 (2.7980e-01)\tAcc@1  96.09 ( 91.43)\tAcc@5 100.00 ( 99.45)\n","Epoch: [75][ 60/391]\tTime  0.092 ( 0.096)\tLoss 2.4017e-01 (2.8792e-01)\tAcc@1  93.75 ( 91.42)\tAcc@5  99.22 ( 99.41)\n","Epoch: [75][ 90/391]\tTime  0.094 ( 0.095)\tLoss 2.9667e-01 (2.8509e-01)\tAcc@1  92.97 ( 91.43)\tAcc@5  99.22 ( 99.43)\n","Epoch: [75][120/391]\tTime  0.090 ( 0.094)\tLoss 2.7514e-01 (2.8164e-01)\tAcc@1  92.97 ( 91.54)\tAcc@5  99.22 ( 99.39)\n","Epoch: [75][150/391]\tTime  0.092 ( 0.094)\tLoss 2.3868e-01 (2.8479e-01)\tAcc@1  94.53 ( 91.42)\tAcc@5 100.00 ( 99.41)\n","Epoch: [75][180/391]\tTime  0.093 ( 0.094)\tLoss 3.1923e-01 (2.8862e-01)\tAcc@1  92.19 ( 91.29)\tAcc@5 100.00 ( 99.37)\n","Epoch: [75][210/391]\tTime  0.094 ( 0.094)\tLoss 1.7396e-01 (2.8649e-01)\tAcc@1  96.09 ( 91.41)\tAcc@5 100.00 ( 99.39)\n","Epoch: [75][240/391]\tTime  0.097 ( 0.093)\tLoss 3.0968e-01 (2.9052e-01)\tAcc@1  89.06 ( 91.26)\tAcc@5 100.00 ( 99.36)\n","Epoch: [75][270/391]\tTime  0.092 ( 0.093)\tLoss 2.8062e-01 (2.9633e-01)\tAcc@1  92.19 ( 91.07)\tAcc@5  99.22 ( 99.31)\n","Epoch: [75][300/391]\tTime  0.092 ( 0.093)\tLoss 2.5729e-01 (3.0115e-01)\tAcc@1  92.97 ( 90.95)\tAcc@5  99.22 ( 99.27)\n","Epoch: [75][330/391]\tTime  0.091 ( 0.093)\tLoss 3.2443e-01 (3.0516e-01)\tAcc@1  90.62 ( 90.79)\tAcc@5  97.66 ( 99.26)\n","Epoch: [75][360/391]\tTime  0.093 ( 0.093)\tLoss 3.8974e-01 (3.0810e-01)\tAcc@1  88.28 ( 90.71)\tAcc@5  98.44 ( 99.24)\n","Epoch: [75][390/391]\tTime  0.081 ( 0.093)\tLoss 4.6980e-01 (3.1075e-01)\tAcc@1  85.00 ( 90.66)\tAcc@5  98.75 ( 99.24)\n","==> Train Accuracy: Acc@1 90.660 || Acc@5 99.242\n","==> Test Accuracy:  Acc@1 70.930 || Acc@5 91.630\n","==> 39.50 seconds to train this epoch\n","\n","\n","----- epoch: 76, lr: 0.020000000000000004 -----\n","Epoch: [76][  0/391]\tTime  0.267 ( 0.267)\tLoss 3.4101e-01 (3.4101e-01)\tAcc@1  88.28 ( 88.28)\tAcc@5 100.00 (100.00)\n","Epoch: [76][ 30/391]\tTime  0.090 ( 0.100)\tLoss 3.0290e-01 (2.8062e-01)\tAcc@1  92.19 ( 91.71)\tAcc@5  98.44 ( 99.37)\n","Epoch: [76][ 60/391]\tTime  0.091 ( 0.097)\tLoss 2.7845e-01 (2.7953e-01)\tAcc@1  92.97 ( 91.76)\tAcc@5 100.00 ( 99.37)\n","Epoch: [76][ 90/391]\tTime  0.090 ( 0.096)\tLoss 3.0990e-01 (2.7618e-01)\tAcc@1  91.41 ( 91.82)\tAcc@5  99.22 ( 99.30)\n","Epoch: [76][120/391]\tTime  0.089 ( 0.095)\tLoss 2.5264e-01 (2.7410e-01)\tAcc@1  92.97 ( 91.84)\tAcc@5  99.22 ( 99.34)\n","Epoch: [76][150/391]\tTime  0.094 ( 0.094)\tLoss 2.6826e-01 (2.7573e-01)\tAcc@1  90.62 ( 91.71)\tAcc@5  99.22 ( 99.36)\n","Epoch: [76][180/391]\tTime  0.087 ( 0.094)\tLoss 3.0527e-01 (2.8308e-01)\tAcc@1  89.06 ( 91.53)\tAcc@5  99.22 ( 99.29)\n","Epoch: [76][210/391]\tTime  0.088 ( 0.094)\tLoss 3.3221e-01 (2.8783e-01)\tAcc@1  89.84 ( 91.39)\tAcc@5  99.22 ( 99.29)\n","Epoch: [76][240/391]\tTime  0.091 ( 0.094)\tLoss 2.7020e-01 (2.9023e-01)\tAcc@1  92.97 ( 91.33)\tAcc@5  98.44 ( 99.28)\n","Epoch: [76][270/391]\tTime  0.089 ( 0.094)\tLoss 2.4768e-01 (2.9132e-01)\tAcc@1  92.97 ( 91.28)\tAcc@5 100.00 ( 99.29)\n","Epoch: [76][300/391]\tTime  0.089 ( 0.093)\tLoss 3.6638e-01 (2.9293e-01)\tAcc@1  88.28 ( 91.23)\tAcc@5  99.22 ( 99.27)\n","Epoch: [76][330/391]\tTime  0.097 ( 0.093)\tLoss 4.9258e-01 (2.9697e-01)\tAcc@1  85.94 ( 91.10)\tAcc@5  97.66 ( 99.28)\n","Epoch: [76][360/391]\tTime  0.092 ( 0.093)\tLoss 4.3597e-01 (3.0112e-01)\tAcc@1  83.59 ( 90.98)\tAcc@5  99.22 ( 99.27)\n","Epoch: [76][390/391]\tTime  0.082 ( 0.093)\tLoss 3.6677e-01 (3.0448e-01)\tAcc@1  87.50 ( 90.84)\tAcc@5  98.75 ( 99.25)\n","==> Train Accuracy: Acc@1 90.842 || Acc@5 99.246\n","==> Test Accuracy:  Acc@1 70.580 || Acc@5 91.580\n","==> 39.53 seconds to train this epoch\n","\n","\n","----- epoch: 77, lr: 0.020000000000000004 -----\n","Epoch: [77][  0/391]\tTime  0.266 ( 0.266)\tLoss 2.6944e-01 (2.6944e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  99.22 ( 99.22)\n","Epoch: [77][ 30/391]\tTime  0.090 ( 0.100)\tLoss 3.6008e-01 (2.7593e-01)\tAcc@1  88.28 ( 91.91)\tAcc@5  99.22 ( 99.37)\n","Epoch: [77][ 60/391]\tTime  0.090 ( 0.096)\tLoss 2.8141e-01 (2.6776e-01)\tAcc@1  92.19 ( 92.10)\tAcc@5  99.22 ( 99.42)\n","Epoch: [77][ 90/391]\tTime  0.091 ( 0.095)\tLoss 2.5775e-01 (2.6623e-01)\tAcc@1  93.75 ( 92.08)\tAcc@5  97.66 ( 99.36)\n","Epoch: [77][120/391]\tTime  0.090 ( 0.095)\tLoss 2.5665e-01 (2.6419e-01)\tAcc@1  90.62 ( 92.06)\tAcc@5  99.22 ( 99.39)\n","Epoch: [77][150/391]\tTime  0.095 ( 0.094)\tLoss 2.8648e-01 (2.6509e-01)\tAcc@1  91.41 ( 92.03)\tAcc@5  97.66 ( 99.39)\n","Epoch: [77][180/391]\tTime  0.089 ( 0.094)\tLoss 2.9774e-01 (2.6794e-01)\tAcc@1  92.19 ( 91.92)\tAcc@5  99.22 ( 99.38)\n","Epoch: [77][210/391]\tTime  0.093 ( 0.094)\tLoss 2.5224e-01 (2.7347e-01)\tAcc@1  93.75 ( 91.80)\tAcc@5  99.22 ( 99.34)\n","Epoch: [77][240/391]\tTime  0.095 ( 0.094)\tLoss 2.7584e-01 (2.7940e-01)\tAcc@1  92.19 ( 91.61)\tAcc@5  98.44 ( 99.32)\n","Epoch: [77][270/391]\tTime  0.096 ( 0.094)\tLoss 2.7307e-01 (2.8059e-01)\tAcc@1  92.19 ( 91.57)\tAcc@5  99.22 ( 99.34)\n","Epoch: [77][300/391]\tTime  0.099 ( 0.094)\tLoss 2.8394e-01 (2.8536e-01)\tAcc@1  93.75 ( 91.47)\tAcc@5  99.22 ( 99.30)\n","Epoch: [77][330/391]\tTime  0.090 ( 0.093)\tLoss 3.2844e-01 (2.8939e-01)\tAcc@1  92.19 ( 91.35)\tAcc@5  98.44 ( 99.30)\n","Epoch: [77][360/391]\tTime  0.097 ( 0.093)\tLoss 2.8528e-01 (2.9224e-01)\tAcc@1  90.62 ( 91.22)\tAcc@5 100.00 ( 99.29)\n","Epoch: [77][390/391]\tTime  0.082 ( 0.093)\tLoss 2.6010e-01 (2.9782e-01)\tAcc@1  93.75 ( 91.07)\tAcc@5 100.00 ( 99.29)\n","==> Train Accuracy: Acc@1 91.066 || Acc@5 99.288\n","==> Test Accuracy:  Acc@1 70.380 || Acc@5 91.690\n","==> 39.57 seconds to train this epoch\n","\n","\n","----- epoch: 78, lr: 0.020000000000000004 -----\n","Epoch: [78][  0/391]\tTime  0.308 ( 0.308)\tLoss 2.5675e-01 (2.5675e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5  97.66 ( 97.66)\n","Epoch: [78][ 30/391]\tTime  0.095 ( 0.100)\tLoss 2.6235e-01 (2.5899e-01)\tAcc@1  92.19 ( 92.57)\tAcc@5 100.00 ( 99.50)\n","Epoch: [78][ 60/391]\tTime  0.102 ( 0.096)\tLoss 3.5421e-01 (2.5992e-01)\tAcc@1  86.72 ( 92.49)\tAcc@5  99.22 ( 99.45)\n","Epoch: [78][ 90/391]\tTime  0.089 ( 0.095)\tLoss 3.1656e-01 (2.6683e-01)\tAcc@1  89.06 ( 92.17)\tAcc@5 100.00 ( 99.50)\n","Epoch: [78][120/391]\tTime  0.088 ( 0.094)\tLoss 3.5353e-01 (2.7571e-01)\tAcc@1  86.72 ( 91.81)\tAcc@5  99.22 ( 99.40)\n","Epoch: [78][150/391]\tTime  0.095 ( 0.094)\tLoss 2.5809e-01 (2.8131e-01)\tAcc@1  92.97 ( 91.66)\tAcc@5  99.22 ( 99.33)\n","Epoch: [78][180/391]\tTime  0.092 ( 0.094)\tLoss 3.2398e-01 (2.8843e-01)\tAcc@1  92.19 ( 91.34)\tAcc@5  99.22 ( 99.32)\n","Epoch: [78][210/391]\tTime  0.091 ( 0.094)\tLoss 3.3052e-01 (2.9381e-01)\tAcc@1  90.62 ( 91.17)\tAcc@5  99.22 ( 99.31)\n","Epoch: [78][240/391]\tTime  0.097 ( 0.093)\tLoss 2.5210e-01 (2.9386e-01)\tAcc@1  91.41 ( 91.16)\tAcc@5 100.00 ( 99.29)\n","Epoch: [78][270/391]\tTime  0.092 ( 0.093)\tLoss 3.6239e-01 (2.9536e-01)\tAcc@1  91.41 ( 91.07)\tAcc@5  99.22 ( 99.29)\n","Epoch: [78][300/391]\tTime  0.092 ( 0.093)\tLoss 2.2544e-01 (2.9677e-01)\tAcc@1  92.97 ( 91.00)\tAcc@5 100.00 ( 99.31)\n","Epoch: [78][330/391]\tTime  0.088 ( 0.093)\tLoss 3.0001e-01 (2.9941e-01)\tAcc@1  91.41 ( 90.97)\tAcc@5 100.00 ( 99.29)\n","Epoch: [78][360/391]\tTime  0.093 ( 0.093)\tLoss 3.6208e-01 (3.0164e-01)\tAcc@1  86.72 ( 90.87)\tAcc@5 100.00 ( 99.27)\n","Epoch: [78][390/391]\tTime  0.082 ( 0.093)\tLoss 3.4824e-01 (3.0336e-01)\tAcc@1  87.50 ( 90.80)\tAcc@5  98.75 ( 99.27)\n","==> Train Accuracy: Acc@1 90.802 || Acc@5 99.274\n","==> Test Accuracy:  Acc@1 69.260 || Acc@5 91.080\n","==> 39.51 seconds to train this epoch\n","\n","\n","----- epoch: 79, lr: 0.020000000000000004 -----\n","Epoch: [79][  0/391]\tTime  0.324 ( 0.324)\tLoss 2.7455e-01 (2.7455e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5 100.00 (100.00)\n","Epoch: [79][ 30/391]\tTime  0.091 ( 0.100)\tLoss 2.0730e-01 (2.7475e-01)\tAcc@1  93.75 ( 92.11)\tAcc@5  99.22 ( 99.34)\n","Epoch: [79][ 60/391]\tTime  0.091 ( 0.097)\tLoss 2.9461e-01 (2.6906e-01)\tAcc@1  90.62 ( 92.34)\tAcc@5  99.22 ( 99.26)\n","Epoch: [79][ 90/391]\tTime  0.089 ( 0.095)\tLoss 2.5522e-01 (2.7283e-01)\tAcc@1  90.62 ( 92.07)\tAcc@5  99.22 ( 99.29)\n","Epoch: [79][120/391]\tTime  0.093 ( 0.095)\tLoss 2.2518e-01 (2.7758e-01)\tAcc@1  93.75 ( 91.90)\tAcc@5 100.00 ( 99.25)\n","Epoch: [79][150/391]\tTime  0.091 ( 0.094)\tLoss 4.0184e-01 (2.7961e-01)\tAcc@1  87.50 ( 91.69)\tAcc@5  98.44 ( 99.30)\n","Epoch: [79][180/391]\tTime  0.090 ( 0.094)\tLoss 2.9244e-01 (2.8170e-01)\tAcc@1  90.62 ( 91.62)\tAcc@5  99.22 ( 99.29)\n","Epoch: [79][210/391]\tTime  0.091 ( 0.094)\tLoss 2.7532e-01 (2.8170e-01)\tAcc@1  92.19 ( 91.65)\tAcc@5  99.22 ( 99.32)\n","Epoch: [79][240/391]\tTime  0.094 ( 0.094)\tLoss 3.4803e-01 (2.8339e-01)\tAcc@1  86.72 ( 91.57)\tAcc@5  99.22 ( 99.31)\n","Epoch: [79][270/391]\tTime  0.089 ( 0.094)\tLoss 2.4749e-01 (2.8454e-01)\tAcc@1  91.41 ( 91.54)\tAcc@5  99.22 ( 99.30)\n","Epoch: [79][300/391]\tTime  0.092 ( 0.093)\tLoss 3.8910e-01 (2.8838e-01)\tAcc@1  88.28 ( 91.42)\tAcc@5  97.66 ( 99.27)\n","Epoch: [79][330/391]\tTime  0.092 ( 0.093)\tLoss 3.2278e-01 (2.9020e-01)\tAcc@1  88.28 ( 91.33)\tAcc@5  99.22 ( 99.27)\n","Epoch: [79][360/391]\tTime  0.092 ( 0.093)\tLoss 2.3331e-01 (2.9328e-01)\tAcc@1  94.53 ( 91.26)\tAcc@5 100.00 ( 99.26)\n","Epoch: [79][390/391]\tTime  0.081 ( 0.093)\tLoss 4.6221e-01 (2.9584e-01)\tAcc@1  80.00 ( 91.19)\tAcc@5 100.00 ( 99.23)\n","==> Train Accuracy: Acc@1 91.188 || Acc@5 99.228\n","==> Test Accuracy:  Acc@1 71.220 || Acc@5 91.560\n","==> 39.50 seconds to train this epoch\n","\n","\n","----- epoch: 80, lr: 0.020000000000000004 -----\n","Epoch: [80][  0/391]\tTime  0.273 ( 0.273)\tLoss 2.5561e-01 (2.5561e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5  99.22 ( 99.22)\n","Epoch: [80][ 30/391]\tTime  0.093 ( 0.099)\tLoss 3.8659e-01 (2.8572e-01)\tAcc@1  89.84 ( 91.26)\tAcc@5  99.22 ( 99.45)\n","Epoch: [80][ 60/391]\tTime  0.103 ( 0.096)\tLoss 2.6743e-01 (2.8196e-01)\tAcc@1  94.53 ( 91.59)\tAcc@5  97.66 ( 99.42)\n","Epoch: [80][ 90/391]\tTime  0.096 ( 0.095)\tLoss 3.3202e-01 (2.8646e-01)\tAcc@1  86.72 ( 91.56)\tAcc@5 100.00 ( 99.36)\n","Epoch: [80][120/391]\tTime  0.091 ( 0.095)\tLoss 2.7025e-01 (2.8136e-01)\tAcc@1  92.19 ( 91.65)\tAcc@5  98.44 ( 99.39)\n","Epoch: [80][150/391]\tTime  0.091 ( 0.094)\tLoss 2.2679e-01 (2.8179e-01)\tAcc@1  93.75 ( 91.59)\tAcc@5 100.00 ( 99.39)\n","Epoch: [80][180/391]\tTime  0.092 ( 0.094)\tLoss 3.8660e-01 (2.8461e-01)\tAcc@1  86.72 ( 91.47)\tAcc@5 100.00 ( 99.37)\n","Epoch: [80][210/391]\tTime  0.091 ( 0.094)\tLoss 2.1478e-01 (2.8621e-01)\tAcc@1  92.19 ( 91.46)\tAcc@5 100.00 ( 99.37)\n","Epoch: [80][240/391]\tTime  0.095 ( 0.094)\tLoss 2.3615e-01 (2.8879e-01)\tAcc@1  91.41 ( 91.31)\tAcc@5  99.22 ( 99.36)\n","Epoch: [80][270/391]\tTime  0.089 ( 0.093)\tLoss 2.8527e-01 (2.9124e-01)\tAcc@1  92.19 ( 91.23)\tAcc@5  99.22 ( 99.34)\n","Epoch: [80][300/391]\tTime  0.089 ( 0.093)\tLoss 3.5740e-01 (2.9470e-01)\tAcc@1  89.06 ( 91.14)\tAcc@5  98.44 ( 99.32)\n","Epoch: [80][330/391]\tTime  0.095 ( 0.093)\tLoss 2.4856e-01 (2.9820e-01)\tAcc@1  93.75 ( 90.98)\tAcc@5 100.00 ( 99.33)\n","Epoch: [80][360/391]\tTime  0.091 ( 0.093)\tLoss 3.2149e-01 (3.0389e-01)\tAcc@1  88.28 ( 90.76)\tAcc@5 100.00 ( 99.30)\n","Epoch: [80][390/391]\tTime  0.082 ( 0.093)\tLoss 2.1051e-01 (3.0633e-01)\tAcc@1  95.00 ( 90.70)\tAcc@5 100.00 ( 99.29)\n","==> Train Accuracy: Acc@1 90.702 || Acc@5 99.292\n","==> Test Accuracy:  Acc@1 70.260 || Acc@5 91.140\n","==> 39.49 seconds to train this epoch\n","\n","\n","----- epoch: 81, lr: 0.020000000000000004 -----\n","Epoch: [81][  0/391]\tTime  0.310 ( 0.310)\tLoss 2.3267e-01 (2.3267e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  99.22 ( 99.22)\n","Epoch: [81][ 30/391]\tTime  0.091 ( 0.099)\tLoss 2.5745e-01 (2.8673e-01)\tAcc@1  91.41 ( 90.85)\tAcc@5 100.00 ( 99.42)\n","Epoch: [81][ 60/391]\tTime  0.093 ( 0.096)\tLoss 2.2043e-01 (2.7817e-01)\tAcc@1  96.88 ( 91.51)\tAcc@5 100.00 ( 99.44)\n","Epoch: [81][ 90/391]\tTime  0.085 ( 0.095)\tLoss 1.9192e-01 (2.6960e-01)\tAcc@1  94.53 ( 91.90)\tAcc@5  99.22 ( 99.41)\n","Epoch: [81][120/391]\tTime  0.095 ( 0.094)\tLoss 1.6728e-01 (2.6782e-01)\tAcc@1  91.41 ( 92.00)\tAcc@5 100.00 ( 99.43)\n","Epoch: [81][150/391]\tTime  0.089 ( 0.094)\tLoss 2.5193e-01 (2.6883e-01)\tAcc@1  90.62 ( 91.91)\tAcc@5 100.00 ( 99.45)\n","Epoch: [81][180/391]\tTime  0.089 ( 0.094)\tLoss 2.5679e-01 (2.7034e-01)\tAcc@1  90.62 ( 91.88)\tAcc@5 100.00 ( 99.43)\n","Epoch: [81][210/391]\tTime  0.093 ( 0.093)\tLoss 2.5343e-01 (2.7373e-01)\tAcc@1  92.19 ( 91.81)\tAcc@5  99.22 ( 99.43)\n","Epoch: [81][240/391]\tTime  0.098 ( 0.093)\tLoss 3.8625e-01 (2.8041e-01)\tAcc@1  85.16 ( 91.56)\tAcc@5 100.00 ( 99.39)\n","Epoch: [81][270/391]\tTime  0.104 ( 0.093)\tLoss 2.5996e-01 (2.8704e-01)\tAcc@1  89.84 ( 91.36)\tAcc@5  99.22 ( 99.34)\n","Epoch: [81][300/391]\tTime  0.089 ( 0.093)\tLoss 4.2855e-01 (2.8986e-01)\tAcc@1  88.28 ( 91.29)\tAcc@5  96.88 ( 99.34)\n","Epoch: [81][330/391]\tTime  0.091 ( 0.093)\tLoss 2.6580e-01 (2.9314e-01)\tAcc@1  91.41 ( 91.16)\tAcc@5  99.22 ( 99.34)\n","Epoch: [81][360/391]\tTime  0.092 ( 0.093)\tLoss 3.3538e-01 (2.9552e-01)\tAcc@1  89.84 ( 91.07)\tAcc@5 100.00 ( 99.34)\n","Epoch: [81][390/391]\tTime  0.079 ( 0.093)\tLoss 6.7085e-01 (3.0187e-01)\tAcc@1  81.25 ( 90.86)\tAcc@5  97.50 ( 99.32)\n","==> Train Accuracy: Acc@1 90.864 || Acc@5 99.316\n","==> Test Accuracy:  Acc@1 69.900 || Acc@5 90.840\n","==> 39.45 seconds to train this epoch\n","\n","\n","----- epoch: 82, lr: 0.020000000000000004 -----\n","Epoch: [82][  0/391]\tTime  0.290 ( 0.290)\tLoss 2.4338e-01 (2.4338e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5  98.44 ( 98.44)\n","Epoch: [82][ 30/391]\tTime  0.090 ( 0.100)\tLoss 1.9044e-01 (2.6645e-01)\tAcc@1  94.53 ( 92.49)\tAcc@5 100.00 ( 99.40)\n","Epoch: [82][ 60/391]\tTime  0.090 ( 0.096)\tLoss 4.1915e-01 (2.7426e-01)\tAcc@1  88.28 ( 92.10)\tAcc@5 100.00 ( 99.41)\n","Epoch: [82][ 90/391]\tTime  0.090 ( 0.095)\tLoss 3.1425e-01 (2.8128e-01)\tAcc@1  90.62 ( 91.90)\tAcc@5  98.44 ( 99.33)\n","Epoch: [82][120/391]\tTime  0.087 ( 0.094)\tLoss 2.0216e-01 (2.8041e-01)\tAcc@1  92.97 ( 91.97)\tAcc@5 100.00 ( 99.35)\n","Epoch: [82][150/391]\tTime  0.091 ( 0.094)\tLoss 2.5764e-01 (2.7950e-01)\tAcc@1  92.19 ( 91.97)\tAcc@5 100.00 ( 99.33)\n","Epoch: [82][180/391]\tTime  0.092 ( 0.094)\tLoss 3.3337e-01 (2.7970e-01)\tAcc@1  89.84 ( 91.92)\tAcc@5 100.00 ( 99.37)\n","Epoch: [82][210/391]\tTime  0.087 ( 0.094)\tLoss 4.2441e-01 (2.8605e-01)\tAcc@1  89.06 ( 91.69)\tAcc@5  97.66 ( 99.35)\n","Epoch: [82][240/391]\tTime  0.097 ( 0.094)\tLoss 4.0401e-01 (2.8954e-01)\tAcc@1  89.84 ( 91.54)\tAcc@5  99.22 ( 99.35)\n","Epoch: [82][270/391]\tTime  0.091 ( 0.094)\tLoss 4.3279e-01 (2.9607e-01)\tAcc@1  89.06 ( 91.31)\tAcc@5  97.66 ( 99.30)\n","Epoch: [82][300/391]\tTime  0.104 ( 0.093)\tLoss 2.9198e-01 (2.9906e-01)\tAcc@1  90.62 ( 91.21)\tAcc@5  99.22 ( 99.29)\n","Epoch: [82][330/391]\tTime  0.090 ( 0.093)\tLoss 2.7207e-01 (3.0101e-01)\tAcc@1  92.97 ( 91.16)\tAcc@5 100.00 ( 99.28)\n","Epoch: [82][360/391]\tTime  0.087 ( 0.093)\tLoss 2.6117e-01 (3.0248e-01)\tAcc@1  94.53 ( 91.12)\tAcc@5  99.22 ( 99.27)\n","Epoch: [82][390/391]\tTime  0.081 ( 0.093)\tLoss 3.8169e-01 (3.0543e-01)\tAcc@1  85.00 ( 90.99)\tAcc@5  97.50 ( 99.26)\n","==> Train Accuracy: Acc@1 90.992 || Acc@5 99.256\n","==> Test Accuracy:  Acc@1 69.540 || Acc@5 91.340\n","==> 39.52 seconds to train this epoch\n","\n","\n","----- epoch: 83, lr: 0.020000000000000004 -----\n","Epoch: [83][  0/391]\tTime  0.291 ( 0.291)\tLoss 2.4088e-01 (2.4088e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5  98.44 ( 98.44)\n","Epoch: [83][ 30/391]\tTime  0.091 ( 0.099)\tLoss 2.1354e-01 (2.7434e-01)\tAcc@1  94.53 ( 91.68)\tAcc@5 100.00 ( 99.45)\n","Epoch: [83][ 60/391]\tTime  0.089 ( 0.096)\tLoss 2.1064e-01 (2.7573e-01)\tAcc@1  92.19 ( 91.55)\tAcc@5 100.00 ( 99.45)\n","Epoch: [83][ 90/391]\tTime  0.089 ( 0.095)\tLoss 3.4564e-01 (2.7783e-01)\tAcc@1  88.28 ( 91.41)\tAcc@5  98.44 ( 99.42)\n","Epoch: [83][120/391]\tTime  0.092 ( 0.094)\tLoss 2.5832e-01 (2.7416e-01)\tAcc@1  92.19 ( 91.62)\tAcc@5 100.00 ( 99.39)\n","Epoch: [83][150/391]\tTime  0.092 ( 0.094)\tLoss 2.4836e-01 (2.7221e-01)\tAcc@1  94.53 ( 91.66)\tAcc@5  99.22 ( 99.39)\n","Epoch: [83][180/391]\tTime  0.094 ( 0.094)\tLoss 2.4672e-01 (2.6882e-01)\tAcc@1  91.41 ( 91.79)\tAcc@5 100.00 ( 99.43)\n","Epoch: [83][210/391]\tTime  0.094 ( 0.093)\tLoss 2.4901e-01 (2.7204e-01)\tAcc@1  92.97 ( 91.73)\tAcc@5  99.22 ( 99.42)\n","Epoch: [83][240/391]\tTime  0.092 ( 0.093)\tLoss 2.4495e-01 (2.7290e-01)\tAcc@1  92.97 ( 91.70)\tAcc@5 100.00 ( 99.43)\n","Epoch: [83][270/391]\tTime  0.092 ( 0.093)\tLoss 3.0819e-01 (2.7547e-01)\tAcc@1  92.19 ( 91.60)\tAcc@5 100.00 ( 99.41)\n","Epoch: [83][300/391]\tTime  0.088 ( 0.093)\tLoss 3.9067e-01 (2.7827e-01)\tAcc@1  87.50 ( 91.52)\tAcc@5  98.44 ( 99.40)\n","Epoch: [83][330/391]\tTime  0.089 ( 0.093)\tLoss 3.4012e-01 (2.8257e-01)\tAcc@1  91.41 ( 91.36)\tAcc@5  98.44 ( 99.37)\n","Epoch: [83][360/391]\tTime  0.093 ( 0.093)\tLoss 2.9938e-01 (2.8867e-01)\tAcc@1  92.97 ( 91.16)\tAcc@5  99.22 ( 99.36)\n","Epoch: [83][390/391]\tTime  0.083 ( 0.093)\tLoss 3.4931e-01 (2.9230e-01)\tAcc@1  87.50 ( 91.07)\tAcc@5  97.50 ( 99.33)\n","==> Train Accuracy: Acc@1 91.072 || Acc@5 99.334\n","==> Test Accuracy:  Acc@1 70.390 || Acc@5 91.510\n","==> 39.44 seconds to train this epoch\n","\n","\n","----- epoch: 84, lr: 0.020000000000000004 -----\n","Epoch: [84][  0/391]\tTime  0.305 ( 0.305)\tLoss 1.9625e-01 (1.9625e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5 100.00 (100.00)\n","Epoch: [84][ 30/391]\tTime  0.091 ( 0.100)\tLoss 2.7107e-01 (2.7173e-01)\tAcc@1  93.75 ( 92.11)\tAcc@5  99.22 ( 99.27)\n","Epoch: [84][ 60/391]\tTime  0.096 ( 0.096)\tLoss 3.0766e-01 (2.7438e-01)\tAcc@1  89.06 ( 91.79)\tAcc@5  99.22 ( 99.32)\n","Epoch: [84][ 90/391]\tTime  0.091 ( 0.095)\tLoss 2.6989e-01 (2.6957e-01)\tAcc@1  92.19 ( 91.93)\tAcc@5  99.22 ( 99.31)\n","Epoch: [84][120/391]\tTime  0.092 ( 0.095)\tLoss 3.3980e-01 (2.7081e-01)\tAcc@1  88.28 ( 91.90)\tAcc@5  99.22 ( 99.32)\n","Epoch: [84][150/391]\tTime  0.092 ( 0.094)\tLoss 3.4920e-01 (2.7817e-01)\tAcc@1  91.41 ( 91.71)\tAcc@5  99.22 ( 99.24)\n","Epoch: [84][180/391]\tTime  0.097 ( 0.094)\tLoss 3.1209e-01 (2.8014e-01)\tAcc@1  90.62 ( 91.66)\tAcc@5  98.44 ( 99.24)\n","Epoch: [84][210/391]\tTime  0.087 ( 0.094)\tLoss 2.8773e-01 (2.8144e-01)\tAcc@1  89.84 ( 91.58)\tAcc@5  99.22 ( 99.26)\n","Epoch: [84][240/391]\tTime  0.092 ( 0.094)\tLoss 2.3857e-01 (2.8215e-01)\tAcc@1  92.19 ( 91.51)\tAcc@5  98.44 ( 99.28)\n","Epoch: [84][270/391]\tTime  0.088 ( 0.094)\tLoss 1.8444e-01 (2.8367e-01)\tAcc@1  93.75 ( 91.46)\tAcc@5  99.22 ( 99.28)\n","Epoch: [84][300/391]\tTime  0.099 ( 0.094)\tLoss 5.4135e-01 (2.8519e-01)\tAcc@1  81.25 ( 91.39)\tAcc@5  97.66 ( 99.28)\n","Epoch: [84][330/391]\tTime  0.092 ( 0.093)\tLoss 3.8669e-01 (2.8998e-01)\tAcc@1  87.50 ( 91.24)\tAcc@5  99.22 ( 99.27)\n","Epoch: [84][360/391]\tTime  0.090 ( 0.093)\tLoss 3.0660e-01 (2.9286e-01)\tAcc@1  87.50 ( 91.18)\tAcc@5  98.44 ( 99.24)\n","Epoch: [84][390/391]\tTime  0.083 ( 0.093)\tLoss 4.0874e-01 (2.9714e-01)\tAcc@1  85.00 ( 91.05)\tAcc@5 100.00 ( 99.23)\n","==> Train Accuracy: Acc@1 91.054 || Acc@5 99.230\n","==> Test Accuracy:  Acc@1 70.790 || Acc@5 91.440\n","==> 39.63 seconds to train this epoch\n","\n","\n","----- epoch: 85, lr: 0.020000000000000004 -----\n","Epoch: [85][  0/391]\tTime  0.274 ( 0.274)\tLoss 3.1507e-01 (3.1507e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  97.66 ( 97.66)\n","Epoch: [85][ 30/391]\tTime  0.097 ( 0.099)\tLoss 2.0748e-01 (2.4758e-01)\tAcc@1  91.41 ( 92.64)\tAcc@5 100.00 ( 99.22)\n","Epoch: [85][ 60/391]\tTime  0.094 ( 0.096)\tLoss 2.6549e-01 (2.4521e-01)\tAcc@1  90.62 ( 92.85)\tAcc@5 100.00 ( 99.35)\n","Epoch: [85][ 90/391]\tTime  0.091 ( 0.095)\tLoss 3.0552e-01 (2.5253e-01)\tAcc@1  91.41 ( 92.65)\tAcc@5  97.66 ( 99.32)\n","Epoch: [85][120/391]\tTime  0.099 ( 0.095)\tLoss 2.9156e-01 (2.5655e-01)\tAcc@1  92.97 ( 92.50)\tAcc@5  99.22 ( 99.38)\n","Epoch: [85][150/391]\tTime  0.092 ( 0.094)\tLoss 2.0253e-01 (2.5895e-01)\tAcc@1  93.75 ( 92.39)\tAcc@5 100.00 ( 99.38)\n","Epoch: [85][180/391]\tTime  0.091 ( 0.094)\tLoss 1.4588e-01 (2.6323e-01)\tAcc@1  95.31 ( 92.17)\tAcc@5 100.00 ( 99.35)\n","Epoch: [85][210/391]\tTime  0.090 ( 0.094)\tLoss 4.4185e-01 (2.6762e-01)\tAcc@1  85.16 ( 92.00)\tAcc@5  98.44 ( 99.35)\n","Epoch: [85][240/391]\tTime  0.098 ( 0.093)\tLoss 2.8398e-01 (2.7002e-01)\tAcc@1  91.41 ( 91.98)\tAcc@5 100.00 ( 99.35)\n","Epoch: [85][270/391]\tTime  0.091 ( 0.093)\tLoss 3.4919e-01 (2.7144e-01)\tAcc@1  89.06 ( 91.90)\tAcc@5  97.66 ( 99.35)\n","Epoch: [85][300/391]\tTime  0.088 ( 0.093)\tLoss 2.5785e-01 (2.7566e-01)\tAcc@1  92.97 ( 91.76)\tAcc@5  99.22 ( 99.34)\n","Epoch: [85][330/391]\tTime  0.094 ( 0.093)\tLoss 3.4061e-01 (2.7966e-01)\tAcc@1  89.84 ( 91.60)\tAcc@5  98.44 ( 99.34)\n","Epoch: [85][360/391]\tTime  0.093 ( 0.093)\tLoss 3.3789e-01 (2.8213e-01)\tAcc@1  90.62 ( 91.50)\tAcc@5 100.00 ( 99.34)\n","Epoch: [85][390/391]\tTime  0.091 ( 0.093)\tLoss 4.3319e-01 (2.8570e-01)\tAcc@1  86.25 ( 91.41)\tAcc@5  98.75 ( 99.32)\n","==> Train Accuracy: Acc@1 91.412 || Acc@5 99.322\n","==> Test Accuracy:  Acc@1 69.890 || Acc@5 90.890\n","==> 39.52 seconds to train this epoch\n","\n","\n","----- epoch: 86, lr: 0.020000000000000004 -----\n","Epoch: [86][  0/391]\tTime  0.281 ( 0.281)\tLoss 2.5715e-01 (2.5715e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.22 ( 99.22)\n","Epoch: [86][ 30/391]\tTime  0.091 ( 0.099)\tLoss 1.7547e-01 (2.8163e-01)\tAcc@1  95.31 ( 91.51)\tAcc@5 100.00 ( 99.37)\n","Epoch: [86][ 60/391]\tTime  0.091 ( 0.096)\tLoss 2.3224e-01 (2.6597e-01)\tAcc@1  92.97 ( 92.12)\tAcc@5 100.00 ( 99.42)\n","Epoch: [86][ 90/391]\tTime  0.091 ( 0.095)\tLoss 1.9501e-01 (2.6319e-01)\tAcc@1  95.31 ( 92.22)\tAcc@5 100.00 ( 99.38)\n","Epoch: [86][120/391]\tTime  0.091 ( 0.094)\tLoss 1.8319e-01 (2.6248e-01)\tAcc@1  92.19 ( 92.26)\tAcc@5 100.00 ( 99.37)\n","Epoch: [86][150/391]\tTime  0.090 ( 0.094)\tLoss 3.3529e-01 (2.6640e-01)\tAcc@1  90.62 ( 92.15)\tAcc@5  99.22 ( 99.32)\n","Epoch: [86][180/391]\tTime  0.089 ( 0.094)\tLoss 2.3402e-01 (2.6888e-01)\tAcc@1  92.19 ( 92.01)\tAcc@5  99.22 ( 99.35)\n","Epoch: [86][210/391]\tTime  0.100 ( 0.093)\tLoss 1.4156e-01 (2.7154e-01)\tAcc@1  96.09 ( 91.92)\tAcc@5 100.00 ( 99.34)\n","Epoch: [86][240/391]\tTime  0.088 ( 0.093)\tLoss 2.2205e-01 (2.7486e-01)\tAcc@1  93.75 ( 91.79)\tAcc@5 100.00 ( 99.37)\n","Epoch: [86][270/391]\tTime  0.092 ( 0.093)\tLoss 4.2805e-01 (2.8055e-01)\tAcc@1  86.72 ( 91.63)\tAcc@5  99.22 ( 99.32)\n","Epoch: [86][300/391]\tTime  0.093 ( 0.093)\tLoss 2.5499e-01 (2.8427e-01)\tAcc@1  91.41 ( 91.47)\tAcc@5 100.00 ( 99.31)\n","Epoch: [86][330/391]\tTime  0.090 ( 0.093)\tLoss 3.0815e-01 (2.8941e-01)\tAcc@1  87.50 ( 91.30)\tAcc@5 100.00 ( 99.28)\n","Epoch: [86][360/391]\tTime  0.092 ( 0.093)\tLoss 4.2365e-01 (2.9282e-01)\tAcc@1  89.06 ( 91.21)\tAcc@5  96.88 ( 99.26)\n","Epoch: [86][390/391]\tTime  0.082 ( 0.093)\tLoss 4.2807e-01 (2.9644e-01)\tAcc@1  90.00 ( 91.11)\tAcc@5  97.50 ( 99.24)\n","==> Train Accuracy: Acc@1 91.108 || Acc@5 99.244\n","==> Test Accuracy:  Acc@1 70.870 || Acc@5 91.240\n","==> 39.50 seconds to train this epoch\n","\n","\n","----- epoch: 87, lr: 0.020000000000000004 -----\n","Epoch: [87][  0/391]\tTime  0.282 ( 0.282)\tLoss 2.4572e-01 (2.4572e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n","Epoch: [87][ 30/391]\tTime  0.090 ( 0.099)\tLoss 2.6850e-01 (2.7138e-01)\tAcc@1  92.19 ( 92.24)\tAcc@5 100.00 ( 99.22)\n","Epoch: [87][ 60/391]\tTime  0.096 ( 0.096)\tLoss 2.2718e-01 (2.6619e-01)\tAcc@1  93.75 ( 92.32)\tAcc@5 100.00 ( 99.36)\n","Epoch: [87][ 90/391]\tTime  0.091 ( 0.095)\tLoss 2.8576e-01 (2.6642e-01)\tAcc@1  92.19 ( 92.28)\tAcc@5  99.22 ( 99.41)\n","Epoch: [87][120/391]\tTime  0.093 ( 0.094)\tLoss 2.2492e-01 (2.6788e-01)\tAcc@1  93.75 ( 92.26)\tAcc@5 100.00 ( 99.36)\n","Epoch: [87][150/391]\tTime  0.089 ( 0.094)\tLoss 2.7425e-01 (2.7508e-01)\tAcc@1  92.97 ( 91.94)\tAcc@5 100.00 ( 99.33)\n","Epoch: [87][180/391]\tTime  0.107 ( 0.094)\tLoss 2.8923e-01 (2.7763e-01)\tAcc@1  92.19 ( 91.85)\tAcc@5  99.22 ( 99.34)\n","Epoch: [87][210/391]\tTime  0.090 ( 0.093)\tLoss 1.9038e-01 (2.7592e-01)\tAcc@1  95.31 ( 91.92)\tAcc@5 100.00 ( 99.37)\n","Epoch: [87][240/391]\tTime  0.086 ( 0.093)\tLoss 3.6477e-01 (2.8022e-01)\tAcc@1  88.28 ( 91.79)\tAcc@5 100.00 ( 99.33)\n","Epoch: [87][270/391]\tTime  0.090 ( 0.093)\tLoss 4.9321e-01 (2.8629e-01)\tAcc@1  83.59 ( 91.58)\tAcc@5  96.88 ( 99.30)\n","Epoch: [87][300/391]\tTime  0.092 ( 0.093)\tLoss 4.2454e-01 (2.9093e-01)\tAcc@1  85.94 ( 91.46)\tAcc@5 100.00 ( 99.30)\n","Epoch: [87][330/391]\tTime  0.087 ( 0.093)\tLoss 3.1761e-01 (2.9362e-01)\tAcc@1  89.84 ( 91.37)\tAcc@5  99.22 ( 99.30)\n","Epoch: [87][360/391]\tTime  0.091 ( 0.093)\tLoss 3.4087e-01 (2.9478e-01)\tAcc@1  91.41 ( 91.35)\tAcc@5  98.44 ( 99.29)\n","Epoch: [87][390/391]\tTime  0.085 ( 0.093)\tLoss 3.6381e-01 (2.9617e-01)\tAcc@1  90.00 ( 91.29)\tAcc@5 100.00 ( 99.28)\n","==> Train Accuracy: Acc@1 91.290 || Acc@5 99.276\n","==> Test Accuracy:  Acc@1 69.640 || Acc@5 91.290\n","==> 39.49 seconds to train this epoch\n","\n","\n","----- epoch: 88, lr: 0.020000000000000004 -----\n","Epoch: [88][  0/391]\tTime  0.301 ( 0.301)\tLoss 2.6294e-01 (2.6294e-01)\tAcc@1  89.84 ( 89.84)\tAcc@5 100.00 (100.00)\n","Epoch: [88][ 30/391]\tTime  0.094 ( 0.100)\tLoss 2.4198e-01 (2.5700e-01)\tAcc@1  91.41 ( 92.36)\tAcc@5  99.22 ( 99.50)\n","Epoch: [88][ 60/391]\tTime  0.092 ( 0.096)\tLoss 1.9911e-01 (2.6188e-01)\tAcc@1  93.75 ( 92.24)\tAcc@5 100.00 ( 99.41)\n","Epoch: [88][ 90/391]\tTime  0.091 ( 0.095)\tLoss 1.8604e-01 (2.6619e-01)\tAcc@1  95.31 ( 92.24)\tAcc@5 100.00 ( 99.36)\n","Epoch: [88][120/391]\tTime  0.098 ( 0.095)\tLoss 3.5418e-01 (2.7055e-01)\tAcc@1  89.06 ( 92.04)\tAcc@5  98.44 ( 99.33)\n","Epoch: [88][150/391]\tTime  0.100 ( 0.094)\tLoss 2.6989e-01 (2.6641e-01)\tAcc@1  91.41 ( 92.08)\tAcc@5  98.44 ( 99.39)\n","Epoch: [88][180/391]\tTime  0.095 ( 0.094)\tLoss 3.1083e-01 (2.6224e-01)\tAcc@1  89.84 ( 92.20)\tAcc@5 100.00 ( 99.39)\n","Epoch: [88][210/391]\tTime  0.092 ( 0.094)\tLoss 1.6186e-01 (2.6065e-01)\tAcc@1  96.09 ( 92.22)\tAcc@5  99.22 ( 99.41)\n","Epoch: [88][240/391]\tTime  0.094 ( 0.094)\tLoss 2.5565e-01 (2.6373e-01)\tAcc@1  90.62 ( 92.21)\tAcc@5 100.00 ( 99.41)\n","Epoch: [88][270/391]\tTime  0.090 ( 0.093)\tLoss 2.6888e-01 (2.6650e-01)\tAcc@1  91.41 ( 92.13)\tAcc@5  99.22 ( 99.40)\n","Epoch: [88][300/391]\tTime  0.090 ( 0.093)\tLoss 3.1522e-01 (2.7166e-01)\tAcc@1  92.19 ( 91.97)\tAcc@5  99.22 ( 99.40)\n","Epoch: [88][330/391]\tTime  0.094 ( 0.093)\tLoss 4.1865e-01 (2.7521e-01)\tAcc@1  84.38 ( 91.88)\tAcc@5  98.44 ( 99.38)\n","Epoch: [88][360/391]\tTime  0.091 ( 0.093)\tLoss 2.9025e-01 (2.7965e-01)\tAcc@1  92.19 ( 91.71)\tAcc@5 100.00 ( 99.37)\n","Epoch: [88][390/391]\tTime  0.082 ( 0.093)\tLoss 3.2124e-01 (2.8334e-01)\tAcc@1  90.00 ( 91.59)\tAcc@5  98.75 ( 99.35)\n","==> Train Accuracy: Acc@1 91.594 || Acc@5 99.348\n","==> Test Accuracy:  Acc@1 70.240 || Acc@5 91.430\n","==> 39.60 seconds to train this epoch\n","\n","\n","----- epoch: 89, lr: 0.020000000000000004 -----\n","Epoch: [89][  0/391]\tTime  0.287 ( 0.287)\tLoss 1.4986e-01 (1.4986e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n","Epoch: [89][ 30/391]\tTime  0.091 ( 0.099)\tLoss 2.7680e-01 (2.6071e-01)\tAcc@1  90.62 ( 92.52)\tAcc@5  99.22 ( 99.45)\n","Epoch: [89][ 60/391]\tTime  0.096 ( 0.096)\tLoss 3.9182e-01 (2.5885e-01)\tAcc@1  89.84 ( 92.25)\tAcc@5  98.44 ( 99.44)\n","Epoch: [89][ 90/391]\tTime  0.093 ( 0.095)\tLoss 2.3504e-01 (2.6655e-01)\tAcc@1  92.97 ( 91.98)\tAcc@5  98.44 ( 99.36)\n","Epoch: [89][120/391]\tTime  0.110 ( 0.094)\tLoss 2.1086e-01 (2.6888e-01)\tAcc@1  93.75 ( 92.00)\tAcc@5  99.22 ( 99.33)\n","Epoch: [89][150/391]\tTime  0.092 ( 0.094)\tLoss 3.6431e-01 (2.7360e-01)\tAcc@1  89.84 ( 91.85)\tAcc@5 100.00 ( 99.31)\n","Epoch: [89][180/391]\tTime  0.092 ( 0.094)\tLoss 1.8940e-01 (2.7637e-01)\tAcc@1  93.75 ( 91.85)\tAcc@5 100.00 ( 99.30)\n","Epoch: [89][210/391]\tTime  0.094 ( 0.094)\tLoss 2.2398e-01 (2.7510e-01)\tAcc@1  92.19 ( 91.83)\tAcc@5 100.00 ( 99.30)\n","Epoch: [89][240/391]\tTime  0.090 ( 0.093)\tLoss 1.8061e-01 (2.7402e-01)\tAcc@1  96.09 ( 91.86)\tAcc@5  99.22 ( 99.31)\n","Epoch: [89][270/391]\tTime  0.095 ( 0.093)\tLoss 2.9524e-01 (2.7604e-01)\tAcc@1  89.84 ( 91.72)\tAcc@5 100.00 ( 99.33)\n","Epoch: [89][300/391]\tTime  0.092 ( 0.093)\tLoss 2.6460e-01 (2.7746e-01)\tAcc@1  93.75 ( 91.64)\tAcc@5  99.22 ( 99.33)\n","Epoch: [89][330/391]\tTime  0.096 ( 0.093)\tLoss 2.8354e-01 (2.7851e-01)\tAcc@1  92.19 ( 91.60)\tAcc@5  98.44 ( 99.32)\n","Epoch: [89][360/391]\tTime  0.092 ( 0.093)\tLoss 3.0619e-01 (2.8174e-01)\tAcc@1  89.84 ( 91.48)\tAcc@5 100.00 ( 99.32)\n","Epoch: [89][390/391]\tTime  0.082 ( 0.093)\tLoss 3.5283e-01 (2.8582e-01)\tAcc@1  88.75 ( 91.35)\tAcc@5  98.75 ( 99.30)\n","==> Train Accuracy: Acc@1 91.348 || Acc@5 99.304\n","==> Test Accuracy:  Acc@1 70.200 || Acc@5 91.180\n","==> 39.50 seconds to train this epoch\n","\n","\n","----- epoch: 90, lr: 0.004000000000000001 -----\n","Epoch: [90][  0/391]\tTime  0.294 ( 0.294)\tLoss 1.9723e-01 (1.9723e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5  99.22 ( 99.22)\n","Epoch: [90][ 30/391]\tTime  0.092 ( 0.100)\tLoss 1.9377e-01 (2.1721e-01)\tAcc@1  96.09 ( 94.18)\tAcc@5  99.22 ( 99.40)\n","Epoch: [90][ 60/391]\tTime  0.096 ( 0.096)\tLoss 1.3170e-01 (1.9478e-01)\tAcc@1  96.09 ( 94.89)\tAcc@5 100.00 ( 99.49)\n","Epoch: [90][ 90/391]\tTime  0.102 ( 0.095)\tLoss 1.5423e-01 (1.8707e-01)\tAcc@1  95.31 ( 95.04)\tAcc@5 100.00 ( 99.52)\n","Epoch: [90][120/391]\tTime  0.089 ( 0.094)\tLoss 1.3106e-01 (1.7920e-01)\tAcc@1  96.09 ( 95.16)\tAcc@5 100.00 ( 99.58)\n","Epoch: [90][150/391]\tTime  0.091 ( 0.094)\tLoss 1.1935e-01 (1.7290e-01)\tAcc@1  96.09 ( 95.38)\tAcc@5 100.00 ( 99.60)\n","Epoch: [90][180/391]\tTime  0.092 ( 0.094)\tLoss 9.6210e-02 (1.6630e-01)\tAcc@1  98.44 ( 95.55)\tAcc@5 100.00 ( 99.63)\n","Epoch: [90][210/391]\tTime  0.105 ( 0.094)\tLoss 2.0064e-01 (1.6004e-01)\tAcc@1  94.53 ( 95.77)\tAcc@5 100.00 ( 99.64)\n","Epoch: [90][240/391]\tTime  0.093 ( 0.093)\tLoss 1.3185e-01 (1.5493e-01)\tAcc@1  96.09 ( 95.92)\tAcc@5  99.22 ( 99.67)\n","Epoch: [90][270/391]\tTime  0.091 ( 0.093)\tLoss 1.6229e-01 (1.5114e-01)\tAcc@1  93.75 ( 96.01)\tAcc@5  99.22 ( 99.68)\n","Epoch: [90][300/391]\tTime  0.091 ( 0.093)\tLoss 8.4703e-02 (1.4770e-01)\tAcc@1  98.44 ( 96.13)\tAcc@5 100.00 ( 99.67)\n","Epoch: [90][330/391]\tTime  0.088 ( 0.093)\tLoss 9.9068e-02 (1.4587e-01)\tAcc@1  98.44 ( 96.19)\tAcc@5 100.00 ( 99.67)\n","Epoch: [90][360/391]\tTime  0.092 ( 0.093)\tLoss 1.3200e-01 (1.4327e-01)\tAcc@1  95.31 ( 96.26)\tAcc@5 100.00 ( 99.67)\n","Epoch: [90][390/391]\tTime  0.082 ( 0.093)\tLoss 1.8056e-01 (1.4071e-01)\tAcc@1  92.50 ( 96.33)\tAcc@5 100.00 ( 99.68)\n","==> Train Accuracy: Acc@1 96.326 || Acc@5 99.684\n","==> Test Accuracy:  Acc@1 76.280 || Acc@5 93.630\n","==> 39.44 seconds to train this epoch\n","\n","\n","----- epoch: 91, lr: 0.004000000000000001 -----\n","Epoch: [91][  0/391]\tTime  0.302 ( 0.302)\tLoss 1.1513e-01 (1.1513e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5  99.22 ( 99.22)\n","Epoch: [91][ 30/391]\tTime  0.093 ( 0.101)\tLoss 1.0202e-01 (9.9107e-02)\tAcc@1  97.66 ( 97.51)\tAcc@5 100.00 ( 99.77)\n","Epoch: [91][ 60/391]\tTime  0.090 ( 0.097)\tLoss 6.3355e-02 (9.9739e-02)\tAcc@1  99.22 ( 97.59)\tAcc@5 100.00 ( 99.76)\n","Epoch: [91][ 90/391]\tTime  0.092 ( 0.095)\tLoss 5.8388e-02 (9.6747e-02)\tAcc@1  98.44 ( 97.78)\tAcc@5 100.00 ( 99.77)\n","Epoch: [91][120/391]\tTime  0.092 ( 0.098)\tLoss 5.4701e-02 (9.4908e-02)\tAcc@1  99.22 ( 97.84)\tAcc@5 100.00 ( 99.79)\n","Epoch: [91][150/391]\tTime  0.090 ( 0.097)\tLoss 1.3355e-01 (9.3383e-02)\tAcc@1  96.88 ( 97.83)\tAcc@5  99.22 ( 99.81)\n","Epoch: [91][180/391]\tTime  0.092 ( 0.096)\tLoss 9.9365e-02 (9.3133e-02)\tAcc@1  97.66 ( 97.82)\tAcc@5 100.00 ( 99.81)\n","Epoch: [91][210/391]\tTime  0.090 ( 0.095)\tLoss 7.1391e-02 (9.2456e-02)\tAcc@1  97.66 ( 97.81)\tAcc@5 100.00 ( 99.82)\n","Epoch: [91][240/391]\tTime  0.090 ( 0.095)\tLoss 2.1033e-01 (9.2921e-02)\tAcc@1  95.31 ( 97.80)\tAcc@5  98.44 ( 99.82)\n","Epoch: [91][270/391]\tTime  0.092 ( 0.095)\tLoss 6.7291e-02 (9.2408e-02)\tAcc@1  97.66 ( 97.79)\tAcc@5 100.00 ( 99.82)\n","Epoch: [91][300/391]\tTime  0.093 ( 0.095)\tLoss 1.0849e-01 (9.2398e-02)\tAcc@1  97.66 ( 97.77)\tAcc@5  99.22 ( 99.82)\n","Epoch: [91][330/391]\tTime  0.093 ( 0.094)\tLoss 5.9873e-02 (9.1748e-02)\tAcc@1  99.22 ( 97.79)\tAcc@5 100.00 ( 99.82)\n","Epoch: [91][360/391]\tTime  0.093 ( 0.094)\tLoss 1.0863e-01 (9.2543e-02)\tAcc@1  98.44 ( 97.77)\tAcc@5 100.00 ( 99.82)\n","Epoch: [91][390/391]\tTime  0.091 ( 0.094)\tLoss 7.1196e-02 (9.2107e-02)\tAcc@1  98.75 ( 97.77)\tAcc@5 100.00 ( 99.81)\n","==> Train Accuracy: Acc@1 97.774 || Acc@5 99.814\n","==> Test Accuracy:  Acc@1 76.990 || Acc@5 93.840\n","==> 39.88 seconds to train this epoch\n","\n","\n","----- epoch: 92, lr: 0.004000000000000001 -----\n","Epoch: [92][  0/391]\tTime  0.321 ( 0.321)\tLoss 4.9727e-02 (4.9727e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [92][ 30/391]\tTime  0.089 ( 0.099)\tLoss 5.1396e-02 (6.9345e-02)\tAcc@1  99.22 ( 98.46)\tAcc@5 100.00 ( 99.85)\n","Epoch: [92][ 60/391]\tTime  0.093 ( 0.096)\tLoss 3.4810e-02 (7.7850e-02)\tAcc@1 100.00 ( 98.09)\tAcc@5 100.00 ( 99.87)\n","Epoch: [92][ 90/391]\tTime  0.091 ( 0.095)\tLoss 5.0915e-02 (7.7610e-02)\tAcc@1  98.44 ( 98.13)\tAcc@5 100.00 ( 99.85)\n","Epoch: [92][120/391]\tTime  0.104 ( 0.097)\tLoss 5.0634e-02 (7.6376e-02)\tAcc@1  99.22 ( 98.19)\tAcc@5 100.00 ( 99.84)\n","Epoch: [92][150/391]\tTime  0.095 ( 0.096)\tLoss 4.5413e-02 (7.7028e-02)\tAcc@1 100.00 ( 98.14)\tAcc@5 100.00 ( 99.83)\n","Epoch: [92][180/391]\tTime  0.090 ( 0.095)\tLoss 5.3424e-02 (7.8304e-02)\tAcc@1  98.44 ( 98.14)\tAcc@5 100.00 ( 99.83)\n","Epoch: [92][210/391]\tTime  0.094 ( 0.095)\tLoss 4.2646e-02 (7.7090e-02)\tAcc@1 100.00 ( 98.17)\tAcc@5 100.00 ( 99.82)\n","Epoch: [92][240/391]\tTime  0.088 ( 0.095)\tLoss 5.1397e-02 (7.7367e-02)\tAcc@1  98.44 ( 98.16)\tAcc@5 100.00 ( 99.82)\n","Epoch: [92][270/391]\tTime  0.094 ( 0.095)\tLoss 1.0062e-01 (7.8116e-02)\tAcc@1  96.88 ( 98.14)\tAcc@5 100.00 ( 99.82)\n","Epoch: [92][300/391]\tTime  0.092 ( 0.094)\tLoss 2.3601e-02 (7.7388e-02)\tAcc@1 100.00 ( 98.16)\tAcc@5 100.00 ( 99.82)\n","Epoch: [92][330/391]\tTime  0.091 ( 0.094)\tLoss 5.3614e-02 (7.7821e-02)\tAcc@1  99.22 ( 98.17)\tAcc@5 100.00 ( 99.82)\n","Epoch: [92][360/391]\tTime  0.096 ( 0.094)\tLoss 1.9659e-01 (7.7431e-02)\tAcc@1  92.97 ( 98.19)\tAcc@5  99.22 ( 99.82)\n","Epoch: [92][390/391]\tTime  0.082 ( 0.094)\tLoss 4.0305e-02 (7.7642e-02)\tAcc@1 100.00 ( 98.20)\tAcc@5 100.00 ( 99.81)\n","==> Train Accuracy: Acc@1 98.200 || Acc@5 99.814\n","==> Test Accuracy:  Acc@1 77.010 || Acc@5 93.690\n","==> 39.84 seconds to train this epoch\n","\n","\n","----- epoch: 93, lr: 0.004000000000000001 -----\n","Epoch: [93][  0/391]\tTime  0.279 ( 0.279)\tLoss 1.3316e-01 (1.3316e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5  99.22 ( 99.22)\n","Epoch: [93][ 30/391]\tTime  0.098 ( 0.099)\tLoss 6.7680e-02 (7.8169e-02)\tAcc@1  97.66 ( 98.26)\tAcc@5 100.00 ( 99.77)\n","Epoch: [93][ 60/391]\tTime  0.104 ( 0.096)\tLoss 8.9165e-02 (7.5895e-02)\tAcc@1  97.66 ( 98.25)\tAcc@5  99.22 ( 99.82)\n","Epoch: [93][ 90/391]\tTime  0.092 ( 0.095)\tLoss 6.1478e-02 (7.4961e-02)\tAcc@1  99.22 ( 98.29)\tAcc@5  99.22 ( 99.80)\n","Epoch: [93][120/391]\tTime  0.093 ( 0.097)\tLoss 3.4976e-02 (7.5308e-02)\tAcc@1 100.00 ( 98.31)\tAcc@5 100.00 ( 99.80)\n","Epoch: [93][150/391]\tTime  0.090 ( 0.096)\tLoss 4.1508e-02 (7.2288e-02)\tAcc@1  98.44 ( 98.42)\tAcc@5 100.00 ( 99.83)\n","Epoch: [93][180/391]\tTime  0.088 ( 0.096)\tLoss 5.7353e-02 (7.2649e-02)\tAcc@1  98.44 ( 98.40)\tAcc@5 100.00 ( 99.83)\n","Epoch: [93][210/391]\tTime  0.091 ( 0.095)\tLoss 1.5576e-01 (7.2613e-02)\tAcc@1  96.09 ( 98.43)\tAcc@5  98.44 ( 99.83)\n","Epoch: [93][240/391]\tTime  0.093 ( 0.095)\tLoss 5.1728e-02 (7.2261e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 ( 99.82)\n","Epoch: [93][270/391]\tTime  0.090 ( 0.095)\tLoss 7.5133e-02 (7.2879e-02)\tAcc@1  96.09 ( 98.38)\tAcc@5 100.00 ( 99.83)\n","Epoch: [93][300/391]\tTime  0.092 ( 0.094)\tLoss 3.5477e-02 (7.3022e-02)\tAcc@1 100.00 ( 98.36)\tAcc@5 100.00 ( 99.84)\n","Epoch: [93][330/391]\tTime  0.095 ( 0.094)\tLoss 7.7106e-02 (7.2147e-02)\tAcc@1  98.44 ( 98.38)\tAcc@5 100.00 ( 99.84)\n","Epoch: [93][360/391]\tTime  0.095 ( 0.094)\tLoss 1.1523e-01 (7.2000e-02)\tAcc@1  96.88 ( 98.38)\tAcc@5 100.00 ( 99.85)\n","Epoch: [93][390/391]\tTime  0.086 ( 0.094)\tLoss 1.4132e-01 (7.2043e-02)\tAcc@1  97.50 ( 98.38)\tAcc@5 100.00 ( 99.85)\n","==> Train Accuracy: Acc@1 98.378 || Acc@5 99.846\n","==> Test Accuracy:  Acc@1 76.490 || Acc@5 93.710\n","==> 39.89 seconds to train this epoch\n","\n","\n","----- epoch: 94, lr: 0.004000000000000001 -----\n","Epoch: [94][  0/391]\tTime  0.295 ( 0.295)\tLoss 6.0102e-02 (6.0102e-02)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n","Epoch: [94][ 30/391]\tTime  0.090 ( 0.099)\tLoss 2.9373e-02 (6.6459e-02)\tAcc@1 100.00 ( 98.24)\tAcc@5 100.00 ( 99.85)\n","Epoch: [94][ 60/391]\tTime  0.094 ( 0.096)\tLoss 1.2493e-01 (6.5978e-02)\tAcc@1  96.88 ( 98.27)\tAcc@5 100.00 ( 99.87)\n","Epoch: [94][ 90/391]\tTime  0.090 ( 0.095)\tLoss 5.5857e-02 (6.4439e-02)\tAcc@1  99.22 ( 98.40)\tAcc@5 100.00 ( 99.87)\n","Epoch: [94][120/391]\tTime  0.091 ( 0.094)\tLoss 7.5556e-02 (6.6662e-02)\tAcc@1  98.44 ( 98.38)\tAcc@5  99.22 ( 99.85)\n","Epoch: [94][150/391]\tTime  0.093 ( 0.094)\tLoss 7.1999e-02 (6.6001e-02)\tAcc@1  97.66 ( 98.39)\tAcc@5 100.00 ( 99.85)\n","Epoch: [94][180/391]\tTime  0.095 ( 0.094)\tLoss 2.8212e-02 (6.5986e-02)\tAcc@1 100.00 ( 98.41)\tAcc@5 100.00 ( 99.85)\n","Epoch: [94][210/391]\tTime  0.092 ( 0.094)\tLoss 3.7916e-02 (6.4947e-02)\tAcc@1 100.00 ( 98.44)\tAcc@5 100.00 ( 99.86)\n","Epoch: [94][240/391]\tTime  0.093 ( 0.093)\tLoss 5.2493e-02 (6.4117e-02)\tAcc@1  99.22 ( 98.47)\tAcc@5  99.22 ( 99.86)\n","Epoch: [94][270/391]\tTime  0.088 ( 0.093)\tLoss 6.9638e-02 (6.4287e-02)\tAcc@1  97.66 ( 98.44)\tAcc@5 100.00 ( 99.86)\n","Epoch: [94][300/391]\tTime  0.091 ( 0.093)\tLoss 4.6630e-02 (6.3960e-02)\tAcc@1  99.22 ( 98.43)\tAcc@5 100.00 ( 99.86)\n","Epoch: [94][330/391]\tTime  0.098 ( 0.093)\tLoss 3.6661e-02 (6.3789e-02)\tAcc@1 100.00 ( 98.45)\tAcc@5 100.00 ( 99.86)\n","Epoch: [94][360/391]\tTime  0.103 ( 0.093)\tLoss 6.4515e-02 (6.3854e-02)\tAcc@1  98.44 ( 98.45)\tAcc@5 100.00 ( 99.87)\n","Epoch: [94][390/391]\tTime  0.082 ( 0.093)\tLoss 5.2797e-02 (6.4477e-02)\tAcc@1  98.75 ( 98.43)\tAcc@5 100.00 ( 99.87)\n","==> Train Accuracy: Acc@1 98.428 || Acc@5 99.868\n","==> Test Accuracy:  Acc@1 76.830 || Acc@5 93.630\n","==> 39.45 seconds to train this epoch\n","\n","\n","----- epoch: 95, lr: 0.004000000000000001 -----\n","Epoch: [95][  0/391]\tTime  0.254 ( 0.254)\tLoss 3.3991e-02 (3.3991e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [95][ 30/391]\tTime  0.096 ( 0.099)\tLoss 6.7659e-02 (5.7153e-02)\tAcc@1  98.44 ( 98.61)\tAcc@5 100.00 ( 99.92)\n","Epoch: [95][ 60/391]\tTime  0.089 ( 0.096)\tLoss 1.1081e-01 (5.9682e-02)\tAcc@1  97.66 ( 98.50)\tAcc@5 100.00 ( 99.92)\n","Epoch: [95][ 90/391]\tTime  0.089 ( 0.095)\tLoss 3.9141e-02 (6.0222e-02)\tAcc@1  99.22 ( 98.63)\tAcc@5 100.00 ( 99.90)\n","Epoch: [95][120/391]\tTime  0.093 ( 0.094)\tLoss 3.4077e-02 (5.9585e-02)\tAcc@1 100.00 ( 98.66)\tAcc@5 100.00 ( 99.91)\n","Epoch: [95][150/391]\tTime  0.094 ( 0.094)\tLoss 7.9176e-02 (6.0307e-02)\tAcc@1  96.88 ( 98.62)\tAcc@5 100.00 ( 99.90)\n","Epoch: [95][180/391]\tTime  0.096 ( 0.094)\tLoss 4.7242e-02 (6.0313e-02)\tAcc@1 100.00 ( 98.64)\tAcc@5 100.00 ( 99.91)\n","Epoch: [95][210/391]\tTime  0.094 ( 0.094)\tLoss 7.9290e-02 (6.0481e-02)\tAcc@1  96.88 ( 98.62)\tAcc@5 100.00 ( 99.90)\n","Epoch: [95][240/391]\tTime  0.092 ( 0.093)\tLoss 3.3951e-02 (5.9885e-02)\tAcc@1 100.00 ( 98.64)\tAcc@5 100.00 ( 99.90)\n","Epoch: [95][270/391]\tTime  0.092 ( 0.093)\tLoss 4.6339e-02 (5.9822e-02)\tAcc@1  99.22 ( 98.65)\tAcc@5 100.00 ( 99.90)\n","Epoch: [95][300/391]\tTime  0.095 ( 0.093)\tLoss 5.5195e-02 (6.0617e-02)\tAcc@1  97.66 ( 98.62)\tAcc@5 100.00 ( 99.90)\n","Epoch: [95][330/391]\tTime  0.092 ( 0.093)\tLoss 4.8268e-02 (6.0986e-02)\tAcc@1 100.00 ( 98.63)\tAcc@5 100.00 ( 99.89)\n","Epoch: [95][360/391]\tTime  0.090 ( 0.093)\tLoss 6.1186e-02 (6.0369e-02)\tAcc@1  98.44 ( 98.66)\tAcc@5 100.00 ( 99.89)\n","Epoch: [95][390/391]\tTime  0.082 ( 0.093)\tLoss 5.5791e-02 (6.0635e-02)\tAcc@1  97.50 ( 98.65)\tAcc@5 100.00 ( 99.89)\n","==> Train Accuracy: Acc@1 98.648 || Acc@5 99.888\n","==> Test Accuracy:  Acc@1 77.020 || Acc@5 93.830\n","==> 39.54 seconds to train this epoch\n","\n","\n","----- epoch: 96, lr: 0.004000000000000001 -----\n","Epoch: [96][  0/391]\tTime  0.324 ( 0.324)\tLoss 2.5193e-02 (2.5193e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [96][ 30/391]\tTime  0.078 ( 0.101)\tLoss 6.5822e-02 (5.7378e-02)\tAcc@1  97.66 ( 98.79)\tAcc@5 100.00 ( 99.80)\n","Epoch: [96][ 60/391]\tTime  0.088 ( 0.097)\tLoss 3.1080e-02 (5.5364e-02)\tAcc@1 100.00 ( 98.73)\tAcc@5 100.00 ( 99.85)\n","Epoch: [96][ 90/391]\tTime  0.090 ( 0.095)\tLoss 5.6164e-02 (5.8219e-02)\tAcc@1  99.22 ( 98.66)\tAcc@5 100.00 ( 99.87)\n","Epoch: [96][120/391]\tTime  0.105 ( 0.097)\tLoss 6.3546e-02 (5.9589e-02)\tAcc@1  99.22 ( 98.63)\tAcc@5 100.00 ( 99.86)\n","Epoch: [96][150/391]\tTime  0.096 ( 0.096)\tLoss 1.9467e-02 (6.0021e-02)\tAcc@1 100.00 ( 98.61)\tAcc@5 100.00 ( 99.87)\n","Epoch: [96][180/391]\tTime  0.097 ( 0.096)\tLoss 4.9872e-02 (5.9830e-02)\tAcc@1  98.44 ( 98.60)\tAcc@5 100.00 ( 99.87)\n","Epoch: [96][210/391]\tTime  0.095 ( 0.095)\tLoss 9.9535e-02 (6.0365e-02)\tAcc@1  98.44 ( 98.59)\tAcc@5  99.22 ( 99.87)\n","Epoch: [96][240/391]\tTime  0.091 ( 0.095)\tLoss 7.5424e-02 (6.0349e-02)\tAcc@1  97.66 ( 98.60)\tAcc@5 100.00 ( 99.87)\n","Epoch: [96][270/391]\tTime  0.091 ( 0.095)\tLoss 1.9228e-02 (6.0792e-02)\tAcc@1 100.00 ( 98.59)\tAcc@5 100.00 ( 99.88)\n","Epoch: [96][300/391]\tTime  0.092 ( 0.094)\tLoss 4.6039e-02 (6.0472e-02)\tAcc@1  98.44 ( 98.60)\tAcc@5 100.00 ( 99.87)\n","Epoch: [96][330/391]\tTime  0.093 ( 0.094)\tLoss 3.1097e-02 (6.0069e-02)\tAcc@1  99.22 ( 98.62)\tAcc@5 100.00 ( 99.86)\n","Epoch: [96][360/391]\tTime  0.091 ( 0.094)\tLoss 3.1406e-02 (5.9839e-02)\tAcc@1 100.00 ( 98.64)\tAcc@5 100.00 ( 99.87)\n","Epoch: [96][390/391]\tTime  0.082 ( 0.094)\tLoss 1.4463e-01 (6.0022e-02)\tAcc@1  95.00 ( 98.65)\tAcc@5  98.75 ( 99.87)\n","==> Train Accuracy: Acc@1 98.646 || Acc@5 99.868\n","==> Test Accuracy:  Acc@1 77.670 || Acc@5 93.770\n","==> 39.86 seconds to train this epoch\n","\n","\n","----- epoch: 97, lr: 0.004000000000000001 -----\n","Epoch: [97][  0/391]\tTime  0.316 ( 0.316)\tLoss 4.4593e-02 (4.4593e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [97][ 30/391]\tTime  0.091 ( 0.099)\tLoss 2.5849e-02 (6.0553e-02)\tAcc@1 100.00 ( 98.66)\tAcc@5 100.00 ( 99.82)\n","Epoch: [97][ 60/391]\tTime  0.090 ( 0.096)\tLoss 4.6478e-02 (5.8527e-02)\tAcc@1  98.44 ( 98.66)\tAcc@5 100.00 ( 99.82)\n","Epoch: [97][ 90/391]\tTime  0.108 ( 0.097)\tLoss 7.3806e-02 (5.7529e-02)\tAcc@1  96.88 ( 98.68)\tAcc@5 100.00 ( 99.85)\n","Epoch: [97][120/391]\tTime  0.092 ( 0.097)\tLoss 3.6517e-02 (5.7876e-02)\tAcc@1 100.00 ( 98.65)\tAcc@5 100.00 ( 99.86)\n","Epoch: [97][150/391]\tTime  0.091 ( 0.096)\tLoss 7.3250e-02 (5.7325e-02)\tAcc@1  98.44 ( 98.69)\tAcc@5 100.00 ( 99.88)\n","Epoch: [97][180/391]\tTime  0.093 ( 0.096)\tLoss 2.2862e-02 (5.8474e-02)\tAcc@1 100.00 ( 98.64)\tAcc@5 100.00 ( 99.87)\n","Epoch: [97][210/391]\tTime  0.091 ( 0.095)\tLoss 4.6921e-02 (5.7853e-02)\tAcc@1 100.00 ( 98.68)\tAcc@5 100.00 ( 99.87)\n","Epoch: [97][240/391]\tTime  0.090 ( 0.095)\tLoss 3.1184e-02 (5.6793e-02)\tAcc@1 100.00 ( 98.70)\tAcc@5 100.00 ( 99.88)\n","Epoch: [97][270/391]\tTime  0.102 ( 0.095)\tLoss 4.6169e-02 (5.6982e-02)\tAcc@1  99.22 ( 98.68)\tAcc@5 100.00 ( 99.88)\n","Epoch: [97][300/391]\tTime  0.089 ( 0.094)\tLoss 5.2940e-02 (5.6328e-02)\tAcc@1  98.44 ( 98.73)\tAcc@5 100.00 ( 99.89)\n","Epoch: [97][330/391]\tTime  0.094 ( 0.094)\tLoss 3.4863e-02 (5.6302e-02)\tAcc@1  99.22 ( 98.73)\tAcc@5 100.00 ( 99.89)\n","Epoch: [97][360/391]\tTime  0.089 ( 0.094)\tLoss 4.1654e-02 (5.5928e-02)\tAcc@1  99.22 ( 98.73)\tAcc@5 100.00 ( 99.89)\n","Epoch: [97][390/391]\tTime  0.082 ( 0.094)\tLoss 5.2823e-02 (5.6042e-02)\tAcc@1  97.50 ( 98.73)\tAcc@5 100.00 ( 99.89)\n","==> Train Accuracy: Acc@1 98.726 || Acc@5 99.894\n","==> Test Accuracy:  Acc@1 77.180 || Acc@5 93.730\n","==> 39.87 seconds to train this epoch\n","\n","\n","----- epoch: 98, lr: 0.004000000000000001 -----\n","Epoch: [98][  0/391]\tTime  0.310 ( 0.310)\tLoss 2.4468e-02 (2.4468e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [98][ 30/391]\tTime  0.091 ( 0.099)\tLoss 2.0937e-02 (4.6043e-02)\tAcc@1 100.00 ( 99.09)\tAcc@5 100.00 ( 99.85)\n","Epoch: [98][ 60/391]\tTime  0.106 ( 0.096)\tLoss 2.8109e-02 (5.1112e-02)\tAcc@1  99.22 ( 98.96)\tAcc@5 100.00 ( 99.83)\n","Epoch: [98][ 90/391]\tTime  0.091 ( 0.095)\tLoss 2.3566e-02 (5.0725e-02)\tAcc@1  99.22 ( 98.93)\tAcc@5 100.00 ( 99.85)\n","Epoch: [98][120/391]\tTime  0.091 ( 0.095)\tLoss 6.9821e-02 (5.1626e-02)\tAcc@1  98.44 ( 98.86)\tAcc@5  99.22 ( 99.85)\n","Epoch: [98][150/391]\tTime  0.093 ( 0.094)\tLoss 5.1822e-02 (5.1479e-02)\tAcc@1  99.22 ( 98.85)\tAcc@5 100.00 ( 99.87)\n","Epoch: [98][180/391]\tTime  0.093 ( 0.094)\tLoss 2.3254e-02 (5.1169e-02)\tAcc@1 100.00 ( 98.86)\tAcc@5 100.00 ( 99.89)\n","Epoch: [98][210/391]\tTime  0.094 ( 0.094)\tLoss 3.6849e-02 (5.1742e-02)\tAcc@1  99.22 ( 98.85)\tAcc@5 100.00 ( 99.89)\n","Epoch: [98][240/391]\tTime  0.095 ( 0.094)\tLoss 4.6272e-02 (5.0825e-02)\tAcc@1  98.44 ( 98.88)\tAcc@5 100.00 ( 99.89)\n","Epoch: [98][270/391]\tTime  0.091 ( 0.094)\tLoss 6.0534e-02 (5.0837e-02)\tAcc@1  98.44 ( 98.88)\tAcc@5  99.22 ( 99.90)\n","Epoch: [98][300/391]\tTime  0.093 ( 0.093)\tLoss 4.5636e-02 (5.0826e-02)\tAcc@1  99.22 ( 98.88)\tAcc@5 100.00 ( 99.91)\n","Epoch: [98][330/391]\tTime  0.093 ( 0.093)\tLoss 2.7241e-02 (5.1066e-02)\tAcc@1 100.00 ( 98.87)\tAcc@5 100.00 ( 99.91)\n","Epoch: [98][360/391]\tTime  0.091 ( 0.093)\tLoss 5.7544e-02 (5.1018e-02)\tAcc@1  98.44 ( 98.87)\tAcc@5 100.00 ( 99.91)\n","Epoch: [98][390/391]\tTime  0.082 ( 0.093)\tLoss 4.1990e-02 (5.0709e-02)\tAcc@1  98.75 ( 98.88)\tAcc@5 100.00 ( 99.91)\n","==> Train Accuracy: Acc@1 98.880 || Acc@5 99.912\n","==> Test Accuracy:  Acc@1 77.280 || Acc@5 93.850\n","==> 39.53 seconds to train this epoch\n","\n","\n","----- epoch: 99, lr: 0.004000000000000001 -----\n","Epoch: [99][  0/391]\tTime  0.249 ( 0.249)\tLoss 4.1138e-02 (4.1138e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [99][ 30/391]\tTime  0.089 ( 0.099)\tLoss 5.5804e-02 (6.0641e-02)\tAcc@1  99.22 ( 98.29)\tAcc@5 100.00 ( 99.87)\n","Epoch: [99][ 60/391]\tTime  0.090 ( 0.096)\tLoss 2.9509e-02 (5.4665e-02)\tAcc@1  99.22 ( 98.63)\tAcc@5 100.00 ( 99.86)\n","Epoch: [99][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.9888e-02 (5.2335e-02)\tAcc@1 100.00 ( 98.70)\tAcc@5 100.00 ( 99.88)\n","Epoch: [99][120/391]\tTime  0.089 ( 0.094)\tLoss 8.7123e-02 (5.1302e-02)\tAcc@1  99.22 ( 98.81)\tAcc@5  99.22 ( 99.88)\n","Epoch: [99][150/391]\tTime  0.094 ( 0.094)\tLoss 9.4494e-02 (5.1965e-02)\tAcc@1  96.88 ( 98.76)\tAcc@5 100.00 ( 99.88)\n","Epoch: [99][180/391]\tTime  0.094 ( 0.094)\tLoss 4.1612e-02 (5.1700e-02)\tAcc@1  98.44 ( 98.78)\tAcc@5 100.00 ( 99.89)\n","Epoch: [99][210/391]\tTime  0.092 ( 0.093)\tLoss 5.1935e-02 (5.1575e-02)\tAcc@1  99.22 ( 98.80)\tAcc@5 100.00 ( 99.90)\n","Epoch: [99][240/391]\tTime  0.094 ( 0.093)\tLoss 4.7876e-02 (5.1402e-02)\tAcc@1  97.66 ( 98.79)\tAcc@5 100.00 ( 99.90)\n","Epoch: [99][270/391]\tTime  0.092 ( 0.093)\tLoss 6.8777e-02 (5.2525e-02)\tAcc@1  97.66 ( 98.75)\tAcc@5  99.22 ( 99.88)\n","Epoch: [99][300/391]\tTime  0.091 ( 0.093)\tLoss 3.2557e-02 (5.2437e-02)\tAcc@1 100.00 ( 98.75)\tAcc@5 100.00 ( 99.88)\n","Epoch: [99][330/391]\tTime  0.090 ( 0.093)\tLoss 1.9541e-02 (5.1602e-02)\tAcc@1 100.00 ( 98.78)\tAcc@5 100.00 ( 99.89)\n","Epoch: [99][360/391]\tTime  0.098 ( 0.093)\tLoss 7.2081e-02 (5.1516e-02)\tAcc@1  99.22 ( 98.80)\tAcc@5 100.00 ( 99.88)\n","Epoch: [99][390/391]\tTime  0.082 ( 0.093)\tLoss 5.3487e-02 (5.1786e-02)\tAcc@1  97.50 ( 98.80)\tAcc@5 100.00 ( 99.88)\n","==> Train Accuracy: Acc@1 98.802 || Acc@5 99.880\n","==> Test Accuracy:  Acc@1 77.370 || Acc@5 93.790\n","==> 39.52 seconds to train this epoch\n","\n","\n","----- epoch: 100, lr: 0.004000000000000001 -----\n","Epoch: [100][  0/391]\tTime  0.292 ( 0.292)\tLoss 2.1178e-02 (2.1178e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [100][ 30/391]\tTime  0.094 ( 0.099)\tLoss 1.8015e-02 (5.0381e-02)\tAcc@1 100.00 ( 98.97)\tAcc@5 100.00 ( 99.82)\n","Epoch: [100][ 60/391]\tTime  0.091 ( 0.096)\tLoss 3.9372e-02 (5.0171e-02)\tAcc@1  99.22 ( 98.86)\tAcc@5 100.00 ( 99.86)\n","Epoch: [100][ 90/391]\tTime  0.091 ( 0.095)\tLoss 2.7800e-02 (4.6272e-02)\tAcc@1 100.00 ( 98.99)\tAcc@5 100.00 ( 99.90)\n","Epoch: [100][120/391]\tTime  0.092 ( 0.094)\tLoss 1.6765e-02 (4.6240e-02)\tAcc@1 100.00 ( 98.97)\tAcc@5 100.00 ( 99.89)\n","Epoch: [100][150/391]\tTime  0.093 ( 0.094)\tLoss 1.1715e-01 (4.7690e-02)\tAcc@1  97.66 ( 98.97)\tAcc@5  99.22 ( 99.89)\n","Epoch: [100][180/391]\tTime  0.090 ( 0.094)\tLoss 6.6888e-02 (4.8582e-02)\tAcc@1  98.44 ( 98.93)\tAcc@5  99.22 ( 99.89)\n","Epoch: [100][210/391]\tTime  0.081 ( 0.093)\tLoss 3.6796e-02 (4.8625e-02)\tAcc@1  99.22 ( 98.94)\tAcc@5 100.00 ( 99.89)\n","Epoch: [100][240/391]\tTime  0.094 ( 0.093)\tLoss 2.4426e-02 (4.7896e-02)\tAcc@1 100.00 ( 98.97)\tAcc@5 100.00 ( 99.89)\n","Epoch: [100][270/391]\tTime  0.103 ( 0.093)\tLoss 2.1088e-02 (4.8968e-02)\tAcc@1 100.00 ( 98.94)\tAcc@5 100.00 ( 99.88)\n","Epoch: [100][300/391]\tTime  0.094 ( 0.093)\tLoss 4.4200e-02 (4.9641e-02)\tAcc@1  99.22 ( 98.94)\tAcc@5 100.00 ( 99.88)\n","Epoch: [100][330/391]\tTime  0.090 ( 0.093)\tLoss 3.3300e-02 (4.9595e-02)\tAcc@1  99.22 ( 98.94)\tAcc@5 100.00 ( 99.88)\n","Epoch: [100][360/391]\tTime  0.095 ( 0.093)\tLoss 1.6854e-02 (4.9351e-02)\tAcc@1 100.00 ( 98.95)\tAcc@5 100.00 ( 99.89)\n","Epoch: [100][390/391]\tTime  0.082 ( 0.093)\tLoss 4.9153e-02 (4.9657e-02)\tAcc@1  98.75 ( 98.93)\tAcc@5 100.00 ( 99.89)\n","==> Train Accuracy: Acc@1 98.934 || Acc@5 99.886\n","==> Test Accuracy:  Acc@1 77.340 || Acc@5 93.720\n","==> 39.40 seconds to train this epoch\n","\n","\n","----- epoch: 101, lr: 0.004000000000000001 -----\n","Epoch: [101][  0/391]\tTime  0.315 ( 0.315)\tLoss 3.2603e-02 (3.2603e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [101][ 30/391]\tTime  0.094 ( 0.100)\tLoss 3.5692e-02 (4.7017e-02)\tAcc@1  98.44 ( 99.09)\tAcc@5 100.00 ( 99.85)\n","Epoch: [101][ 60/391]\tTime  0.098 ( 0.097)\tLoss 4.4017e-02 (4.5529e-02)\tAcc@1  98.44 ( 99.07)\tAcc@5 100.00 ( 99.90)\n","Epoch: [101][ 90/391]\tTime  0.093 ( 0.095)\tLoss 5.6171e-02 (5.0346e-02)\tAcc@1  98.44 ( 98.83)\tAcc@5 100.00 ( 99.91)\n","Epoch: [101][120/391]\tTime  0.092 ( 0.095)\tLoss 2.2615e-02 (4.7504e-02)\tAcc@1 100.00 ( 98.93)\tAcc@5 100.00 ( 99.92)\n","Epoch: [101][150/391]\tTime  0.088 ( 0.094)\tLoss 3.4783e-02 (4.8314e-02)\tAcc@1  98.44 ( 98.92)\tAcc@5 100.00 ( 99.92)\n","Epoch: [101][180/391]\tTime  0.093 ( 0.094)\tLoss 5.2499e-02 (4.8349e-02)\tAcc@1  98.44 ( 98.91)\tAcc@5 100.00 ( 99.92)\n","Epoch: [101][210/391]\tTime  0.091 ( 0.094)\tLoss 2.9615e-02 (4.8694e-02)\tAcc@1  99.22 ( 98.92)\tAcc@5 100.00 ( 99.91)\n","Epoch: [101][240/391]\tTime  0.104 ( 0.094)\tLoss 8.6995e-02 (4.8136e-02)\tAcc@1  99.22 ( 98.93)\tAcc@5  99.22 ( 99.91)\n","Epoch: [101][270/391]\tTime  0.093 ( 0.094)\tLoss 1.6902e-02 (4.8298e-02)\tAcc@1 100.00 ( 98.94)\tAcc@5 100.00 ( 99.92)\n","Epoch: [101][300/391]\tTime  0.089 ( 0.093)\tLoss 4.0088e-02 (4.8659e-02)\tAcc@1  98.44 ( 98.92)\tAcc@5 100.00 ( 99.91)\n","Epoch: [101][330/391]\tTime  0.094 ( 0.093)\tLoss 3.8829e-02 (4.8763e-02)\tAcc@1  98.44 ( 98.92)\tAcc@5 100.00 ( 99.91)\n","Epoch: [101][360/391]\tTime  0.096 ( 0.093)\tLoss 2.1183e-02 (4.9047e-02)\tAcc@1 100.00 ( 98.91)\tAcc@5 100.00 ( 99.91)\n","Epoch: [101][390/391]\tTime  0.082 ( 0.093)\tLoss 1.2063e-02 (4.8560e-02)\tAcc@1 100.00 ( 98.92)\tAcc@5 100.00 ( 99.92)\n","==> Train Accuracy: Acc@1 98.924 || Acc@5 99.918\n","==> Test Accuracy:  Acc@1 77.490 || Acc@5 94.050\n","==> 39.54 seconds to train this epoch\n","\n","\n","----- epoch: 102, lr: 0.004000000000000001 -----\n","Epoch: [102][  0/391]\tTime  0.286 ( 0.286)\tLoss 6.0482e-02 (6.0482e-02)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [102][ 30/391]\tTime  0.092 ( 0.099)\tLoss 5.5021e-02 (4.2735e-02)\tAcc@1  99.22 ( 99.14)\tAcc@5  99.22 ( 99.82)\n","Epoch: [102][ 60/391]\tTime  0.090 ( 0.096)\tLoss 5.5887e-02 (4.1701e-02)\tAcc@1  97.66 ( 99.10)\tAcc@5 100.00 ( 99.91)\n","Epoch: [102][ 90/391]\tTime  0.087 ( 0.095)\tLoss 4.0359e-02 (4.2253e-02)\tAcc@1  99.22 ( 99.12)\tAcc@5 100.00 ( 99.91)\n","Epoch: [102][120/391]\tTime  0.105 ( 0.094)\tLoss 2.3036e-02 (4.2241e-02)\tAcc@1  99.22 ( 99.14)\tAcc@5 100.00 ( 99.92)\n","Epoch: [102][150/391]\tTime  0.094 ( 0.094)\tLoss 2.1971e-02 (4.2833e-02)\tAcc@1 100.00 ( 99.09)\tAcc@5 100.00 ( 99.93)\n","Epoch: [102][180/391]\tTime  0.089 ( 0.094)\tLoss 1.8809e-02 (4.3983e-02)\tAcc@1 100.00 ( 99.07)\tAcc@5 100.00 ( 99.91)\n","Epoch: [102][210/391]\tTime  0.097 ( 0.094)\tLoss 3.3445e-02 (4.5083e-02)\tAcc@1  99.22 ( 99.03)\tAcc@5 100.00 ( 99.91)\n","Epoch: [102][240/391]\tTime  0.095 ( 0.093)\tLoss 4.1762e-02 (4.5341e-02)\tAcc@1  98.44 ( 99.01)\tAcc@5 100.00 ( 99.90)\n","Epoch: [102][270/391]\tTime  0.091 ( 0.093)\tLoss 6.3628e-02 (4.6604e-02)\tAcc@1  99.22 ( 98.98)\tAcc@5  99.22 ( 99.90)\n","Epoch: [102][300/391]\tTime  0.091 ( 0.093)\tLoss 3.8568e-02 (4.6851e-02)\tAcc@1  99.22 ( 98.97)\tAcc@5 100.00 ( 99.91)\n","Epoch: [102][330/391]\tTime  0.092 ( 0.093)\tLoss 7.6275e-02 (4.7891e-02)\tAcc@1  99.22 ( 98.94)\tAcc@5 100.00 ( 99.90)\n","Epoch: [102][360/391]\tTime  0.091 ( 0.093)\tLoss 6.7994e-02 (4.8274e-02)\tAcc@1  98.44 ( 98.93)\tAcc@5 100.00 ( 99.90)\n","Epoch: [102][390/391]\tTime  0.083 ( 0.093)\tLoss 3.4292e-02 (4.8113e-02)\tAcc@1 100.00 ( 98.94)\tAcc@5 100.00 ( 99.90)\n","==> Train Accuracy: Acc@1 98.940 || Acc@5 99.904\n","==> Test Accuracy:  Acc@1 77.500 || Acc@5 93.840\n","==> 39.42 seconds to train this epoch\n","\n","\n","----- epoch: 103, lr: 0.004000000000000001 -----\n","Epoch: [103][  0/391]\tTime  0.287 ( 0.287)\tLoss 2.2492e-02 (2.2492e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [103][ 30/391]\tTime  0.093 ( 0.099)\tLoss 6.2890e-02 (4.5685e-02)\tAcc@1  98.44 ( 98.99)\tAcc@5  99.22 ( 99.95)\n","Epoch: [103][ 60/391]\tTime  0.093 ( 0.096)\tLoss 4.6877e-02 (4.4444e-02)\tAcc@1  99.22 ( 99.12)\tAcc@5 100.00 ( 99.94)\n","Epoch: [103][ 90/391]\tTime  0.094 ( 0.095)\tLoss 4.7809e-02 (4.5168e-02)\tAcc@1  99.22 ( 99.09)\tAcc@5 100.00 ( 99.92)\n","Epoch: [103][120/391]\tTime  0.095 ( 0.094)\tLoss 3.5380e-02 (4.5022e-02)\tAcc@1  99.22 ( 99.05)\tAcc@5 100.00 ( 99.91)\n","Epoch: [103][150/391]\tTime  0.088 ( 0.094)\tLoss 2.5257e-02 (4.3771e-02)\tAcc@1  99.22 ( 99.08)\tAcc@5 100.00 ( 99.93)\n","Epoch: [103][180/391]\tTime  0.093 ( 0.094)\tLoss 9.3652e-02 (4.4575e-02)\tAcc@1  97.66 ( 99.05)\tAcc@5  99.22 ( 99.91)\n","Epoch: [103][210/391]\tTime  0.093 ( 0.094)\tLoss 1.9884e-02 (4.3819e-02)\tAcc@1 100.00 ( 99.05)\tAcc@5 100.00 ( 99.91)\n","Epoch: [103][240/391]\tTime  0.090 ( 0.093)\tLoss 4.9875e-02 (4.5096e-02)\tAcc@1  99.22 ( 99.01)\tAcc@5 100.00 ( 99.90)\n","Epoch: [103][270/391]\tTime  0.108 ( 0.093)\tLoss 4.5233e-02 (4.5259e-02)\tAcc@1  99.22 ( 99.02)\tAcc@5 100.00 ( 99.90)\n","Epoch: [103][300/391]\tTime  0.103 ( 0.093)\tLoss 4.1264e-02 (4.4958e-02)\tAcc@1  99.22 ( 99.03)\tAcc@5 100.00 ( 99.91)\n","Epoch: [103][330/391]\tTime  0.092 ( 0.093)\tLoss 4.4028e-02 (4.4825e-02)\tAcc@1  98.44 ( 99.03)\tAcc@5 100.00 ( 99.91)\n","Epoch: [103][360/391]\tTime  0.100 ( 0.093)\tLoss 4.9750e-02 (4.5160e-02)\tAcc@1  98.44 ( 99.01)\tAcc@5 100.00 ( 99.91)\n","Epoch: [103][390/391]\tTime  0.082 ( 0.093)\tLoss 5.8940e-02 (4.4900e-02)\tAcc@1  98.75 ( 99.02)\tAcc@5 100.00 ( 99.92)\n","==> Train Accuracy: Acc@1 99.020 || Acc@5 99.918\n","==> Test Accuracy:  Acc@1 77.920 || Acc@5 94.120\n","==> 39.41 seconds to train this epoch\n","\n","\n","----- epoch: 104, lr: 0.004000000000000001 -----\n","Epoch: [104][  0/391]\tTime  0.286 ( 0.286)\tLoss 2.5199e-02 (2.5199e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [104][ 30/391]\tTime  0.093 ( 0.101)\tLoss 8.2130e-02 (4.4593e-02)\tAcc@1  97.66 ( 99.04)\tAcc@5 100.00 ( 99.95)\n","Epoch: [104][ 60/391]\tTime  0.091 ( 0.097)\tLoss 3.8152e-02 (4.6949e-02)\tAcc@1  98.44 ( 99.01)\tAcc@5 100.00 ( 99.92)\n","Epoch: [104][ 90/391]\tTime  0.091 ( 0.096)\tLoss 1.3877e-01 (4.8716e-02)\tAcc@1  95.31 ( 98.99)\tAcc@5  99.22 ( 99.91)\n","Epoch: [104][120/391]\tTime  0.112 ( 0.098)\tLoss 5.4065e-02 (4.7803e-02)\tAcc@1  98.44 ( 98.95)\tAcc@5 100.00 ( 99.93)\n","Epoch: [104][150/391]\tTime  0.097 ( 0.097)\tLoss 6.0351e-02 (4.7541e-02)\tAcc@1  97.66 ( 98.97)\tAcc@5 100.00 ( 99.93)\n","Epoch: [104][180/391]\tTime  0.092 ( 0.096)\tLoss 9.1333e-02 (4.6865e-02)\tAcc@1  97.66 ( 98.98)\tAcc@5  99.22 ( 99.94)\n","Epoch: [104][210/391]\tTime  0.093 ( 0.096)\tLoss 3.2298e-02 (4.6291e-02)\tAcc@1  99.22 ( 98.99)\tAcc@5 100.00 ( 99.93)\n","Epoch: [104][240/391]\tTime  0.102 ( 0.095)\tLoss 7.1098e-02 (4.6366e-02)\tAcc@1  97.66 ( 98.96)\tAcc@5 100.00 ( 99.93)\n","Epoch: [104][270/391]\tTime  0.094 ( 0.095)\tLoss 4.0043e-02 (4.6178e-02)\tAcc@1  99.22 ( 98.98)\tAcc@5 100.00 ( 99.93)\n","Epoch: [104][300/391]\tTime  0.098 ( 0.095)\tLoss 4.7856e-02 (4.6390e-02)\tAcc@1 100.00 ( 98.97)\tAcc@5 100.00 ( 99.92)\n","Epoch: [104][330/391]\tTime  0.089 ( 0.094)\tLoss 3.3388e-02 (4.6214e-02)\tAcc@1  98.44 ( 98.98)\tAcc@5 100.00 ( 99.92)\n","Epoch: [104][360/391]\tTime  0.090 ( 0.094)\tLoss 5.7731e-02 (4.6269e-02)\tAcc@1  98.44 ( 98.98)\tAcc@5 100.00 ( 99.92)\n","Epoch: [104][390/391]\tTime  0.082 ( 0.094)\tLoss 3.3322e-02 (4.6346e-02)\tAcc@1  98.75 ( 98.97)\tAcc@5 100.00 ( 99.92)\n","==> Train Accuracy: Acc@1 98.970 || Acc@5 99.924\n","==> Test Accuracy:  Acc@1 77.450 || Acc@5 93.800\n","==> 39.82 seconds to train this epoch\n","\n","\n","----- epoch: 105, lr: 0.004000000000000001 -----\n","Epoch: [105][  0/391]\tTime  0.267 ( 0.267)\tLoss 2.4407e-02 (2.4407e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [105][ 30/391]\tTime  0.107 ( 0.099)\tLoss 2.7555e-02 (3.8911e-02)\tAcc@1 100.00 ( 99.12)\tAcc@5 100.00 ( 99.97)\n","Epoch: [105][ 60/391]\tTime  0.097 ( 0.096)\tLoss 5.0329e-02 (4.3622e-02)\tAcc@1  98.44 ( 99.04)\tAcc@5 100.00 ( 99.96)\n","Epoch: [105][ 90/391]\tTime  0.094 ( 0.095)\tLoss 3.9694e-02 (4.2853e-02)\tAcc@1  98.44 ( 99.13)\tAcc@5 100.00 ( 99.94)\n","Epoch: [105][120/391]\tTime  0.092 ( 0.094)\tLoss 5.3757e-02 (4.4910e-02)\tAcc@1  98.44 ( 99.10)\tAcc@5 100.00 ( 99.93)\n","Epoch: [105][150/391]\tTime  0.091 ( 0.094)\tLoss 2.2274e-02 (4.3864e-02)\tAcc@1 100.00 ( 99.13)\tAcc@5 100.00 ( 99.92)\n","Epoch: [105][180/391]\tTime  0.090 ( 0.094)\tLoss 1.5707e-02 (4.4518e-02)\tAcc@1 100.00 ( 99.11)\tAcc@5 100.00 ( 99.93)\n","Epoch: [105][210/391]\tTime  0.095 ( 0.093)\tLoss 4.9062e-02 (4.4263e-02)\tAcc@1  98.44 ( 99.12)\tAcc@5 100.00 ( 99.93)\n","Epoch: [105][240/391]\tTime  0.087 ( 0.093)\tLoss 1.9563e-02 (4.3694e-02)\tAcc@1 100.00 ( 99.12)\tAcc@5 100.00 ( 99.93)\n","Epoch: [105][270/391]\tTime  0.093 ( 0.093)\tLoss 2.7671e-02 (4.3973e-02)\tAcc@1  99.22 ( 99.10)\tAcc@5 100.00 ( 99.93)\n","Epoch: [105][300/391]\tTime  0.091 ( 0.093)\tLoss 2.3526e-02 (4.4755e-02)\tAcc@1  99.22 ( 99.07)\tAcc@5 100.00 ( 99.92)\n","Epoch: [105][330/391]\tTime  0.096 ( 0.093)\tLoss 2.0115e-02 (4.4580e-02)\tAcc@1  99.22 ( 99.08)\tAcc@5 100.00 ( 99.92)\n","Epoch: [105][360/391]\tTime  0.091 ( 0.093)\tLoss 1.9773e-02 (4.4948e-02)\tAcc@1 100.00 ( 99.07)\tAcc@5 100.00 ( 99.92)\n","Epoch: [105][390/391]\tTime  0.082 ( 0.093)\tLoss 9.3219e-02 (4.4627e-02)\tAcc@1  97.50 ( 99.08)\tAcc@5 100.00 ( 99.91)\n","==> Train Accuracy: Acc@1 99.078 || Acc@5 99.912\n","==> Test Accuracy:  Acc@1 77.430 || Acc@5 93.860\n","==> 39.43 seconds to train this epoch\n","\n","\n","----- epoch: 106, lr: 0.004000000000000001 -----\n","Epoch: [106][  0/391]\tTime  0.291 ( 0.291)\tLoss 1.7255e-02 (1.7255e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [106][ 30/391]\tTime  0.095 ( 0.099)\tLoss 2.4141e-02 (3.6984e-02)\tAcc@1 100.00 ( 99.34)\tAcc@5 100.00 ( 99.95)\n","Epoch: [106][ 60/391]\tTime  0.094 ( 0.096)\tLoss 6.8182e-02 (3.9257e-02)\tAcc@1  99.22 ( 99.21)\tAcc@5  99.22 ( 99.94)\n","Epoch: [106][ 90/391]\tTime  0.095 ( 0.095)\tLoss 3.1857e-02 (4.1782e-02)\tAcc@1 100.00 ( 99.12)\tAcc@5 100.00 ( 99.93)\n","Epoch: [106][120/391]\tTime  0.090 ( 0.094)\tLoss 1.0899e-01 (4.2654e-02)\tAcc@1  96.09 ( 99.09)\tAcc@5  99.22 ( 99.92)\n","Epoch: [106][150/391]\tTime  0.089 ( 0.094)\tLoss 7.1157e-02 (4.4214e-02)\tAcc@1  97.66 ( 99.02)\tAcc@5 100.00 ( 99.91)\n","Epoch: [106][180/391]\tTime  0.091 ( 0.094)\tLoss 3.1862e-02 (4.2818e-02)\tAcc@1 100.00 ( 99.07)\tAcc@5 100.00 ( 99.93)\n","Epoch: [106][210/391]\tTime  0.088 ( 0.093)\tLoss 4.0034e-02 (4.2462e-02)\tAcc@1  98.44 ( 99.06)\tAcc@5 100.00 ( 99.92)\n","Epoch: [106][240/391]\tTime  0.091 ( 0.093)\tLoss 3.2801e-02 (4.1567e-02)\tAcc@1 100.00 ( 99.11)\tAcc@5 100.00 ( 99.93)\n","Epoch: [106][270/391]\tTime  0.100 ( 0.093)\tLoss 1.9297e-02 (4.1306e-02)\tAcc@1 100.00 ( 99.11)\tAcc@5 100.00 ( 99.93)\n","Epoch: [106][300/391]\tTime  0.092 ( 0.093)\tLoss 3.6014e-02 (4.1697e-02)\tAcc@1 100.00 ( 99.10)\tAcc@5 100.00 ( 99.92)\n","Epoch: [106][330/391]\tTime  0.092 ( 0.093)\tLoss 2.7575e-02 (4.1623e-02)\tAcc@1 100.00 ( 99.10)\tAcc@5 100.00 ( 99.92)\n","Epoch: [106][360/391]\tTime  0.094 ( 0.093)\tLoss 1.7416e-02 (4.1286e-02)\tAcc@1 100.00 ( 99.12)\tAcc@5 100.00 ( 99.93)\n","Epoch: [106][390/391]\tTime  0.082 ( 0.093)\tLoss 1.2076e-01 (4.1555e-02)\tAcc@1  97.50 ( 99.12)\tAcc@5 100.00 ( 99.93)\n","==> Train Accuracy: Acc@1 99.122 || Acc@5 99.926\n","==> Test Accuracy:  Acc@1 77.520 || Acc@5 93.930\n","==> 39.42 seconds to train this epoch\n","\n","\n","----- epoch: 107, lr: 0.004000000000000001 -----\n","Epoch: [107][  0/391]\tTime  0.297 ( 0.297)\tLoss 3.8946e-02 (3.8946e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [107][ 30/391]\tTime  0.094 ( 0.100)\tLoss 1.5452e-02 (4.3900e-02)\tAcc@1 100.00 ( 98.97)\tAcc@5 100.00 ( 99.85)\n","Epoch: [107][ 60/391]\tTime  0.093 ( 0.096)\tLoss 4.0519e-02 (4.1256e-02)\tAcc@1  98.44 ( 99.09)\tAcc@5 100.00 ( 99.90)\n","Epoch: [107][ 90/391]\tTime  0.089 ( 0.095)\tLoss 7.4965e-02 (4.2704e-02)\tAcc@1  98.44 ( 99.07)\tAcc@5 100.00 ( 99.91)\n","Epoch: [107][120/391]\tTime  0.090 ( 0.095)\tLoss 4.6734e-02 (4.3965e-02)\tAcc@1  99.22 ( 99.06)\tAcc@5 100.00 ( 99.88)\n","Epoch: [107][150/391]\tTime  0.094 ( 0.094)\tLoss 1.7639e-02 (4.6426e-02)\tAcc@1  99.22 ( 98.94)\tAcc@5 100.00 ( 99.87)\n","Epoch: [107][180/391]\tTime  0.097 ( 0.094)\tLoss 3.6744e-02 (4.6397e-02)\tAcc@1  99.22 ( 98.95)\tAcc@5 100.00 ( 99.88)\n","Epoch: [107][210/391]\tTime  0.092 ( 0.094)\tLoss 8.4638e-02 (4.6553e-02)\tAcc@1  97.66 ( 98.94)\tAcc@5 100.00 ( 99.89)\n","Epoch: [107][240/391]\tTime  0.089 ( 0.093)\tLoss 6.1021e-02 (4.6455e-02)\tAcc@1  98.44 ( 98.95)\tAcc@5 100.00 ( 99.89)\n","Epoch: [107][270/391]\tTime  0.091 ( 0.093)\tLoss 7.6433e-02 (4.6128e-02)\tAcc@1  98.44 ( 98.97)\tAcc@5 100.00 ( 99.89)\n","Epoch: [107][300/391]\tTime  0.088 ( 0.093)\tLoss 4.4936e-02 (4.6267e-02)\tAcc@1  99.22 ( 98.96)\tAcc@5 100.00 ( 99.89)\n","Epoch: [107][330/391]\tTime  0.097 ( 0.093)\tLoss 6.5064e-02 (4.5926e-02)\tAcc@1  98.44 ( 98.96)\tAcc@5 100.00 ( 99.90)\n","Epoch: [107][360/391]\tTime  0.086 ( 0.093)\tLoss 7.7452e-02 (4.6282e-02)\tAcc@1  98.44 ( 98.97)\tAcc@5  99.22 ( 99.89)\n","Epoch: [107][390/391]\tTime  0.080 ( 0.093)\tLoss 7.4820e-02 (4.6123e-02)\tAcc@1  97.50 ( 98.97)\tAcc@5  98.75 ( 99.90)\n","==> Train Accuracy: Acc@1 98.972 || Acc@5 99.898\n","==> Test Accuracy:  Acc@1 77.600 || Acc@5 93.900\n","==> 39.41 seconds to train this epoch\n","\n","\n","----- epoch: 108, lr: 0.004000000000000001 -----\n","Epoch: [108][  0/391]\tTime  0.285 ( 0.285)\tLoss 6.1091e-02 (6.1091e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5  99.22 ( 99.22)\n","Epoch: [108][ 30/391]\tTime  0.089 ( 0.098)\tLoss 1.3112e-02 (3.9278e-02)\tAcc@1 100.00 ( 99.32)\tAcc@5 100.00 ( 99.92)\n","Epoch: [108][ 60/391]\tTime  0.092 ( 0.095)\tLoss 4.3208e-02 (4.1343e-02)\tAcc@1  98.44 ( 99.12)\tAcc@5 100.00 ( 99.94)\n","Epoch: [108][ 90/391]\tTime  0.082 ( 0.095)\tLoss 1.7135e-02 (4.0607e-02)\tAcc@1 100.00 ( 99.14)\tAcc@5 100.00 ( 99.96)\n","Epoch: [108][120/391]\tTime  0.088 ( 0.094)\tLoss 2.1167e-02 (4.2034e-02)\tAcc@1 100.00 ( 99.09)\tAcc@5 100.00 ( 99.94)\n","Epoch: [108][150/391]\tTime  0.093 ( 0.094)\tLoss 1.1224e-02 (4.2722e-02)\tAcc@1 100.00 ( 99.06)\tAcc@5 100.00 ( 99.93)\n","Epoch: [108][180/391]\tTime  0.094 ( 0.093)\tLoss 2.1794e-02 (4.2149e-02)\tAcc@1 100.00 ( 99.09)\tAcc@5 100.00 ( 99.93)\n","Epoch: [108][210/391]\tTime  0.088 ( 0.093)\tLoss 7.7904e-02 (4.2836e-02)\tAcc@1  98.44 ( 99.07)\tAcc@5 100.00 ( 99.92)\n","Epoch: [108][240/391]\tTime  0.095 ( 0.093)\tLoss 3.2943e-02 (4.3056e-02)\tAcc@1  99.22 ( 99.07)\tAcc@5 100.00 ( 99.92)\n","Epoch: [108][270/391]\tTime  0.090 ( 0.093)\tLoss 2.6713e-02 (4.2508e-02)\tAcc@1  99.22 ( 99.09)\tAcc@5 100.00 ( 99.93)\n","Epoch: [108][300/391]\tTime  0.100 ( 0.093)\tLoss 4.4231e-02 (4.3383e-02)\tAcc@1  99.22 ( 99.06)\tAcc@5 100.00 ( 99.92)\n","Epoch: [108][330/391]\tTime  0.091 ( 0.093)\tLoss 2.0072e-02 (4.2784e-02)\tAcc@1 100.00 ( 99.08)\tAcc@5 100.00 ( 99.93)\n","Epoch: [108][360/391]\tTime  0.090 ( 0.093)\tLoss 2.6349e-02 (4.2488e-02)\tAcc@1  99.22 ( 99.09)\tAcc@5 100.00 ( 99.93)\n","Epoch: [108][390/391]\tTime  0.082 ( 0.093)\tLoss 8.9717e-02 (4.2571e-02)\tAcc@1  96.25 ( 99.09)\tAcc@5 100.00 ( 99.93)\n","==> Train Accuracy: Acc@1 99.086 || Acc@5 99.928\n","==> Test Accuracy:  Acc@1 77.530 || Acc@5 93.920\n","==> 39.30 seconds to train this epoch\n","\n","\n","----- epoch: 109, lr: 0.004000000000000001 -----\n","Epoch: [109][  0/391]\tTime  0.260 ( 0.260)\tLoss 3.8787e-02 (3.8787e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [109][ 30/391]\tTime  0.092 ( 0.099)\tLoss 4.6677e-02 (4.4464e-02)\tAcc@1  98.44 ( 99.07)\tAcc@5 100.00 ( 99.85)\n","Epoch: [109][ 60/391]\tTime  0.098 ( 0.096)\tLoss 1.3681e-02 (4.5807e-02)\tAcc@1 100.00 ( 99.01)\tAcc@5 100.00 ( 99.87)\n","Epoch: [109][ 90/391]\tTime  0.092 ( 0.095)\tLoss 2.1101e-02 (4.6503e-02)\tAcc@1 100.00 ( 99.01)\tAcc@5 100.00 ( 99.87)\n","Epoch: [109][120/391]\tTime  0.093 ( 0.094)\tLoss 2.8409e-02 (4.6265e-02)\tAcc@1 100.00 ( 99.01)\tAcc@5 100.00 ( 99.86)\n","Epoch: [109][150/391]\tTime  0.093 ( 0.094)\tLoss 1.1100e-02 (4.6632e-02)\tAcc@1 100.00 ( 99.00)\tAcc@5 100.00 ( 99.87)\n","Epoch: [109][180/391]\tTime  0.094 ( 0.094)\tLoss 3.0522e-02 (4.6671e-02)\tAcc@1  99.22 ( 99.00)\tAcc@5 100.00 ( 99.87)\n","Epoch: [109][210/391]\tTime  0.095 ( 0.093)\tLoss 2.7650e-02 (4.6139e-02)\tAcc@1 100.00 ( 99.03)\tAcc@5 100.00 ( 99.87)\n","Epoch: [109][240/391]\tTime  0.092 ( 0.093)\tLoss 1.2667e-02 (4.4713e-02)\tAcc@1 100.00 ( 99.07)\tAcc@5 100.00 ( 99.88)\n","Epoch: [109][270/391]\tTime  0.093 ( 0.093)\tLoss 1.8982e-02 (4.4098e-02)\tAcc@1 100.00 ( 99.08)\tAcc@5 100.00 ( 99.89)\n","Epoch: [109][300/391]\tTime  0.094 ( 0.093)\tLoss 3.0146e-02 (4.4013e-02)\tAcc@1 100.00 ( 99.07)\tAcc@5 100.00 ( 99.89)\n","Epoch: [109][330/391]\tTime  0.087 ( 0.093)\tLoss 3.5113e-02 (4.3585e-02)\tAcc@1  99.22 ( 99.07)\tAcc@5 100.00 ( 99.90)\n","Epoch: [109][360/391]\tTime  0.112 ( 0.093)\tLoss 2.2223e-02 (4.4226e-02)\tAcc@1  99.22 ( 99.05)\tAcc@5 100.00 ( 99.90)\n","Epoch: [109][390/391]\tTime  0.082 ( 0.093)\tLoss 4.2968e-02 (4.4075e-02)\tAcc@1 100.00 ( 99.05)\tAcc@5 100.00 ( 99.90)\n","==> Train Accuracy: Acc@1 99.050 || Acc@5 99.898\n","==> Test Accuracy:  Acc@1 77.470 || Acc@5 93.790\n","==> 39.48 seconds to train this epoch\n","\n","\n","----- epoch: 110, lr: 0.004000000000000001 -----\n","Epoch: [110][  0/391]\tTime  0.307 ( 0.307)\tLoss 3.6486e-02 (3.6486e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [110][ 30/391]\tTime  0.093 ( 0.099)\tLoss 3.3054e-02 (4.4091e-02)\tAcc@1  99.22 ( 99.04)\tAcc@5 100.00 ( 99.92)\n","Epoch: [110][ 60/391]\tTime  0.091 ( 0.096)\tLoss 3.4683e-02 (4.2479e-02)\tAcc@1  98.44 ( 99.00)\tAcc@5 100.00 ( 99.92)\n","Epoch: [110][ 90/391]\tTime  0.088 ( 0.095)\tLoss 8.7168e-02 (4.2856e-02)\tAcc@1  97.66 ( 99.03)\tAcc@5 100.00 ( 99.93)\n","Epoch: [110][120/391]\tTime  0.092 ( 0.094)\tLoss 3.9262e-02 (4.2339e-02)\tAcc@1  99.22 ( 99.04)\tAcc@5 100.00 ( 99.92)\n","Epoch: [110][150/391]\tTime  0.095 ( 0.094)\tLoss 2.9719e-02 (4.1768e-02)\tAcc@1  99.22 ( 99.05)\tAcc@5 100.00 ( 99.92)\n","Epoch: [110][180/391]\tTime  0.097 ( 0.094)\tLoss 3.7637e-02 (4.2214e-02)\tAcc@1 100.00 ( 99.06)\tAcc@5 100.00 ( 99.91)\n","Epoch: [110][210/391]\tTime  0.089 ( 0.093)\tLoss 5.4353e-02 (4.3310e-02)\tAcc@1  98.44 ( 99.02)\tAcc@5 100.00 ( 99.91)\n","Epoch: [110][240/391]\tTime  0.092 ( 0.093)\tLoss 4.3936e-02 (4.3623e-02)\tAcc@1 100.00 ( 98.99)\tAcc@5 100.00 ( 99.91)\n","Epoch: [110][270/391]\tTime  0.091 ( 0.093)\tLoss 1.9159e-02 (4.4251e-02)\tAcc@1 100.00 ( 98.98)\tAcc@5 100.00 ( 99.91)\n","Epoch: [110][300/391]\tTime  0.089 ( 0.093)\tLoss 3.5943e-02 (4.4704e-02)\tAcc@1  98.44 ( 98.98)\tAcc@5 100.00 ( 99.91)\n","Epoch: [110][330/391]\tTime  0.093 ( 0.093)\tLoss 9.2384e-02 (4.5036e-02)\tAcc@1  97.66 ( 98.97)\tAcc@5  99.22 ( 99.90)\n","Epoch: [110][360/391]\tTime  0.092 ( 0.093)\tLoss 1.4107e-02 (4.5058e-02)\tAcc@1 100.00 ( 98.99)\tAcc@5 100.00 ( 99.90)\n","Epoch: [110][390/391]\tTime  0.083 ( 0.093)\tLoss 2.6820e-02 (4.4689e-02)\tAcc@1 100.00 ( 99.00)\tAcc@5 100.00 ( 99.91)\n","==> Train Accuracy: Acc@1 98.996 || Acc@5 99.910\n","==> Test Accuracy:  Acc@1 77.450 || Acc@5 93.890\n","==> 39.41 seconds to train this epoch\n","\n","\n","----- epoch: 111, lr: 0.004000000000000001 -----\n","Epoch: [111][  0/391]\tTime  0.298 ( 0.298)\tLoss 3.0088e-02 (3.0088e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [111][ 30/391]\tTime  0.095 ( 0.099)\tLoss 4.8898e-02 (3.8754e-02)\tAcc@1  99.22 ( 99.29)\tAcc@5 100.00 ( 99.92)\n","Epoch: [111][ 60/391]\tTime  0.086 ( 0.096)\tLoss 5.6862e-02 (4.1547e-02)\tAcc@1  97.66 ( 99.13)\tAcc@5 100.00 ( 99.94)\n","Epoch: [111][ 90/391]\tTime  0.090 ( 0.095)\tLoss 1.1990e-01 (4.2690e-02)\tAcc@1  96.88 ( 99.09)\tAcc@5  99.22 ( 99.91)\n","Epoch: [111][120/391]\tTime  0.090 ( 0.094)\tLoss 3.0703e-02 (4.1426e-02)\tAcc@1  99.22 ( 99.12)\tAcc@5 100.00 ( 99.93)\n","Epoch: [111][150/391]\tTime  0.097 ( 0.094)\tLoss 7.2153e-02 (4.1688e-02)\tAcc@1  98.44 ( 99.11)\tAcc@5 100.00 ( 99.93)\n","Epoch: [111][180/391]\tTime  0.092 ( 0.094)\tLoss 2.7702e-02 (4.1674e-02)\tAcc@1  99.22 ( 99.10)\tAcc@5 100.00 ( 99.93)\n","Epoch: [111][210/391]\tTime  0.092 ( 0.093)\tLoss 9.7627e-02 (4.2834e-02)\tAcc@1  97.66 ( 99.07)\tAcc@5 100.00 ( 99.91)\n","Epoch: [111][240/391]\tTime  0.094 ( 0.093)\tLoss 2.7835e-02 (4.1907e-02)\tAcc@1  99.22 ( 99.08)\tAcc@5 100.00 ( 99.92)\n","Epoch: [111][270/391]\tTime  0.086 ( 0.093)\tLoss 4.7168e-02 (4.1822e-02)\tAcc@1  97.66 ( 99.10)\tAcc@5 100.00 ( 99.93)\n","Epoch: [111][300/391]\tTime  0.095 ( 0.093)\tLoss 5.2739e-02 (4.1868e-02)\tAcc@1  98.44 ( 99.10)\tAcc@5 100.00 ( 99.93)\n","Epoch: [111][330/391]\tTime  0.094 ( 0.093)\tLoss 1.6394e-02 (4.2191e-02)\tAcc@1 100.00 ( 99.08)\tAcc@5 100.00 ( 99.93)\n","Epoch: [111][360/391]\tTime  0.100 ( 0.093)\tLoss 2.5518e-02 (4.2383e-02)\tAcc@1 100.00 ( 99.08)\tAcc@5 100.00 ( 99.93)\n","Epoch: [111][390/391]\tTime  0.087 ( 0.093)\tLoss 3.3589e-02 (4.2005e-02)\tAcc@1  98.75 ( 99.09)\tAcc@5 100.00 ( 99.94)\n","==> Train Accuracy: Acc@1 99.094 || Acc@5 99.936\n","==> Test Accuracy:  Acc@1 77.470 || Acc@5 93.760\n","==> 39.37 seconds to train this epoch\n","\n","\n","----- epoch: 112, lr: 0.004000000000000001 -----\n","Epoch: [112][  0/391]\tTime  0.329 ( 0.329)\tLoss 7.5069e-02 (7.5069e-02)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n","Epoch: [112][ 30/391]\tTime  0.105 ( 0.100)\tLoss 4.0011e-02 (3.6250e-02)\tAcc@1  99.22 ( 99.24)\tAcc@5 100.00 (100.00)\n","Epoch: [112][ 60/391]\tTime  0.094 ( 0.096)\tLoss 2.2456e-02 (3.6210e-02)\tAcc@1 100.00 ( 99.22)\tAcc@5 100.00 ( 99.97)\n","Epoch: [112][ 90/391]\tTime  0.091 ( 0.095)\tLoss 4.2055e-02 (3.9117e-02)\tAcc@1  99.22 ( 99.10)\tAcc@5 100.00 ( 99.93)\n","Epoch: [112][120/391]\tTime  0.088 ( 0.094)\tLoss 6.0904e-02 (4.0102e-02)\tAcc@1  98.44 ( 99.10)\tAcc@5  99.22 ( 99.92)\n","Epoch: [112][150/391]\tTime  0.093 ( 0.094)\tLoss 2.4264e-02 (3.9363e-02)\tAcc@1  99.22 ( 99.14)\tAcc@5 100.00 ( 99.92)\n","Epoch: [112][180/391]\tTime  0.090 ( 0.094)\tLoss 3.5255e-02 (4.1093e-02)\tAcc@1  99.22 ( 99.11)\tAcc@5 100.00 ( 99.91)\n","Epoch: [112][210/391]\tTime  0.093 ( 0.094)\tLoss 2.8471e-02 (4.0114e-02)\tAcc@1  99.22 ( 99.13)\tAcc@5 100.00 ( 99.92)\n","Epoch: [112][240/391]\tTime  0.099 ( 0.093)\tLoss 2.2873e-02 (4.0512e-02)\tAcc@1 100.00 ( 99.13)\tAcc@5 100.00 ( 99.92)\n","Epoch: [112][270/391]\tTime  0.089 ( 0.093)\tLoss 3.1915e-02 (4.0388e-02)\tAcc@1 100.00 ( 99.15)\tAcc@5 100.00 ( 99.92)\n","Epoch: [112][300/391]\tTime  0.093 ( 0.093)\tLoss 2.4532e-02 (4.0265e-02)\tAcc@1  99.22 ( 99.15)\tAcc@5 100.00 ( 99.91)\n","Epoch: [112][330/391]\tTime  0.092 ( 0.093)\tLoss 1.3580e-02 (4.0070e-02)\tAcc@1 100.00 ( 99.17)\tAcc@5 100.00 ( 99.92)\n","Epoch: [112][360/391]\tTime  0.090 ( 0.093)\tLoss 7.5516e-02 (3.9855e-02)\tAcc@1  97.66 ( 99.19)\tAcc@5  98.44 ( 99.92)\n","Epoch: [112][390/391]\tTime  0.081 ( 0.093)\tLoss 2.2685e-02 (3.9994e-02)\tAcc@1 100.00 ( 99.17)\tAcc@5 100.00 ( 99.92)\n","==> Train Accuracy: Acc@1 99.166 || Acc@5 99.918\n","==> Test Accuracy:  Acc@1 77.380 || Acc@5 93.720\n","==> 39.38 seconds to train this epoch\n","\n","\n","----- epoch: 113, lr: 0.004000000000000001 -----\n","Epoch: [113][  0/391]\tTime  0.284 ( 0.284)\tLoss 2.9487e-02 (2.9487e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [113][ 30/391]\tTime  0.094 ( 0.099)\tLoss 9.7324e-02 (4.3411e-02)\tAcc@1  97.66 ( 99.14)\tAcc@5  99.22 ( 99.90)\n","Epoch: [113][ 60/391]\tTime  0.088 ( 0.096)\tLoss 6.1529e-02 (4.1123e-02)\tAcc@1  98.44 ( 99.21)\tAcc@5 100.00 ( 99.92)\n","Epoch: [113][ 90/391]\tTime  0.091 ( 0.094)\tLoss 3.8158e-02 (4.0331e-02)\tAcc@1  99.22 ( 99.14)\tAcc@5 100.00 ( 99.94)\n","Epoch: [113][120/391]\tTime  0.090 ( 0.094)\tLoss 3.1478e-02 (3.9625e-02)\tAcc@1  99.22 ( 99.17)\tAcc@5 100.00 ( 99.94)\n","Epoch: [113][150/391]\tTime  0.090 ( 0.094)\tLoss 2.2802e-02 (4.0634e-02)\tAcc@1 100.00 ( 99.17)\tAcc@5 100.00 ( 99.92)\n","Epoch: [113][180/391]\tTime  0.088 ( 0.094)\tLoss 2.1363e-02 (3.9097e-02)\tAcc@1 100.00 ( 99.22)\tAcc@5 100.00 ( 99.93)\n","Epoch: [113][210/391]\tTime  0.093 ( 0.094)\tLoss 9.9299e-02 (3.8628e-02)\tAcc@1  97.66 ( 99.24)\tAcc@5  99.22 ( 99.93)\n","Epoch: [113][240/391]\tTime  0.092 ( 0.093)\tLoss 1.7523e-02 (3.9449e-02)\tAcc@1 100.00 ( 99.22)\tAcc@5 100.00 ( 99.92)\n","Epoch: [113][270/391]\tTime  0.093 ( 0.093)\tLoss 4.8545e-02 (3.9610e-02)\tAcc@1  98.44 ( 99.20)\tAcc@5 100.00 ( 99.93)\n","Epoch: [113][300/391]\tTime  0.099 ( 0.093)\tLoss 4.7622e-02 (3.9880e-02)\tAcc@1  99.22 ( 99.19)\tAcc@5  99.22 ( 99.91)\n","Epoch: [113][330/391]\tTime  0.090 ( 0.093)\tLoss 2.2649e-02 (3.9948e-02)\tAcc@1 100.00 ( 99.19)\tAcc@5 100.00 ( 99.92)\n","Epoch: [113][360/391]\tTime  0.090 ( 0.093)\tLoss 2.5324e-02 (3.9683e-02)\tAcc@1 100.00 ( 99.21)\tAcc@5 100.00 ( 99.92)\n","Epoch: [113][390/391]\tTime  0.082 ( 0.093)\tLoss 1.1070e-01 (3.9934e-02)\tAcc@1  97.50 ( 99.20)\tAcc@5  98.75 ( 99.92)\n","==> Train Accuracy: Acc@1 99.204 || Acc@5 99.920\n","==> Test Accuracy:  Acc@1 76.920 || Acc@5 93.680\n","==> 39.39 seconds to train this epoch\n","\n","\n","----- epoch: 114, lr: 0.004000000000000001 -----\n","Epoch: [114][  0/391]\tTime  0.301 ( 0.301)\tLoss 4.8716e-02 (4.8716e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [114][ 30/391]\tTime  0.090 ( 0.099)\tLoss 3.7355e-02 (4.2691e-02)\tAcc@1  99.22 ( 99.12)\tAcc@5 100.00 ( 99.87)\n","Epoch: [114][ 60/391]\tTime  0.093 ( 0.096)\tLoss 2.5410e-02 (4.0262e-02)\tAcc@1  99.22 ( 99.15)\tAcc@5 100.00 ( 99.92)\n","Epoch: [114][ 90/391]\tTime  0.092 ( 0.095)\tLoss 2.3670e-02 (3.6741e-02)\tAcc@1 100.00 ( 99.29)\tAcc@5 100.00 ( 99.95)\n","Epoch: [114][120/391]\tTime  0.095 ( 0.094)\tLoss 2.4585e-02 (3.8040e-02)\tAcc@1 100.00 ( 99.28)\tAcc@5 100.00 ( 99.94)\n","Epoch: [114][150/391]\tTime  0.096 ( 0.094)\tLoss 4.0144e-02 (3.9141e-02)\tAcc@1  98.44 ( 99.22)\tAcc@5 100.00 ( 99.94)\n","Epoch: [114][180/391]\tTime  0.092 ( 0.094)\tLoss 3.2281e-02 (3.9550e-02)\tAcc@1  99.22 ( 99.18)\tAcc@5 100.00 ( 99.94)\n","Epoch: [114][210/391]\tTime  0.093 ( 0.093)\tLoss 3.7611e-02 (3.9070e-02)\tAcc@1  99.22 ( 99.19)\tAcc@5 100.00 ( 99.94)\n","Epoch: [114][240/391]\tTime  0.088 ( 0.093)\tLoss 1.3878e-02 (3.8487e-02)\tAcc@1 100.00 ( 99.20)\tAcc@5 100.00 ( 99.94)\n","Epoch: [114][270/391]\tTime  0.091 ( 0.093)\tLoss 4.0022e-02 (3.8197e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 ( 99.94)\n","Epoch: [114][300/391]\tTime  0.093 ( 0.093)\tLoss 2.5371e-02 (3.9105e-02)\tAcc@1  99.22 ( 99.19)\tAcc@5 100.00 ( 99.94)\n","Epoch: [114][330/391]\tTime  0.086 ( 0.093)\tLoss 1.8749e-02 (3.9335e-02)\tAcc@1 100.00 ( 99.19)\tAcc@5 100.00 ( 99.93)\n","Epoch: [114][360/391]\tTime  0.098 ( 0.093)\tLoss 3.0352e-02 (3.9601e-02)\tAcc@1  99.22 ( 99.18)\tAcc@5 100.00 ( 99.93)\n","Epoch: [114][390/391]\tTime  0.081 ( 0.093)\tLoss 2.7950e-02 (3.9433e-02)\tAcc@1  98.75 ( 99.19)\tAcc@5 100.00 ( 99.93)\n","==> Train Accuracy: Acc@1 99.188 || Acc@5 99.926\n","==> Test Accuracy:  Acc@1 77.670 || Acc@5 93.950\n","==> 39.35 seconds to train this epoch\n","\n","\n","----- epoch: 115, lr: 0.004000000000000001 -----\n","Epoch: [115][  0/391]\tTime  0.299 ( 0.299)\tLoss 8.5111e-02 (8.5111e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [115][ 30/391]\tTime  0.090 ( 0.099)\tLoss 7.5973e-02 (4.0854e-02)\tAcc@1  97.66 ( 99.22)\tAcc@5 100.00 ( 99.90)\n","Epoch: [115][ 60/391]\tTime  0.089 ( 0.096)\tLoss 5.5250e-02 (4.0182e-02)\tAcc@1  99.22 ( 99.19)\tAcc@5 100.00 ( 99.91)\n","Epoch: [115][ 90/391]\tTime  0.088 ( 0.095)\tLoss 3.0378e-02 (4.0129e-02)\tAcc@1 100.00 ( 99.20)\tAcc@5 100.00 ( 99.93)\n","Epoch: [115][120/391]\tTime  0.091 ( 0.094)\tLoss 6.0016e-02 (4.0257e-02)\tAcc@1  97.66 ( 99.19)\tAcc@5 100.00 ( 99.93)\n","Epoch: [115][150/391]\tTime  0.091 ( 0.094)\tLoss 4.7261e-02 (4.0206e-02)\tAcc@1  98.44 ( 99.17)\tAcc@5 100.00 ( 99.92)\n","Epoch: [115][180/391]\tTime  0.091 ( 0.094)\tLoss 1.3898e-02 (4.0390e-02)\tAcc@1 100.00 ( 99.18)\tAcc@5 100.00 ( 99.91)\n","Epoch: [115][210/391]\tTime  0.096 ( 0.093)\tLoss 1.0017e-01 (4.0603e-02)\tAcc@1  97.66 ( 99.15)\tAcc@5 100.00 ( 99.91)\n","Epoch: [115][240/391]\tTime  0.090 ( 0.093)\tLoss 2.7868e-02 (3.9443e-02)\tAcc@1 100.00 ( 99.19)\tAcc@5 100.00 ( 99.92)\n","Epoch: [115][270/391]\tTime  0.090 ( 0.093)\tLoss 7.0036e-02 (3.9939e-02)\tAcc@1  98.44 ( 99.19)\tAcc@5 100.00 ( 99.91)\n","Epoch: [115][300/391]\tTime  0.086 ( 0.093)\tLoss 6.2638e-02 (4.0244e-02)\tAcc@1  98.44 ( 99.19)\tAcc@5 100.00 ( 99.92)\n","Epoch: [115][330/391]\tTime  0.089 ( 0.093)\tLoss 1.5911e-02 (4.0231e-02)\tAcc@1 100.00 ( 99.18)\tAcc@5 100.00 ( 99.92)\n","Epoch: [115][360/391]\tTime  0.091 ( 0.093)\tLoss 6.9644e-02 (4.1081e-02)\tAcc@1  98.44 ( 99.16)\tAcc@5 100.00 ( 99.91)\n","Epoch: [115][390/391]\tTime  0.081 ( 0.093)\tLoss 3.8330e-02 (4.1247e-02)\tAcc@1 100.00 ( 99.16)\tAcc@5 100.00 ( 99.91)\n","==> Train Accuracy: Acc@1 99.160 || Acc@5 99.912\n","==> Test Accuracy:  Acc@1 77.380 || Acc@5 93.740\n","==> 39.33 seconds to train this epoch\n","\n","\n","----- epoch: 116, lr: 0.004000000000000001 -----\n","Epoch: [116][  0/391]\tTime  0.249 ( 0.249)\tLoss 6.1031e-02 (6.1031e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [116][ 30/391]\tTime  0.090 ( 0.098)\tLoss 2.7715e-02 (4.3332e-02)\tAcc@1  99.22 ( 99.07)\tAcc@5 100.00 ( 99.95)\n","Epoch: [116][ 60/391]\tTime  0.096 ( 0.096)\tLoss 1.9077e-02 (4.3123e-02)\tAcc@1 100.00 ( 99.09)\tAcc@5 100.00 ( 99.94)\n","Epoch: [116][ 90/391]\tTime  0.090 ( 0.095)\tLoss 8.3341e-02 (4.4272e-02)\tAcc@1  95.31 ( 99.06)\tAcc@5 100.00 ( 99.91)\n","Epoch: [116][120/391]\tTime  0.089 ( 0.094)\tLoss 4.1483e-02 (4.3418e-02)\tAcc@1  99.22 ( 99.06)\tAcc@5 100.00 ( 99.92)\n","Epoch: [116][150/391]\tTime  0.095 ( 0.094)\tLoss 9.2059e-02 (4.3157e-02)\tAcc@1  97.66 ( 99.06)\tAcc@5 100.00 ( 99.92)\n","Epoch: [116][180/391]\tTime  0.091 ( 0.093)\tLoss 4.9903e-02 (4.2850e-02)\tAcc@1  97.66 ( 99.08)\tAcc@5 100.00 ( 99.92)\n","Epoch: [116][210/391]\tTime  0.098 ( 0.093)\tLoss 2.7346e-02 (4.2896e-02)\tAcc@1 100.00 ( 99.07)\tAcc@5 100.00 ( 99.93)\n","Epoch: [116][240/391]\tTime  0.091 ( 0.093)\tLoss 2.7482e-02 (4.2586e-02)\tAcc@1  99.22 ( 99.08)\tAcc@5 100.00 ( 99.93)\n","Epoch: [116][270/391]\tTime  0.091 ( 0.093)\tLoss 5.8256e-02 (4.2070e-02)\tAcc@1  96.88 ( 99.09)\tAcc@5 100.00 ( 99.94)\n","Epoch: [116][300/391]\tTime  0.091 ( 0.093)\tLoss 7.6053e-02 (4.2323e-02)\tAcc@1  98.44 ( 99.08)\tAcc@5 100.00 ( 99.94)\n","Epoch: [116][330/391]\tTime  0.091 ( 0.093)\tLoss 3.7082e-02 (4.2353e-02)\tAcc@1 100.00 ( 99.08)\tAcc@5 100.00 ( 99.94)\n","Epoch: [116][360/391]\tTime  0.092 ( 0.093)\tLoss 6.2596e-02 (4.2657e-02)\tAcc@1  98.44 ( 99.07)\tAcc@5 100.00 ( 99.94)\n","Epoch: [116][390/391]\tTime  0.082 ( 0.093)\tLoss 5.2999e-02 (4.2331e-02)\tAcc@1  98.75 ( 99.09)\tAcc@5 100.00 ( 99.94)\n","==> Train Accuracy: Acc@1 99.088 || Acc@5 99.938\n","==> Test Accuracy:  Acc@1 77.520 || Acc@5 93.690\n","==> 39.28 seconds to train this epoch\n","\n","\n","----- epoch: 117, lr: 0.004000000000000001 -----\n","Epoch: [117][  0/391]\tTime  0.306 ( 0.306)\tLoss 5.0930e-02 (5.0930e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [117][ 30/391]\tTime  0.089 ( 0.099)\tLoss 1.6873e-02 (4.1537e-02)\tAcc@1 100.00 ( 99.14)\tAcc@5 100.00 ( 99.95)\n","Epoch: [117][ 60/391]\tTime  0.089 ( 0.096)\tLoss 1.4213e-02 (3.9464e-02)\tAcc@1 100.00 ( 99.24)\tAcc@5 100.00 ( 99.95)\n","Epoch: [117][ 90/391]\tTime  0.089 ( 0.095)\tLoss 5.6779e-02 (3.8390e-02)\tAcc@1  99.22 ( 99.25)\tAcc@5  99.22 ( 99.95)\n","Epoch: [117][120/391]\tTime  0.097 ( 0.094)\tLoss 2.7783e-02 (3.8272e-02)\tAcc@1 100.00 ( 99.24)\tAcc@5 100.00 ( 99.95)\n","Epoch: [117][150/391]\tTime  0.092 ( 0.094)\tLoss 3.4075e-02 (3.7986e-02)\tAcc@1  98.44 ( 99.24)\tAcc@5 100.00 ( 99.95)\n","Epoch: [117][180/391]\tTime  0.093 ( 0.094)\tLoss 3.4225e-02 (3.8052e-02)\tAcc@1  99.22 ( 99.23)\tAcc@5 100.00 ( 99.95)\n","Epoch: [117][210/391]\tTime  0.097 ( 0.093)\tLoss 4.5218e-02 (3.8422e-02)\tAcc@1  98.44 ( 99.22)\tAcc@5 100.00 ( 99.95)\n","Epoch: [117][240/391]\tTime  0.091 ( 0.093)\tLoss 4.0611e-02 (3.8343e-02)\tAcc@1  99.22 ( 99.20)\tAcc@5 100.00 ( 99.95)\n","Epoch: [117][270/391]\tTime  0.092 ( 0.093)\tLoss 2.2552e-02 (3.8860e-02)\tAcc@1 100.00 ( 99.19)\tAcc@5 100.00 ( 99.95)\n","Epoch: [117][300/391]\tTime  0.094 ( 0.093)\tLoss 2.3806e-02 (3.8844e-02)\tAcc@1 100.00 ( 99.21)\tAcc@5 100.00 ( 99.95)\n","Epoch: [117][330/391]\tTime  0.093 ( 0.093)\tLoss 2.9804e-02 (3.9111e-02)\tAcc@1 100.00 ( 99.19)\tAcc@5 100.00 ( 99.94)\n","Epoch: [117][360/391]\tTime  0.089 ( 0.093)\tLoss 3.5478e-02 (3.9113e-02)\tAcc@1  98.44 ( 99.19)\tAcc@5 100.00 ( 99.95)\n","Epoch: [117][390/391]\tTime  0.082 ( 0.093)\tLoss 2.8513e-02 (3.8765e-02)\tAcc@1  98.75 ( 99.21)\tAcc@5 100.00 ( 99.95)\n","==> Train Accuracy: Acc@1 99.210 || Acc@5 99.948\n","==> Test Accuracy:  Acc@1 77.620 || Acc@5 93.710\n","==> 39.38 seconds to train this epoch\n","\n","\n","----- epoch: 118, lr: 0.004000000000000001 -----\n","Epoch: [118][  0/391]\tTime  0.266 ( 0.266)\tLoss 7.6189e-03 (7.6189e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [118][ 30/391]\tTime  0.085 ( 0.099)\tLoss 4.6373e-02 (4.4128e-02)\tAcc@1  98.44 ( 99.02)\tAcc@5 100.00 ( 99.82)\n","Epoch: [118][ 60/391]\tTime  0.102 ( 0.096)\tLoss 1.1840e-02 (4.0746e-02)\tAcc@1 100.00 ( 99.09)\tAcc@5 100.00 ( 99.90)\n","Epoch: [118][ 90/391]\tTime  0.090 ( 0.094)\tLoss 1.2560e-01 (4.0266e-02)\tAcc@1  97.66 ( 99.10)\tAcc@5  99.22 ( 99.91)\n","Epoch: [118][120/391]\tTime  0.091 ( 0.094)\tLoss 4.1188e-02 (3.8812e-02)\tAcc@1  98.44 ( 99.13)\tAcc@5 100.00 ( 99.93)\n","Epoch: [118][150/391]\tTime  0.089 ( 0.094)\tLoss 1.4620e-01 (4.0398e-02)\tAcc@1  95.31 ( 99.10)\tAcc@5  99.22 ( 99.92)\n","Epoch: [118][180/391]\tTime  0.093 ( 0.093)\tLoss 5.1346e-02 (4.0950e-02)\tAcc@1  99.22 ( 99.08)\tAcc@5 100.00 ( 99.94)\n","Epoch: [118][210/391]\tTime  0.094 ( 0.093)\tLoss 5.0035e-02 (4.1274e-02)\tAcc@1  98.44 ( 99.10)\tAcc@5 100.00 ( 99.93)\n","Epoch: [118][240/391]\tTime  0.091 ( 0.093)\tLoss 3.4150e-02 (4.1118e-02)\tAcc@1 100.00 ( 99.11)\tAcc@5 100.00 ( 99.93)\n","Epoch: [118][270/391]\tTime  0.101 ( 0.093)\tLoss 4.6013e-02 (4.0756e-02)\tAcc@1  99.22 ( 99.12)\tAcc@5 100.00 ( 99.93)\n","Epoch: [118][300/391]\tTime  0.090 ( 0.093)\tLoss 4.0760e-02 (4.0845e-02)\tAcc@1  99.22 ( 99.12)\tAcc@5 100.00 ( 99.93)\n","Epoch: [118][330/391]\tTime  0.091 ( 0.093)\tLoss 4.5750e-02 (4.0843e-02)\tAcc@1  99.22 ( 99.13)\tAcc@5  99.22 ( 99.92)\n","Epoch: [118][360/391]\tTime  0.094 ( 0.093)\tLoss 4.3638e-02 (4.1003e-02)\tAcc@1  98.44 ( 99.12)\tAcc@5 100.00 ( 99.92)\n","Epoch: [118][390/391]\tTime  0.080 ( 0.093)\tLoss 3.8783e-02 (4.0794e-02)\tAcc@1  98.75 ( 99.13)\tAcc@5 100.00 ( 99.93)\n","==> Train Accuracy: Acc@1 99.128 || Acc@5 99.926\n","==> Test Accuracy:  Acc@1 77.360 || Acc@5 93.810\n","==> 39.30 seconds to train this epoch\n","\n","\n","----- epoch: 119, lr: 0.004000000000000001 -----\n","Epoch: [119][  0/391]\tTime  0.268 ( 0.268)\tLoss 2.0260e-02 (2.0260e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [119][ 30/391]\tTime  0.095 ( 0.099)\tLoss 1.8348e-02 (4.0801e-02)\tAcc@1 100.00 ( 99.12)\tAcc@5 100.00 ( 99.90)\n","Epoch: [119][ 60/391]\tTime  0.089 ( 0.096)\tLoss 1.9832e-02 (4.1657e-02)\tAcc@1 100.00 ( 99.14)\tAcc@5 100.00 ( 99.90)\n","Epoch: [119][ 90/391]\tTime  0.096 ( 0.095)\tLoss 3.5794e-02 (3.8127e-02)\tAcc@1  99.22 ( 99.25)\tAcc@5 100.00 ( 99.92)\n","Epoch: [119][120/391]\tTime  0.095 ( 0.094)\tLoss 3.3625e-02 (3.7272e-02)\tAcc@1  99.22 ( 99.25)\tAcc@5 100.00 ( 99.93)\n","Epoch: [119][150/391]\tTime  0.091 ( 0.094)\tLoss 1.6237e-02 (3.6952e-02)\tAcc@1 100.00 ( 99.25)\tAcc@5 100.00 ( 99.93)\n","Epoch: [119][180/391]\tTime  0.087 ( 0.093)\tLoss 3.1223e-02 (3.7295e-02)\tAcc@1 100.00 ( 99.26)\tAcc@5 100.00 ( 99.93)\n","Epoch: [119][210/391]\tTime  0.087 ( 0.093)\tLoss 4.3433e-02 (3.8307e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 ( 99.93)\n","Epoch: [119][240/391]\tTime  0.091 ( 0.093)\tLoss 2.0779e-02 (3.9032e-02)\tAcc@1 100.00 ( 99.18)\tAcc@5 100.00 ( 99.92)\n","Epoch: [119][270/391]\tTime  0.099 ( 0.093)\tLoss 3.1201e-02 (3.8926e-02)\tAcc@1 100.00 ( 99.20)\tAcc@5 100.00 ( 99.93)\n","Epoch: [119][300/391]\tTime  0.093 ( 0.093)\tLoss 2.2173e-02 (3.8858e-02)\tAcc@1 100.00 ( 99.20)\tAcc@5 100.00 ( 99.93)\n","Epoch: [119][330/391]\tTime  0.099 ( 0.093)\tLoss 4.2708e-02 (3.8779e-02)\tAcc@1  98.44 ( 99.19)\tAcc@5 100.00 ( 99.93)\n","Epoch: [119][360/391]\tTime  0.092 ( 0.093)\tLoss 1.8924e-02 (3.8096e-02)\tAcc@1 100.00 ( 99.21)\tAcc@5 100.00 ( 99.94)\n","Epoch: [119][390/391]\tTime  0.089 ( 0.093)\tLoss 1.9375e-02 (3.7924e-02)\tAcc@1 100.00 ( 99.21)\tAcc@5 100.00 ( 99.94)\n","==> Train Accuracy: Acc@1 99.214 || Acc@5 99.940\n","==> Test Accuracy:  Acc@1 77.550 || Acc@5 94.190\n","==> 39.41 seconds to train this epoch\n","\n","\n","----- epoch: 120, lr: 0.0008000000000000003 -----\n","Epoch: [120][  0/391]\tTime  0.280 ( 0.280)\tLoss 2.6368e-02 (2.6368e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [120][ 30/391]\tTime  0.088 ( 0.098)\tLoss 1.8027e-02 (3.9140e-02)\tAcc@1 100.00 ( 99.32)\tAcc@5 100.00 ( 99.80)\n","Epoch: [120][ 60/391]\tTime  0.087 ( 0.095)\tLoss 2.8646e-02 (3.7092e-02)\tAcc@1  99.22 ( 99.40)\tAcc@5 100.00 ( 99.82)\n","Epoch: [120][ 90/391]\tTime  0.094 ( 0.094)\tLoss 4.7271e-02 (3.8349e-02)\tAcc@1  99.22 ( 99.35)\tAcc@5 100.00 ( 99.86)\n","Epoch: [120][120/391]\tTime  0.092 ( 0.094)\tLoss 1.1317e-02 (3.7342e-02)\tAcc@1 100.00 ( 99.33)\tAcc@5 100.00 ( 99.88)\n","Epoch: [120][150/391]\tTime  0.093 ( 0.094)\tLoss 1.0911e-02 (3.5694e-02)\tAcc@1 100.00 ( 99.35)\tAcc@5 100.00 ( 99.91)\n","Epoch: [120][180/391]\tTime  0.097 ( 0.094)\tLoss 3.1864e-02 (3.6952e-02)\tAcc@1  98.44 ( 99.31)\tAcc@5 100.00 ( 99.91)\n","Epoch: [120][210/391]\tTime  0.089 ( 0.093)\tLoss 1.2412e-02 (3.7162e-02)\tAcc@1 100.00 ( 99.31)\tAcc@5 100.00 ( 99.90)\n","Epoch: [120][240/391]\tTime  0.088 ( 0.093)\tLoss 7.4794e-02 (3.6917e-02)\tAcc@1  98.44 ( 99.30)\tAcc@5  99.22 ( 99.90)\n","Epoch: [120][270/391]\tTime  0.099 ( 0.093)\tLoss 4.6418e-02 (3.5667e-02)\tAcc@1  98.44 ( 99.33)\tAcc@5 100.00 ( 99.91)\n","Epoch: [120][300/391]\tTime  0.092 ( 0.093)\tLoss 3.1202e-02 (3.5594e-02)\tAcc@1  99.22 ( 99.32)\tAcc@5 100.00 ( 99.91)\n","Epoch: [120][330/391]\tTime  0.090 ( 0.093)\tLoss 2.0888e-02 (3.4745e-02)\tAcc@1 100.00 ( 99.33)\tAcc@5 100.00 ( 99.92)\n","Epoch: [120][360/391]\tTime  0.088 ( 0.093)\tLoss 2.1813e-02 (3.4481e-02)\tAcc@1 100.00 ( 99.33)\tAcc@5 100.00 ( 99.91)\n","Epoch: [120][390/391]\tTime  0.096 ( 0.093)\tLoss 3.5693e-02 (3.4176e-02)\tAcc@1  98.75 ( 99.34)\tAcc@5 100.00 ( 99.91)\n","==> Train Accuracy: Acc@1 99.342 || Acc@5 99.912\n","==> Test Accuracy:  Acc@1 77.740 || Acc@5 94.170\n","==> 39.33 seconds to train this epoch\n","\n","\n","----- epoch: 121, lr: 0.0008000000000000003 -----\n","Epoch: [121][  0/391]\tTime  0.270 ( 0.270)\tLoss 3.9512e-02 (3.9512e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [121][ 30/391]\tTime  0.093 ( 0.099)\tLoss 7.0703e-02 (3.0588e-02)\tAcc@1  98.44 ( 99.47)\tAcc@5  99.22 ( 99.92)\n","Epoch: [121][ 60/391]\tTime  0.094 ( 0.096)\tLoss 9.2154e-03 (3.0741e-02)\tAcc@1 100.00 ( 99.42)\tAcc@5 100.00 ( 99.94)\n","Epoch: [121][ 90/391]\tTime  0.090 ( 0.095)\tLoss 1.9583e-02 (2.9782e-02)\tAcc@1 100.00 ( 99.46)\tAcc@5 100.00 ( 99.96)\n","Epoch: [121][120/391]\tTime  0.089 ( 0.094)\tLoss 2.0370e-02 (2.8960e-02)\tAcc@1 100.00 ( 99.49)\tAcc@5 100.00 ( 99.96)\n","Epoch: [121][150/391]\tTime  0.091 ( 0.094)\tLoss 1.8840e-02 (2.9713e-02)\tAcc@1 100.00 ( 99.46)\tAcc@5 100.00 ( 99.97)\n","Epoch: [121][180/391]\tTime  0.090 ( 0.094)\tLoss 1.6959e-02 (2.9773e-02)\tAcc@1 100.00 ( 99.44)\tAcc@5 100.00 ( 99.97)\n","Epoch: [121][210/391]\tTime  0.091 ( 0.094)\tLoss 2.1086e-02 (2.9605e-02)\tAcc@1  99.22 ( 99.46)\tAcc@5 100.00 ( 99.97)\n","Epoch: [121][240/391]\tTime  0.100 ( 0.093)\tLoss 3.7567e-02 (3.0027e-02)\tAcc@1  99.22 ( 99.44)\tAcc@5 100.00 ( 99.97)\n","Epoch: [121][270/391]\tTime  0.094 ( 0.093)\tLoss 1.9911e-02 (2.9728e-02)\tAcc@1 100.00 ( 99.44)\tAcc@5 100.00 ( 99.97)\n","Epoch: [121][300/391]\tTime  0.091 ( 0.093)\tLoss 6.6651e-02 (2.9891e-02)\tAcc@1  98.44 ( 99.44)\tAcc@5 100.00 ( 99.97)\n","Epoch: [121][330/391]\tTime  0.091 ( 0.093)\tLoss 7.0687e-02 (3.0080e-02)\tAcc@1  97.66 ( 99.43)\tAcc@5  99.22 ( 99.96)\n","Epoch: [121][360/391]\tTime  0.093 ( 0.093)\tLoss 2.4535e-02 (2.9991e-02)\tAcc@1  99.22 ( 99.44)\tAcc@5 100.00 ( 99.96)\n","Epoch: [121][390/391]\tTime  0.082 ( 0.093)\tLoss 1.3421e-02 (3.0529e-02)\tAcc@1 100.00 ( 99.42)\tAcc@5 100.00 ( 99.95)\n","==> Train Accuracy: Acc@1 99.418 || Acc@5 99.954\n","==> Test Accuracy:  Acc@1 78.090 || Acc@5 94.180\n","==> 39.40 seconds to train this epoch\n","\n","\n","----- epoch: 122, lr: 0.0008000000000000003 -----\n","Epoch: [122][  0/391]\tTime  0.306 ( 0.306)\tLoss 2.5493e-02 (2.5493e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [122][ 30/391]\tTime  0.091 ( 0.099)\tLoss 6.2525e-02 (3.5561e-02)\tAcc@1  97.66 ( 99.09)\tAcc@5 100.00 ( 99.95)\n","Epoch: [122][ 60/391]\tTime  0.102 ( 0.096)\tLoss 1.7624e-02 (3.3006e-02)\tAcc@1 100.00 ( 99.24)\tAcc@5 100.00 ( 99.95)\n","Epoch: [122][ 90/391]\tTime  0.095 ( 0.095)\tLoss 2.9102e-02 (3.0352e-02)\tAcc@1  98.44 ( 99.30)\tAcc@5 100.00 ( 99.96)\n","Epoch: [122][120/391]\tTime  0.112 ( 0.096)\tLoss 1.4082e-02 (2.9958e-02)\tAcc@1 100.00 ( 99.34)\tAcc@5 100.00 ( 99.95)\n","Epoch: [122][150/391]\tTime  0.096 ( 0.095)\tLoss 3.8794e-02 (3.0219e-02)\tAcc@1  98.44 ( 99.32)\tAcc@5 100.00 ( 99.96)\n","Epoch: [122][180/391]\tTime  0.091 ( 0.095)\tLoss 1.1579e-02 (2.9600e-02)\tAcc@1 100.00 ( 99.36)\tAcc@5 100.00 ( 99.97)\n","Epoch: [122][210/391]\tTime  0.092 ( 0.095)\tLoss 2.3521e-02 (2.9893e-02)\tAcc@1  99.22 ( 99.39)\tAcc@5 100.00 ( 99.96)\n","Epoch: [122][240/391]\tTime  0.095 ( 0.094)\tLoss 9.6101e-03 (3.0165e-02)\tAcc@1 100.00 ( 99.38)\tAcc@5 100.00 ( 99.95)\n","Epoch: [122][270/391]\tTime  0.093 ( 0.094)\tLoss 3.2301e-02 (3.0013e-02)\tAcc@1  99.22 ( 99.38)\tAcc@5 100.00 ( 99.96)\n","Epoch: [122][300/391]\tTime  0.106 ( 0.094)\tLoss 2.7058e-02 (2.9691e-02)\tAcc@1  99.22 ( 99.41)\tAcc@5 100.00 ( 99.96)\n","Epoch: [122][330/391]\tTime  0.090 ( 0.094)\tLoss 2.1350e-02 (2.9352e-02)\tAcc@1 100.00 ( 99.41)\tAcc@5 100.00 ( 99.95)\n","Epoch: [122][360/391]\tTime  0.088 ( 0.094)\tLoss 8.5842e-02 (2.9618e-02)\tAcc@1  98.44 ( 99.41)\tAcc@5  99.22 ( 99.95)\n","Epoch: [122][390/391]\tTime  0.082 ( 0.094)\tLoss 7.8699e-02 (2.9641e-02)\tAcc@1  98.75 ( 99.41)\tAcc@5  98.75 ( 99.95)\n","==> Train Accuracy: Acc@1 99.412 || Acc@5 99.950\n","==> Test Accuracy:  Acc@1 78.300 || Acc@5 94.220\n","==> 39.60 seconds to train this epoch\n","\n","\n","----- epoch: 123, lr: 0.0008000000000000003 -----\n","Epoch: [123][  0/391]\tTime  0.321 ( 0.321)\tLoss 1.2576e-02 (1.2576e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [123][ 30/391]\tTime  0.088 ( 0.100)\tLoss 5.5370e-02 (3.2544e-02)\tAcc@1  98.44 ( 99.24)\tAcc@5 100.00 ( 99.97)\n","Epoch: [123][ 60/391]\tTime  0.092 ( 0.096)\tLoss 5.7016e-02 (3.1548e-02)\tAcc@1  98.44 ( 99.37)\tAcc@5 100.00 ( 99.95)\n","Epoch: [123][ 90/391]\tTime  0.090 ( 0.096)\tLoss 4.0528e-02 (3.1561e-02)\tAcc@1  99.22 ( 99.34)\tAcc@5 100.00 ( 99.94)\n","Epoch: [123][120/391]\tTime  0.089 ( 0.098)\tLoss 2.0142e-02 (3.0213e-02)\tAcc@1 100.00 ( 99.39)\tAcc@5 100.00 ( 99.95)\n","Epoch: [123][150/391]\tTime  0.093 ( 0.097)\tLoss 3.2704e-02 (3.0335e-02)\tAcc@1  99.22 ( 99.37)\tAcc@5 100.00 ( 99.95)\n","Epoch: [123][180/391]\tTime  0.092 ( 0.096)\tLoss 1.9937e-02 (3.0225e-02)\tAcc@1  99.22 ( 99.37)\tAcc@5 100.00 ( 99.95)\n","Epoch: [123][210/391]\tTime  0.095 ( 0.096)\tLoss 4.2713e-02 (3.1412e-02)\tAcc@1  99.22 ( 99.33)\tAcc@5 100.00 ( 99.94)\n","Epoch: [123][240/391]\tTime  0.090 ( 0.095)\tLoss 1.8910e-02 (3.0865e-02)\tAcc@1 100.00 ( 99.34)\tAcc@5 100.00 ( 99.95)\n","Epoch: [123][270/391]\tTime  0.092 ( 0.095)\tLoss 2.1360e-02 (3.0273e-02)\tAcc@1  99.22 ( 99.36)\tAcc@5 100.00 ( 99.95)\n","Epoch: [123][300/391]\tTime  0.090 ( 0.095)\tLoss 7.4525e-03 (3.0022e-02)\tAcc@1 100.00 ( 99.38)\tAcc@5 100.00 ( 99.95)\n","Epoch: [123][330/391]\tTime  0.095 ( 0.095)\tLoss 2.4516e-02 (2.9551e-02)\tAcc@1 100.00 ( 99.39)\tAcc@5 100.00 ( 99.96)\n","Epoch: [123][360/391]\tTime  0.102 ( 0.095)\tLoss 1.3340e-02 (2.9449e-02)\tAcc@1 100.00 ( 99.39)\tAcc@5 100.00 ( 99.96)\n","Epoch: [123][390/391]\tTime  0.090 ( 0.094)\tLoss 1.2833e-02 (2.9206e-02)\tAcc@1 100.00 ( 99.40)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 99.402 || Acc@5 99.962\n","==> Test Accuracy:  Acc@1 78.270 || Acc@5 94.160\n","==> 40.10 seconds to train this epoch\n","\n","\n","----- epoch: 124, lr: 0.0008000000000000003 -----\n","Epoch: [124][  0/391]\tTime  0.316 ( 0.316)\tLoss 2.2142e-02 (2.2142e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [124][ 30/391]\tTime  0.095 ( 0.100)\tLoss 2.2935e-02 (2.8248e-02)\tAcc@1  99.22 ( 99.45)\tAcc@5 100.00 ( 99.92)\n","Epoch: [124][ 60/391]\tTime  0.096 ( 0.097)\tLoss 2.0382e-02 (2.6861e-02)\tAcc@1  99.22 ( 99.51)\tAcc@5 100.00 ( 99.96)\n","Epoch: [124][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.5089e-02 (2.6287e-02)\tAcc@1 100.00 ( 99.54)\tAcc@5 100.00 ( 99.95)\n","Epoch: [124][120/391]\tTime  0.092 ( 0.095)\tLoss 1.2831e-02 (2.5766e-02)\tAcc@1 100.00 ( 99.55)\tAcc@5 100.00 ( 99.95)\n","Epoch: [124][150/391]\tTime  0.087 ( 0.094)\tLoss 1.9516e-02 (2.6867e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 ( 99.96)\n","Epoch: [124][180/391]\tTime  0.091 ( 0.094)\tLoss 2.7148e-02 (2.6240e-02)\tAcc@1  99.22 ( 99.52)\tAcc@5 100.00 ( 99.96)\n","Epoch: [124][210/391]\tTime  0.097 ( 0.094)\tLoss 2.4289e-02 (2.6031e-02)\tAcc@1  99.22 ( 99.53)\tAcc@5 100.00 ( 99.96)\n","Epoch: [124][240/391]\tTime  0.087 ( 0.094)\tLoss 4.4182e-02 (2.7532e-02)\tAcc@1  99.22 ( 99.48)\tAcc@5 100.00 ( 99.95)\n","Epoch: [124][270/391]\tTime  0.090 ( 0.094)\tLoss 3.8195e-02 (2.7569e-02)\tAcc@1  99.22 ( 99.49)\tAcc@5 100.00 ( 99.95)\n","Epoch: [124][300/391]\tTime  0.095 ( 0.093)\tLoss 1.2945e-02 (2.7415e-02)\tAcc@1 100.00 ( 99.49)\tAcc@5 100.00 ( 99.96)\n","Epoch: [124][330/391]\tTime  0.089 ( 0.093)\tLoss 3.6983e-02 (2.7328e-02)\tAcc@1  99.22 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","Epoch: [124][360/391]\tTime  0.094 ( 0.093)\tLoss 1.7877e-02 (2.7497e-02)\tAcc@1 100.00 ( 99.47)\tAcc@5 100.00 ( 99.95)\n","Epoch: [124][390/391]\tTime  0.082 ( 0.093)\tLoss 5.9002e-02 (2.7219e-02)\tAcc@1  98.75 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 99.484 || Acc@5 99.956\n","==> Test Accuracy:  Acc@1 78.200 || Acc@5 94.070\n","==> 39.51 seconds to train this epoch\n","\n","\n","----- epoch: 125, lr: 0.0008000000000000003 -----\n","Epoch: [125][  0/391]\tTime  0.317 ( 0.317)\tLoss 1.5596e-02 (1.5596e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [125][ 30/391]\tTime  0.095 ( 0.100)\tLoss 1.0811e-02 (2.6605e-02)\tAcc@1 100.00 ( 99.55)\tAcc@5 100.00 ( 99.97)\n","Epoch: [125][ 60/391]\tTime  0.093 ( 0.096)\tLoss 2.2759e-02 (2.7246e-02)\tAcc@1  99.22 ( 99.46)\tAcc@5 100.00 ( 99.99)\n","Epoch: [125][ 90/391]\tTime  0.098 ( 0.095)\tLoss 2.0254e-02 (2.6290e-02)\tAcc@1  99.22 ( 99.49)\tAcc@5 100.00 ( 99.98)\n","Epoch: [125][120/391]\tTime  0.099 ( 0.095)\tLoss 2.8068e-02 (2.8135e-02)\tAcc@1  99.22 ( 99.42)\tAcc@5 100.00 ( 99.97)\n","Epoch: [125][150/391]\tTime  0.091 ( 0.094)\tLoss 7.4477e-02 (2.8855e-02)\tAcc@1  98.44 ( 99.38)\tAcc@5 100.00 ( 99.97)\n","Epoch: [125][180/391]\tTime  0.090 ( 0.094)\tLoss 1.5693e-02 (2.8362e-02)\tAcc@1 100.00 ( 99.40)\tAcc@5 100.00 ( 99.97)\n","Epoch: [125][210/391]\tTime  0.097 ( 0.094)\tLoss 2.4639e-02 (2.8801e-02)\tAcc@1  99.22 ( 99.40)\tAcc@5 100.00 ( 99.96)\n","Epoch: [125][240/391]\tTime  0.094 ( 0.093)\tLoss 4.6621e-02 (2.8376e-02)\tAcc@1  99.22 ( 99.43)\tAcc@5 100.00 ( 99.95)\n","Epoch: [125][270/391]\tTime  0.093 ( 0.093)\tLoss 3.0664e-02 (2.8501e-02)\tAcc@1  99.22 ( 99.42)\tAcc@5 100.00 ( 99.95)\n","Epoch: [125][300/391]\tTime  0.086 ( 0.093)\tLoss 4.8569e-02 (2.7711e-02)\tAcc@1  99.22 ( 99.47)\tAcc@5 100.00 ( 99.96)\n","Epoch: [125][330/391]\tTime  0.095 ( 0.093)\tLoss 2.0112e-02 (2.7592e-02)\tAcc@1  99.22 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","Epoch: [125][360/391]\tTime  0.086 ( 0.093)\tLoss 3.5223e-02 (2.7452e-02)\tAcc@1  98.44 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","Epoch: [125][390/391]\tTime  0.082 ( 0.093)\tLoss 1.8609e-02 (2.7331e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 99.480 || Acc@5 99.962\n","==> Test Accuracy:  Acc@1 78.210 || Acc@5 94.100\n","==> 39.36 seconds to train this epoch\n","\n","\n","----- epoch: 126, lr: 0.0008000000000000003 -----\n","Epoch: [126][  0/391]\tTime  0.281 ( 0.281)\tLoss 3.7827e-02 (3.7827e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [126][ 30/391]\tTime  0.091 ( 0.099)\tLoss 2.9132e-02 (2.7735e-02)\tAcc@1  99.22 ( 99.32)\tAcc@5 100.00 ( 99.95)\n","Epoch: [126][ 60/391]\tTime  0.088 ( 0.096)\tLoss 1.0647e-02 (2.5013e-02)\tAcc@1 100.00 ( 99.49)\tAcc@5 100.00 ( 99.97)\n","Epoch: [126][ 90/391]\tTime  0.090 ( 0.095)\tLoss 3.1641e-02 (2.6793e-02)\tAcc@1 100.00 ( 99.40)\tAcc@5 100.00 ( 99.97)\n","Epoch: [126][120/391]\tTime  0.090 ( 0.094)\tLoss 5.5141e-02 (2.7833e-02)\tAcc@1  99.22 ( 99.39)\tAcc@5 100.00 ( 99.97)\n","Epoch: [126][150/391]\tTime  0.097 ( 0.094)\tLoss 1.2900e-02 (2.7476e-02)\tAcc@1 100.00 ( 99.41)\tAcc@5 100.00 ( 99.97)\n","Epoch: [126][180/391]\tTime  0.100 ( 0.094)\tLoss 5.8010e-02 (2.8285e-02)\tAcc@1  98.44 ( 99.41)\tAcc@5  99.22 ( 99.97)\n","Epoch: [126][210/391]\tTime  0.097 ( 0.094)\tLoss 1.2339e-02 (2.8462e-02)\tAcc@1 100.00 ( 99.42)\tAcc@5 100.00 ( 99.96)\n","Epoch: [126][240/391]\tTime  0.092 ( 0.094)\tLoss 1.3502e-02 (2.8394e-02)\tAcc@1 100.00 ( 99.42)\tAcc@5 100.00 ( 99.96)\n","Epoch: [126][270/391]\tTime  0.089 ( 0.094)\tLoss 1.4605e-02 (2.8477e-02)\tAcc@1 100.00 ( 99.44)\tAcc@5 100.00 ( 99.96)\n","Epoch: [126][300/391]\tTime  0.095 ( 0.093)\tLoss 1.0850e-02 (2.8540e-02)\tAcc@1 100.00 ( 99.43)\tAcc@5 100.00 ( 99.96)\n","Epoch: [126][330/391]\tTime  0.100 ( 0.093)\tLoss 1.2456e-02 (2.8718e-02)\tAcc@1 100.00 ( 99.43)\tAcc@5 100.00 ( 99.96)\n","Epoch: [126][360/391]\tTime  0.092 ( 0.093)\tLoss 1.6167e-02 (2.8547e-02)\tAcc@1 100.00 ( 99.43)\tAcc@5 100.00 ( 99.96)\n","Epoch: [126][390/391]\tTime  0.081 ( 0.093)\tLoss 3.1480e-02 (2.8540e-02)\tAcc@1  98.75 ( 99.43)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 99.426 || Acc@5 99.956\n","==> Test Accuracy:  Acc@1 78.180 || Acc@5 94.240\n","==> 39.42 seconds to train this epoch\n","\n","\n","----- epoch: 127, lr: 0.0008000000000000003 -----\n","Epoch: [127][  0/391]\tTime  0.280 ( 0.280)\tLoss 1.5363e-02 (1.5363e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [127][ 30/391]\tTime  0.092 ( 0.099)\tLoss 1.5785e-02 (2.6147e-02)\tAcc@1  99.22 ( 99.37)\tAcc@5 100.00 (100.00)\n","Epoch: [127][ 60/391]\tTime  0.095 ( 0.096)\tLoss 1.6194e-02 (2.6619e-02)\tAcc@1  99.22 ( 99.40)\tAcc@5 100.00 ( 99.99)\n","Epoch: [127][ 90/391]\tTime  0.090 ( 0.095)\tLoss 1.5944e-02 (2.7698e-02)\tAcc@1 100.00 ( 99.41)\tAcc@5 100.00 ( 99.98)\n","Epoch: [127][120/391]\tTime  0.085 ( 0.094)\tLoss 9.8959e-03 (2.7248e-02)\tAcc@1 100.00 ( 99.41)\tAcc@5 100.00 ( 99.97)\n","Epoch: [127][150/391]\tTime  0.091 ( 0.094)\tLoss 1.0230e-02 (2.7111e-02)\tAcc@1 100.00 ( 99.44)\tAcc@5 100.00 ( 99.96)\n","Epoch: [127][180/391]\tTime  0.091 ( 0.094)\tLoss 4.0056e-02 (2.7216e-02)\tAcc@1  99.22 ( 99.43)\tAcc@5 100.00 ( 99.97)\n","Epoch: [127][210/391]\tTime  0.091 ( 0.094)\tLoss 3.3530e-02 (2.7298e-02)\tAcc@1  99.22 ( 99.42)\tAcc@5 100.00 ( 99.97)\n","Epoch: [127][240/391]\tTime  0.103 ( 0.093)\tLoss 1.5081e-02 (2.7479e-02)\tAcc@1 100.00 ( 99.42)\tAcc@5 100.00 ( 99.96)\n","Epoch: [127][270/391]\tTime  0.093 ( 0.093)\tLoss 1.5892e-02 (2.7551e-02)\tAcc@1 100.00 ( 99.43)\tAcc@5 100.00 ( 99.96)\n","Epoch: [127][300/391]\tTime  0.091 ( 0.093)\tLoss 1.1510e-02 (2.7504e-02)\tAcc@1 100.00 ( 99.43)\tAcc@5 100.00 ( 99.96)\n","Epoch: [127][330/391]\tTime  0.093 ( 0.093)\tLoss 8.5747e-03 (2.7674e-02)\tAcc@1 100.00 ( 99.42)\tAcc@5 100.00 ( 99.95)\n","Epoch: [127][360/391]\tTime  0.091 ( 0.093)\tLoss 3.3665e-02 (2.7882e-02)\tAcc@1  99.22 ( 99.41)\tAcc@5 100.00 ( 99.95)\n","Epoch: [127][390/391]\tTime  0.083 ( 0.093)\tLoss 2.8734e-02 (2.8097e-02)\tAcc@1 100.00 ( 99.41)\tAcc@5 100.00 ( 99.95)\n","==> Train Accuracy: Acc@1 99.414 || Acc@5 99.954\n","==> Test Accuracy:  Acc@1 78.400 || Acc@5 94.050\n","==> 39.38 seconds to train this epoch\n","\n","\n","----- epoch: 128, lr: 0.0008000000000000003 -----\n","Epoch: [128][  0/391]\tTime  0.295 ( 0.295)\tLoss 1.7175e-02 (1.7175e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [128][ 30/391]\tTime  0.088 ( 0.099)\tLoss 7.0442e-03 (2.4858e-02)\tAcc@1 100.00 ( 99.47)\tAcc@5 100.00 (100.00)\n","Epoch: [128][ 60/391]\tTime  0.104 ( 0.096)\tLoss 3.4059e-02 (2.6645e-02)\tAcc@1  99.22 ( 99.41)\tAcc@5 100.00 ( 99.99)\n","Epoch: [128][ 90/391]\tTime  0.092 ( 0.095)\tLoss 2.2778e-02 (2.7294e-02)\tAcc@1  99.22 ( 99.42)\tAcc@5 100.00 ( 99.97)\n","Epoch: [128][120/391]\tTime  0.103 ( 0.097)\tLoss 2.6134e-02 (2.7463e-02)\tAcc@1  99.22 ( 99.41)\tAcc@5 100.00 ( 99.97)\n","Epoch: [128][150/391]\tTime  0.102 ( 0.096)\tLoss 6.8850e-03 (2.6185e-02)\tAcc@1 100.00 ( 99.47)\tAcc@5 100.00 ( 99.97)\n","Epoch: [128][180/391]\tTime  0.092 ( 0.096)\tLoss 1.3703e-02 (2.5997e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.97)\n","Epoch: [128][210/391]\tTime  0.094 ( 0.095)\tLoss 2.6388e-02 (2.6801e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.97)\n","Epoch: [128][240/391]\tTime  0.092 ( 0.095)\tLoss 1.8961e-02 (2.7053e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","Epoch: [128][270/391]\tTime  0.091 ( 0.095)\tLoss 1.2032e-02 (2.7454e-02)\tAcc@1 100.00 ( 99.47)\tAcc@5 100.00 ( 99.96)\n","Epoch: [128][300/391]\tTime  0.101 ( 0.094)\tLoss 3.2815e-02 (2.7379e-02)\tAcc@1  99.22 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","Epoch: [128][330/391]\tTime  0.090 ( 0.094)\tLoss 7.6399e-03 (2.7090e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.97)\n","Epoch: [128][360/391]\tTime  0.091 ( 0.094)\tLoss 1.2642e-02 (2.6563e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 ( 99.97)\n","Epoch: [128][390/391]\tTime  0.081 ( 0.094)\tLoss 4.0042e-02 (2.6235e-02)\tAcc@1  98.75 ( 99.50)\tAcc@5 100.00 ( 99.97)\n","==> Train Accuracy: Acc@1 99.504 || Acc@5 99.970\n","==> Test Accuracy:  Acc@1 78.580 || Acc@5 94.170\n","==> 39.77 seconds to train this epoch\n","\n","\n","----- epoch: 129, lr: 0.0008000000000000003 -----\n","Epoch: [129][  0/391]\tTime  0.302 ( 0.302)\tLoss 2.5126e-02 (2.5126e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [129][ 30/391]\tTime  0.099 ( 0.100)\tLoss 4.7700e-02 (3.0956e-02)\tAcc@1  98.44 ( 99.37)\tAcc@5 100.00 ( 99.97)\n","Epoch: [129][ 60/391]\tTime  0.090 ( 0.096)\tLoss 2.3326e-02 (2.9091e-02)\tAcc@1 100.00 ( 99.44)\tAcc@5 100.00 ( 99.96)\n","Epoch: [129][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.1834e-02 (2.9755e-02)\tAcc@1 100.00 ( 99.42)\tAcc@5 100.00 ( 99.94)\n","Epoch: [129][120/391]\tTime  0.074 ( 0.096)\tLoss 2.8590e-02 (2.8318e-02)\tAcc@1  99.22 ( 99.44)\tAcc@5 100.00 ( 99.95)\n","Epoch: [129][150/391]\tTime  0.094 ( 0.096)\tLoss 1.8204e-02 (2.7496e-02)\tAcc@1 100.00 ( 99.47)\tAcc@5 100.00 ( 99.95)\n","Epoch: [129][180/391]\tTime  0.091 ( 0.095)\tLoss 1.8069e-02 (2.6841e-02)\tAcc@1 100.00 ( 99.51)\tAcc@5 100.00 ( 99.94)\n","Epoch: [129][210/391]\tTime  0.090 ( 0.095)\tLoss 1.2755e-02 (2.6616e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 ( 99.95)\n","Epoch: [129][240/391]\tTime  0.089 ( 0.094)\tLoss 1.2292e-02 (2.6346e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 ( 99.95)\n","Epoch: [129][270/391]\tTime  0.098 ( 0.094)\tLoss 1.7789e-02 (2.6226e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 ( 99.96)\n","Epoch: [129][300/391]\tTime  0.092 ( 0.094)\tLoss 1.9955e-02 (2.6649e-02)\tAcc@1 100.00 ( 99.49)\tAcc@5 100.00 ( 99.96)\n","Epoch: [129][330/391]\tTime  0.092 ( 0.094)\tLoss 5.1002e-02 (2.7204e-02)\tAcc@1  99.22 ( 99.47)\tAcc@5 100.00 ( 99.95)\n","Epoch: [129][360/391]\tTime  0.088 ( 0.094)\tLoss 1.7953e-02 (2.6981e-02)\tAcc@1  99.22 ( 99.47)\tAcc@5 100.00 ( 99.95)\n","Epoch: [129][390/391]\tTime  0.082 ( 0.094)\tLoss 8.3311e-03 (2.6805e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.95)\n","==> Train Accuracy: Acc@1 99.478 || Acc@5 99.950\n","==> Test Accuracy:  Acc@1 78.310 || Acc@5 94.110\n","==> 39.63 seconds to train this epoch\n","\n","\n","----- epoch: 130, lr: 0.0008000000000000003 -----\n","Epoch: [130][  0/391]\tTime  0.291 ( 0.291)\tLoss 3.1380e-02 (3.1380e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [130][ 30/391]\tTime  0.099 ( 0.098)\tLoss 2.0932e-02 (2.7624e-02)\tAcc@1 100.00 ( 99.42)\tAcc@5 100.00 ( 99.95)\n","Epoch: [130][ 60/391]\tTime  0.093 ( 0.096)\tLoss 6.0585e-02 (3.0470e-02)\tAcc@1  99.22 ( 99.32)\tAcc@5  99.22 ( 99.94)\n","Epoch: [130][ 90/391]\tTime  0.091 ( 0.095)\tLoss 1.2006e-02 (2.5884e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","Epoch: [130][120/391]\tTime  0.104 ( 0.094)\tLoss 2.3410e-02 (2.5495e-02)\tAcc@1 100.00 ( 99.51)\tAcc@5 100.00 ( 99.97)\n","Epoch: [130][150/391]\tTime  0.095 ( 0.094)\tLoss 4.7678e-02 (2.7161e-02)\tAcc@1  97.66 ( 99.44)\tAcc@5 100.00 ( 99.96)\n","Epoch: [130][180/391]\tTime  0.092 ( 0.094)\tLoss 4.9633e-02 (2.6732e-02)\tAcc@1  98.44 ( 99.47)\tAcc@5  99.22 ( 99.96)\n","Epoch: [130][210/391]\tTime  0.091 ( 0.094)\tLoss 1.8176e-02 (2.6905e-02)\tAcc@1 100.00 ( 99.47)\tAcc@5 100.00 ( 99.96)\n","Epoch: [130][240/391]\tTime  0.089 ( 0.094)\tLoss 1.3655e-02 (2.6932e-02)\tAcc@1 100.00 ( 99.47)\tAcc@5 100.00 ( 99.96)\n","Epoch: [130][270/391]\tTime  0.092 ( 0.093)\tLoss 1.3552e-02 (2.6884e-02)\tAcc@1 100.00 ( 99.47)\tAcc@5 100.00 ( 99.96)\n","Epoch: [130][300/391]\tTime  0.094 ( 0.093)\tLoss 3.8741e-02 (2.7081e-02)\tAcc@1  99.22 ( 99.45)\tAcc@5 100.00 ( 99.96)\n","Epoch: [130][330/391]\tTime  0.101 ( 0.093)\tLoss 2.9477e-02 (2.7207e-02)\tAcc@1  99.22 ( 99.45)\tAcc@5 100.00 ( 99.96)\n","Epoch: [130][360/391]\tTime  0.092 ( 0.093)\tLoss 2.4197e-02 (2.7208e-02)\tAcc@1 100.00 ( 99.45)\tAcc@5 100.00 ( 99.96)\n","Epoch: [130][390/391]\tTime  0.082 ( 0.093)\tLoss 2.5460e-02 (2.7330e-02)\tAcc@1 100.00 ( 99.44)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 99.436 || Acc@5 99.958\n","==> Test Accuracy:  Acc@1 78.410 || Acc@5 94.240\n","==> 39.43 seconds to train this epoch\n","\n","\n","----- epoch: 131, lr: 0.0008000000000000003 -----\n","Epoch: [131][  0/391]\tTime  0.297 ( 0.297)\tLoss 2.9014e-02 (2.9014e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [131][ 30/391]\tTime  0.094 ( 0.099)\tLoss 1.0486e-02 (2.9262e-02)\tAcc@1 100.00 ( 99.37)\tAcc@5 100.00 ( 99.92)\n","Epoch: [131][ 60/391]\tTime  0.096 ( 0.096)\tLoss 1.0194e-02 (2.5932e-02)\tAcc@1 100.00 ( 99.46)\tAcc@5 100.00 ( 99.95)\n","Epoch: [131][ 90/391]\tTime  0.090 ( 0.095)\tLoss 2.3649e-02 (2.6870e-02)\tAcc@1 100.00 ( 99.44)\tAcc@5 100.00 ( 99.97)\n","Epoch: [131][120/391]\tTime  0.094 ( 0.094)\tLoss 6.2294e-02 (2.7015e-02)\tAcc@1  99.22 ( 99.44)\tAcc@5 100.00 ( 99.97)\n","Epoch: [131][150/391]\tTime  0.090 ( 0.094)\tLoss 1.2490e-02 (2.6653e-02)\tAcc@1 100.00 ( 99.45)\tAcc@5 100.00 ( 99.97)\n","Epoch: [131][180/391]\tTime  0.100 ( 0.094)\tLoss 2.3677e-02 (2.6371e-02)\tAcc@1  99.22 ( 99.47)\tAcc@5 100.00 ( 99.97)\n","Epoch: [131][210/391]\tTime  0.093 ( 0.093)\tLoss 2.0914e-02 (2.6317e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","Epoch: [131][240/391]\tTime  0.092 ( 0.093)\tLoss 1.9015e-02 (2.7382e-02)\tAcc@1 100.00 ( 99.45)\tAcc@5 100.00 ( 99.95)\n","Epoch: [131][270/391]\tTime  0.092 ( 0.093)\tLoss 3.6601e-02 (2.7730e-02)\tAcc@1  98.44 ( 99.42)\tAcc@5 100.00 ( 99.95)\n","Epoch: [131][300/391]\tTime  0.104 ( 0.093)\tLoss 3.3375e-02 (2.7582e-02)\tAcc@1  99.22 ( 99.43)\tAcc@5 100.00 ( 99.96)\n","Epoch: [131][330/391]\tTime  0.088 ( 0.093)\tLoss 5.5273e-02 (2.7712e-02)\tAcc@1  98.44 ( 99.42)\tAcc@5 100.00 ( 99.96)\n","Epoch: [131][360/391]\tTime  0.092 ( 0.093)\tLoss 2.6119e-02 (2.7138e-02)\tAcc@1  99.22 ( 99.44)\tAcc@5 100.00 ( 99.96)\n","Epoch: [131][390/391]\tTime  0.081 ( 0.093)\tLoss 8.2850e-02 (2.7096e-02)\tAcc@1  98.75 ( 99.44)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 99.444 || Acc@5 99.960\n","==> Test Accuracy:  Acc@1 78.390 || Acc@5 94.190\n","==> 39.43 seconds to train this epoch\n","\n","\n","----- epoch: 132, lr: 0.0008000000000000003 -----\n","Epoch: [132][  0/391]\tTime  0.266 ( 0.266)\tLoss 5.1300e-02 (5.1300e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [132][ 30/391]\tTime  0.091 ( 0.099)\tLoss 2.4875e-02 (2.3300e-02)\tAcc@1  99.22 ( 99.55)\tAcc@5 100.00 (100.00)\n","Epoch: [132][ 60/391]\tTime  0.093 ( 0.096)\tLoss 2.7059e-02 (2.2492e-02)\tAcc@1  99.22 ( 99.60)\tAcc@5 100.00 ( 99.97)\n","Epoch: [132][ 90/391]\tTime  0.101 ( 0.095)\tLoss 1.1971e-02 (2.3281e-02)\tAcc@1 100.00 ( 99.58)\tAcc@5 100.00 ( 99.97)\n","Epoch: [132][120/391]\tTime  0.092 ( 0.094)\tLoss 2.1211e-02 (2.3805e-02)\tAcc@1  99.22 ( 99.55)\tAcc@5 100.00 ( 99.96)\n","Epoch: [132][150/391]\tTime  0.094 ( 0.094)\tLoss 2.1532e-02 (2.3103e-02)\tAcc@1 100.00 ( 99.58)\tAcc@5 100.00 ( 99.96)\n","Epoch: [132][180/391]\tTime  0.096 ( 0.094)\tLoss 2.2940e-02 (2.3662e-02)\tAcc@1  99.22 ( 99.56)\tAcc@5 100.00 ( 99.97)\n","Epoch: [132][210/391]\tTime  0.097 ( 0.094)\tLoss 4.2699e-02 (2.3955e-02)\tAcc@1  99.22 ( 99.56)\tAcc@5 100.00 ( 99.97)\n","Epoch: [132][240/391]\tTime  0.096 ( 0.094)\tLoss 9.2273e-03 (2.4347e-02)\tAcc@1 100.00 ( 99.56)\tAcc@5 100.00 ( 99.96)\n","Epoch: [132][270/391]\tTime  0.091 ( 0.094)\tLoss 1.2557e-02 (2.4204e-02)\tAcc@1 100.00 ( 99.56)\tAcc@5 100.00 ( 99.96)\n","Epoch: [132][300/391]\tTime  0.090 ( 0.094)\tLoss 5.6880e-02 (2.5013e-02)\tAcc@1  98.44 ( 99.52)\tAcc@5 100.00 ( 99.96)\n","Epoch: [132][330/391]\tTime  0.094 ( 0.093)\tLoss 1.5733e-02 (2.4985e-02)\tAcc@1 100.00 ( 99.51)\tAcc@5 100.00 ( 99.96)\n","Epoch: [132][360/391]\tTime  0.090 ( 0.093)\tLoss 6.4817e-02 (2.5075e-02)\tAcc@1  98.44 ( 99.50)\tAcc@5  99.22 ( 99.96)\n","Epoch: [132][390/391]\tTime  0.080 ( 0.093)\tLoss 8.6734e-02 (2.5230e-02)\tAcc@1  96.25 ( 99.49)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 99.490 || Acc@5 99.958\n","==> Test Accuracy:  Acc@1 78.480 || Acc@5 94.160\n","==> 39.51 seconds to train this epoch\n","\n","\n","----- epoch: 133, lr: 0.0008000000000000003 -----\n","Epoch: [133][  0/391]\tTime  0.308 ( 0.308)\tLoss 2.6267e-02 (2.6267e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [133][ 30/391]\tTime  0.102 ( 0.099)\tLoss 1.0125e-02 (2.4387e-02)\tAcc@1 100.00 ( 99.62)\tAcc@5 100.00 ( 99.95)\n","Epoch: [133][ 60/391]\tTime  0.090 ( 0.096)\tLoss 1.0652e-02 (2.3500e-02)\tAcc@1 100.00 ( 99.56)\tAcc@5 100.00 ( 99.97)\n","Epoch: [133][ 90/391]\tTime  0.090 ( 0.095)\tLoss 1.3634e-02 (2.5172e-02)\tAcc@1 100.00 ( 99.53)\tAcc@5 100.00 ( 99.97)\n","Epoch: [133][120/391]\tTime  0.101 ( 0.094)\tLoss 4.0434e-02 (2.6162e-02)\tAcc@1  99.22 ( 99.50)\tAcc@5 100.00 ( 99.96)\n","Epoch: [133][150/391]\tTime  0.093 ( 0.094)\tLoss 4.5867e-02 (2.6911e-02)\tAcc@1  98.44 ( 99.48)\tAcc@5 100.00 ( 99.97)\n","Epoch: [133][180/391]\tTime  0.092 ( 0.094)\tLoss 3.7189e-02 (2.6680e-02)\tAcc@1  98.44 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","Epoch: [133][210/391]\tTime  0.091 ( 0.094)\tLoss 1.2439e-02 (2.6500e-02)\tAcc@1 100.00 ( 99.47)\tAcc@5 100.00 ( 99.96)\n","Epoch: [133][240/391]\tTime  0.090 ( 0.093)\tLoss 1.7649e-02 (2.6657e-02)\tAcc@1  99.22 ( 99.46)\tAcc@5 100.00 ( 99.95)\n","Epoch: [133][270/391]\tTime  0.093 ( 0.093)\tLoss 6.2778e-02 (2.6522e-02)\tAcc@1  98.44 ( 99.47)\tAcc@5 100.00 ( 99.95)\n","Epoch: [133][300/391]\tTime  0.092 ( 0.093)\tLoss 1.1022e-02 (2.6397e-02)\tAcc@1 100.00 ( 99.47)\tAcc@5 100.00 ( 99.96)\n","Epoch: [133][330/391]\tTime  0.100 ( 0.093)\tLoss 2.8624e-02 (2.6134e-02)\tAcc@1  98.44 ( 99.47)\tAcc@5 100.00 ( 99.96)\n","Epoch: [133][360/391]\tTime  0.088 ( 0.093)\tLoss 4.7383e-02 (2.6027e-02)\tAcc@1  99.22 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","Epoch: [133][390/391]\tTime  0.082 ( 0.093)\tLoss 1.8059e-02 (2.5917e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 99.482 || Acc@5 99.960\n","==> Test Accuracy:  Acc@1 78.440 || Acc@5 94.220\n","==> 39.33 seconds to train this epoch\n","\n","\n","----- epoch: 134, lr: 0.0008000000000000003 -----\n","Epoch: [134][  0/391]\tTime  0.312 ( 0.312)\tLoss 4.4030e-02 (4.4030e-02)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [134][ 30/391]\tTime  0.091 ( 0.099)\tLoss 4.0840e-02 (2.2826e-02)\tAcc@1  97.66 ( 99.52)\tAcc@5 100.00 (100.00)\n","Epoch: [134][ 60/391]\tTime  0.089 ( 0.096)\tLoss 2.1920e-02 (2.7227e-02)\tAcc@1  99.22 ( 99.39)\tAcc@5 100.00 ( 99.96)\n","Epoch: [134][ 90/391]\tTime  0.096 ( 0.095)\tLoss 1.5375e-02 (2.5126e-02)\tAcc@1 100.00 ( 99.47)\tAcc@5 100.00 ( 99.97)\n","Epoch: [134][120/391]\tTime  0.093 ( 0.094)\tLoss 3.3107e-02 (2.4224e-02)\tAcc@1  99.22 ( 99.50)\tAcc@5 100.00 ( 99.96)\n","Epoch: [134][150/391]\tTime  0.092 ( 0.094)\tLoss 9.4080e-03 (2.3613e-02)\tAcc@1 100.00 ( 99.52)\tAcc@5 100.00 ( 99.96)\n","Epoch: [134][180/391]\tTime  0.098 ( 0.094)\tLoss 9.1647e-03 (2.4084e-02)\tAcc@1 100.00 ( 99.51)\tAcc@5 100.00 ( 99.97)\n","Epoch: [134][210/391]\tTime  0.089 ( 0.094)\tLoss 4.0917e-02 (2.4731e-02)\tAcc@1  99.22 ( 99.47)\tAcc@5  99.22 ( 99.96)\n","Epoch: [134][240/391]\tTime  0.089 ( 0.093)\tLoss 8.0439e-03 (2.5005e-02)\tAcc@1 100.00 ( 99.47)\tAcc@5 100.00 ( 99.95)\n","Epoch: [134][270/391]\tTime  0.090 ( 0.093)\tLoss 1.0782e-02 (2.4759e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","Epoch: [134][300/391]\tTime  0.090 ( 0.093)\tLoss 2.6738e-02 (2.4796e-02)\tAcc@1  99.22 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","Epoch: [134][330/391]\tTime  0.088 ( 0.093)\tLoss 3.5730e-02 (2.4780e-02)\tAcc@1  99.22 ( 99.49)\tAcc@5 100.00 ( 99.96)\n","Epoch: [134][360/391]\tTime  0.091 ( 0.093)\tLoss 3.5902e-02 (2.4689e-02)\tAcc@1  99.22 ( 99.49)\tAcc@5 100.00 ( 99.96)\n","Epoch: [134][390/391]\tTime  0.082 ( 0.093)\tLoss 1.4882e-02 (2.4817e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 99.482 || Acc@5 99.958\n","==> Test Accuracy:  Acc@1 78.320 || Acc@5 94.250\n","==> 39.39 seconds to train this epoch\n","\n","\n","----- epoch: 135, lr: 0.0008000000000000003 -----\n","Epoch: [135][  0/391]\tTime  0.273 ( 0.273)\tLoss 8.1780e-02 (8.1780e-02)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [135][ 30/391]\tTime  0.094 ( 0.099)\tLoss 7.2057e-02 (2.9324e-02)\tAcc@1  99.22 ( 99.42)\tAcc@5  99.22 ( 99.95)\n","Epoch: [135][ 60/391]\tTime  0.090 ( 0.096)\tLoss 2.3128e-02 (2.6940e-02)\tAcc@1 100.00 ( 99.45)\tAcc@5 100.00 ( 99.97)\n","Epoch: [135][ 90/391]\tTime  0.091 ( 0.095)\tLoss 2.5514e-02 (2.6695e-02)\tAcc@1  99.22 ( 99.45)\tAcc@5 100.00 ( 99.97)\n","Epoch: [135][120/391]\tTime  0.087 ( 0.094)\tLoss 2.7191e-02 (2.6590e-02)\tAcc@1  98.44 ( 99.43)\tAcc@5 100.00 ( 99.97)\n","Epoch: [135][150/391]\tTime  0.094 ( 0.094)\tLoss 2.8973e-02 (2.7053e-02)\tAcc@1  99.22 ( 99.43)\tAcc@5 100.00 ( 99.96)\n","Epoch: [135][180/391]\tTime  0.092 ( 0.094)\tLoss 4.2909e-02 (2.6767e-02)\tAcc@1  99.22 ( 99.44)\tAcc@5  99.22 ( 99.96)\n","Epoch: [135][210/391]\tTime  0.097 ( 0.094)\tLoss 3.4432e-02 (2.7608e-02)\tAcc@1  99.22 ( 99.44)\tAcc@5 100.00 ( 99.95)\n","Epoch: [135][240/391]\tTime  0.091 ( 0.094)\tLoss 1.3994e-02 (2.7265e-02)\tAcc@1 100.00 ( 99.47)\tAcc@5 100.00 ( 99.95)\n","Epoch: [135][270/391]\tTime  0.093 ( 0.093)\tLoss 2.9310e-02 (2.6696e-02)\tAcc@1  99.22 ( 99.49)\tAcc@5 100.00 ( 99.96)\n","Epoch: [135][300/391]\tTime  0.092 ( 0.093)\tLoss 2.0401e-02 (2.6302e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 ( 99.96)\n","Epoch: [135][330/391]\tTime  0.091 ( 0.093)\tLoss 1.0174e-02 (2.5868e-02)\tAcc@1 100.00 ( 99.51)\tAcc@5 100.00 ( 99.96)\n","Epoch: [135][360/391]\tTime  0.093 ( 0.093)\tLoss 1.7715e-02 (2.5828e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 ( 99.97)\n","Epoch: [135][390/391]\tTime  0.081 ( 0.093)\tLoss 3.0892e-02 (2.5719e-02)\tAcc@1 100.00 ( 99.51)\tAcc@5 100.00 ( 99.97)\n","==> Train Accuracy: Acc@1 99.508 || Acc@5 99.966\n","==> Test Accuracy:  Acc@1 78.580 || Acc@5 94.030\n","==> 39.38 seconds to train this epoch\n","\n","\n","----- epoch: 136, lr: 0.0008000000000000003 -----\n","Epoch: [136][  0/391]\tTime  0.291 ( 0.291)\tLoss 6.8041e-03 (6.8041e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [136][ 30/391]\tTime  0.090 ( 0.099)\tLoss 9.2071e-03 (2.3888e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 ( 99.95)\n","Epoch: [136][ 60/391]\tTime  0.087 ( 0.096)\tLoss 1.9376e-02 (2.4513e-02)\tAcc@1 100.00 ( 99.54)\tAcc@5 100.00 ( 99.97)\n","Epoch: [136][ 90/391]\tTime  0.094 ( 0.095)\tLoss 8.2453e-03 (2.3371e-02)\tAcc@1 100.00 ( 99.61)\tAcc@5 100.00 ( 99.97)\n","Epoch: [136][120/391]\tTime  0.105 ( 0.094)\tLoss 1.0701e-02 (2.4059e-02)\tAcc@1 100.00 ( 99.58)\tAcc@5 100.00 ( 99.97)\n","Epoch: [136][150/391]\tTime  0.091 ( 0.094)\tLoss 3.3576e-02 (2.4656e-02)\tAcc@1  99.22 ( 99.57)\tAcc@5 100.00 ( 99.97)\n","Epoch: [136][180/391]\tTime  0.089 ( 0.094)\tLoss 2.3385e-02 (2.5142e-02)\tAcc@1 100.00 ( 99.54)\tAcc@5 100.00 ( 99.97)\n","Epoch: [136][210/391]\tTime  0.089 ( 0.093)\tLoss 2.6026e-02 (2.5508e-02)\tAcc@1  99.22 ( 99.52)\tAcc@5 100.00 ( 99.97)\n","Epoch: [136][240/391]\tTime  0.094 ( 0.093)\tLoss 6.3955e-02 (2.6236e-02)\tAcc@1  99.22 ( 99.50)\tAcc@5 100.00 ( 99.97)\n","Epoch: [136][270/391]\tTime  0.093 ( 0.093)\tLoss 4.6756e-02 (2.6199e-02)\tAcc@1  99.22 ( 99.50)\tAcc@5 100.00 ( 99.97)\n","Epoch: [136][300/391]\tTime  0.090 ( 0.093)\tLoss 3.5897e-02 (2.5895e-02)\tAcc@1  99.22 ( 99.51)\tAcc@5 100.00 ( 99.98)\n","Epoch: [136][330/391]\tTime  0.094 ( 0.093)\tLoss 3.2401e-02 (2.6041e-02)\tAcc@1  99.22 ( 99.50)\tAcc@5 100.00 ( 99.98)\n","Epoch: [136][360/391]\tTime  0.090 ( 0.093)\tLoss 3.6584e-02 (2.6722e-02)\tAcc@1  99.22 ( 99.49)\tAcc@5  99.22 ( 99.97)\n","Epoch: [136][390/391]\tTime  0.082 ( 0.093)\tLoss 7.8117e-02 (2.6434e-02)\tAcc@1  97.50 ( 99.49)\tAcc@5  98.75 ( 99.97)\n","==> Train Accuracy: Acc@1 99.494 || Acc@5 99.970\n","==> Test Accuracy:  Acc@1 78.490 || Acc@5 94.190\n","==> 39.41 seconds to train this epoch\n","\n","\n","----- epoch: 137, lr: 0.0008000000000000003 -----\n","Epoch: [137][  0/391]\tTime  0.284 ( 0.284)\tLoss 9.1506e-03 (9.1506e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [137][ 30/391]\tTime  0.095 ( 0.100)\tLoss 6.8641e-02 (2.0953e-02)\tAcc@1  98.44 ( 99.67)\tAcc@5  99.22 ( 99.95)\n","Epoch: [137][ 60/391]\tTime  0.094 ( 0.096)\tLoss 1.1695e-02 (2.2597e-02)\tAcc@1 100.00 ( 99.56)\tAcc@5 100.00 ( 99.95)\n","Epoch: [137][ 90/391]\tTime  0.089 ( 0.095)\tLoss 5.7429e-02 (2.3285e-02)\tAcc@1  98.44 ( 99.54)\tAcc@5 100.00 ( 99.97)\n","Epoch: [137][120/391]\tTime  0.095 ( 0.094)\tLoss 3.5741e-02 (2.6128e-02)\tAcc@1  99.22 ( 99.49)\tAcc@5 100.00 ( 99.94)\n","Epoch: [137][150/391]\tTime  0.089 ( 0.094)\tLoss 2.0111e-02 (2.6191e-02)\tAcc@1  99.22 ( 99.48)\tAcc@5 100.00 ( 99.94)\n","Epoch: [137][180/391]\tTime  0.103 ( 0.094)\tLoss 2.9883e-02 (2.6399e-02)\tAcc@1  99.22 ( 99.48)\tAcc@5 100.00 ( 99.94)\n","Epoch: [137][210/391]\tTime  0.090 ( 0.094)\tLoss 1.6133e-02 (2.5585e-02)\tAcc@1  99.22 ( 99.50)\tAcc@5 100.00 ( 99.95)\n","Epoch: [137][240/391]\tTime  0.092 ( 0.094)\tLoss 2.9382e-02 (2.5394e-02)\tAcc@1  98.44 ( 99.50)\tAcc@5 100.00 ( 99.96)\n","Epoch: [137][270/391]\tTime  0.107 ( 0.093)\tLoss 4.7544e-02 (2.5223e-02)\tAcc@1  99.22 ( 99.50)\tAcc@5  99.22 ( 99.96)\n","Epoch: [137][300/391]\tTime  0.090 ( 0.093)\tLoss 7.7263e-03 (2.5537e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 ( 99.96)\n","Epoch: [137][330/391]\tTime  0.088 ( 0.093)\tLoss 3.2179e-02 (2.5789e-02)\tAcc@1  98.44 ( 99.49)\tAcc@5 100.00 ( 99.96)\n","Epoch: [137][360/391]\tTime  0.090 ( 0.093)\tLoss 9.0412e-03 (2.5528e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 ( 99.96)\n","Epoch: [137][390/391]\tTime  0.083 ( 0.093)\tLoss 1.2224e-02 (2.5390e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 ( 99.97)\n","==> Train Accuracy: Acc@1 99.502 || Acc@5 99.966\n","==> Test Accuracy:  Acc@1 78.340 || Acc@5 94.220\n","==> 39.45 seconds to train this epoch\n","\n","\n","----- epoch: 138, lr: 0.0008000000000000003 -----\n","Epoch: [138][  0/391]\tTime  0.291 ( 0.291)\tLoss 1.2476e-02 (1.2476e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [138][ 30/391]\tTime  0.095 ( 0.099)\tLoss 2.9405e-02 (2.3661e-02)\tAcc@1  99.22 ( 99.65)\tAcc@5 100.00 (100.00)\n","Epoch: [138][ 60/391]\tTime  0.095 ( 0.096)\tLoss 2.2554e-02 (2.4223e-02)\tAcc@1 100.00 ( 99.62)\tAcc@5 100.00 ( 99.95)\n","Epoch: [138][ 90/391]\tTime  0.086 ( 0.095)\tLoss 2.8801e-02 (2.4046e-02)\tAcc@1  99.22 ( 99.62)\tAcc@5 100.00 ( 99.97)\n","Epoch: [138][120/391]\tTime  0.095 ( 0.094)\tLoss 2.4277e-02 (2.3624e-02)\tAcc@1 100.00 ( 99.62)\tAcc@5 100.00 ( 99.97)\n","Epoch: [138][150/391]\tTime  0.092 ( 0.094)\tLoss 1.4181e-02 (2.3986e-02)\tAcc@1 100.00 ( 99.59)\tAcc@5 100.00 ( 99.97)\n","Epoch: [138][180/391]\tTime  0.093 ( 0.094)\tLoss 5.0496e-02 (2.3560e-02)\tAcc@1  99.22 ( 99.60)\tAcc@5 100.00 ( 99.97)\n","Epoch: [138][210/391]\tTime  0.092 ( 0.094)\tLoss 1.8300e-02 (2.3054e-02)\tAcc@1  99.22 ( 99.61)\tAcc@5 100.00 ( 99.97)\n","Epoch: [138][240/391]\tTime  0.098 ( 0.093)\tLoss 1.9006e-02 (2.3453e-02)\tAcc@1 100.00 ( 99.61)\tAcc@5 100.00 ( 99.97)\n","Epoch: [138][270/391]\tTime  0.090 ( 0.093)\tLoss 2.3380e-02 (2.4039e-02)\tAcc@1  99.22 ( 99.58)\tAcc@5 100.00 ( 99.97)\n","Epoch: [138][300/391]\tTime  0.094 ( 0.093)\tLoss 2.2385e-02 (2.4072e-02)\tAcc@1  99.22 ( 99.59)\tAcc@5 100.00 ( 99.96)\n","Epoch: [138][330/391]\tTime  0.102 ( 0.093)\tLoss 9.8739e-03 (2.4098e-02)\tAcc@1 100.00 ( 99.58)\tAcc@5 100.00 ( 99.96)\n","Epoch: [138][360/391]\tTime  0.094 ( 0.093)\tLoss 7.9642e-03 (2.3866e-02)\tAcc@1 100.00 ( 99.59)\tAcc@5 100.00 ( 99.96)\n","Epoch: [138][390/391]\tTime  0.081 ( 0.093)\tLoss 2.1648e-02 (2.4076e-02)\tAcc@1 100.00 ( 99.57)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 99.574 || Acc@5 99.960\n","==> Test Accuracy:  Acc@1 78.600 || Acc@5 94.220\n","==> 39.45 seconds to train this epoch\n","\n","\n","----- epoch: 139, lr: 0.0008000000000000003 -----\n","Epoch: [139][  0/391]\tTime  0.322 ( 0.322)\tLoss 4.2835e-02 (4.2835e-02)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [139][ 30/391]\tTime  0.090 ( 0.100)\tLoss 1.2176e-02 (2.6313e-02)\tAcc@1 100.00 ( 99.34)\tAcc@5 100.00 ( 99.97)\n","Epoch: [139][ 60/391]\tTime  0.089 ( 0.096)\tLoss 2.4933e-02 (2.3403e-02)\tAcc@1  99.22 ( 99.53)\tAcc@5 100.00 ( 99.99)\n","Epoch: [139][ 90/391]\tTime  0.100 ( 0.095)\tLoss 5.8356e-02 (2.4580e-02)\tAcc@1  99.22 ( 99.52)\tAcc@5 100.00 ( 99.97)\n","Epoch: [139][120/391]\tTime  0.101 ( 0.096)\tLoss 2.3443e-02 (2.4309e-02)\tAcc@1  99.22 ( 99.54)\tAcc@5 100.00 ( 99.98)\n","Epoch: [139][150/391]\tTime  0.090 ( 0.096)\tLoss 2.8973e-02 (2.3123e-02)\tAcc@1 100.00 ( 99.59)\tAcc@5 100.00 ( 99.98)\n","Epoch: [139][180/391]\tTime  0.104 ( 0.095)\tLoss 1.4021e-02 (2.2265e-02)\tAcc@1 100.00 ( 99.62)\tAcc@5 100.00 ( 99.99)\n","Epoch: [139][210/391]\tTime  0.093 ( 0.095)\tLoss 2.0696e-02 (2.1940e-02)\tAcc@1  99.22 ( 99.63)\tAcc@5 100.00 ( 99.99)\n","Epoch: [139][240/391]\tTime  0.093 ( 0.094)\tLoss 6.3918e-02 (2.2077e-02)\tAcc@1  98.44 ( 99.62)\tAcc@5  99.22 ( 99.98)\n","Epoch: [139][270/391]\tTime  0.091 ( 0.094)\tLoss 1.5013e-02 (2.2615e-02)\tAcc@1 100.00 ( 99.61)\tAcc@5 100.00 ( 99.97)\n","Epoch: [139][300/391]\tTime  0.093 ( 0.094)\tLoss 2.3737e-02 (2.2866e-02)\tAcc@1 100.00 ( 99.60)\tAcc@5 100.00 ( 99.97)\n","Epoch: [139][330/391]\tTime  0.093 ( 0.094)\tLoss 2.7499e-02 (2.3087e-02)\tAcc@1 100.00 ( 99.59)\tAcc@5 100.00 ( 99.97)\n","Epoch: [139][360/391]\tTime  0.089 ( 0.094)\tLoss 3.6788e-02 (2.3254e-02)\tAcc@1  99.22 ( 99.59)\tAcc@5 100.00 ( 99.96)\n","Epoch: [139][390/391]\tTime  0.083 ( 0.094)\tLoss 2.7074e-02 (2.3264e-02)\tAcc@1 100.00 ( 99.58)\tAcc@5 100.00 ( 99.97)\n","==> Train Accuracy: Acc@1 99.584 || Acc@5 99.966\n","==> Test Accuracy:  Acc@1 78.430 || Acc@5 94.250\n","==> 39.70 seconds to train this epoch\n","\n","\n","----- epoch: 140, lr: 0.0008000000000000003 -----\n","Epoch: [140][  0/391]\tTime  0.282 ( 0.282)\tLoss 1.4783e-02 (1.4783e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [140][ 30/391]\tTime  0.097 ( 0.098)\tLoss 5.5501e-03 (2.4656e-02)\tAcc@1 100.00 ( 99.55)\tAcc@5 100.00 (100.00)\n","Epoch: [140][ 60/391]\tTime  0.097 ( 0.095)\tLoss 1.3711e-02 (2.4284e-02)\tAcc@1 100.00 ( 99.53)\tAcc@5 100.00 ( 99.99)\n","Epoch: [140][ 90/391]\tTime  0.099 ( 0.095)\tLoss 1.1476e-02 (2.5222e-02)\tAcc@1 100.00 ( 99.51)\tAcc@5 100.00 ( 99.97)\n","Epoch: [140][120/391]\tTime  0.091 ( 0.094)\tLoss 3.0804e-02 (2.3871e-02)\tAcc@1  99.22 ( 99.57)\tAcc@5 100.00 ( 99.97)\n","Epoch: [140][150/391]\tTime  0.095 ( 0.094)\tLoss 4.6764e-02 (2.4733e-02)\tAcc@1  98.44 ( 99.53)\tAcc@5 100.00 ( 99.97)\n","Epoch: [140][180/391]\tTime  0.093 ( 0.093)\tLoss 2.2298e-02 (2.4882e-02)\tAcc@1  99.22 ( 99.53)\tAcc@5 100.00 ( 99.98)\n","Epoch: [140][210/391]\tTime  0.088 ( 0.093)\tLoss 2.6049e-02 (2.4568e-02)\tAcc@1 100.00 ( 99.55)\tAcc@5 100.00 ( 99.98)\n","Epoch: [140][240/391]\tTime  0.090 ( 0.093)\tLoss 5.8530e-02 (2.4674e-02)\tAcc@1  99.22 ( 99.54)\tAcc@5 100.00 ( 99.98)\n","Epoch: [140][270/391]\tTime  0.096 ( 0.093)\tLoss 1.3927e-02 (2.4377e-02)\tAcc@1 100.00 ( 99.54)\tAcc@5 100.00 ( 99.98)\n","Epoch: [140][300/391]\tTime  0.085 ( 0.093)\tLoss 1.2634e-02 (2.4563e-02)\tAcc@1 100.00 ( 99.53)\tAcc@5 100.00 ( 99.98)\n","Epoch: [140][330/391]\tTime  0.093 ( 0.093)\tLoss 4.0980e-02 (2.4586e-02)\tAcc@1  99.22 ( 99.53)\tAcc@5 100.00 ( 99.98)\n","Epoch: [140][360/391]\tTime  0.102 ( 0.093)\tLoss 2.9888e-02 (2.5079e-02)\tAcc@1  98.44 ( 99.51)\tAcc@5 100.00 ( 99.97)\n","Epoch: [140][390/391]\tTime  0.082 ( 0.093)\tLoss 2.1462e-02 (2.5145e-02)\tAcc@1 100.00 ( 99.51)\tAcc@5 100.00 ( 99.97)\n","==> Train Accuracy: Acc@1 99.506 || Acc@5 99.968\n","==> Test Accuracy:  Acc@1 78.280 || Acc@5 94.160\n","==> 39.40 seconds to train this epoch\n","\n","\n","----- epoch: 141, lr: 0.0008000000000000003 -----\n","Epoch: [141][  0/391]\tTime  0.289 ( 0.289)\tLoss 2.5978e-02 (2.5978e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [141][ 30/391]\tTime  0.092 ( 0.099)\tLoss 1.4098e-02 (2.5683e-02)\tAcc@1 100.00 ( 99.57)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][ 60/391]\tTime  0.094 ( 0.096)\tLoss 2.2699e-02 (2.5887e-02)\tAcc@1  99.22 ( 99.49)\tAcc@5 100.00 ( 99.95)\n","Epoch: [141][ 90/391]\tTime  0.090 ( 0.095)\tLoss 9.8549e-03 (2.4375e-02)\tAcc@1 100.00 ( 99.53)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][120/391]\tTime  0.096 ( 0.094)\tLoss 1.7125e-02 (2.3244e-02)\tAcc@1 100.00 ( 99.57)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][150/391]\tTime  0.091 ( 0.094)\tLoss 2.0213e-02 (2.3740e-02)\tAcc@1  99.22 ( 99.54)\tAcc@5 100.00 ( 99.98)\n","Epoch: [141][180/391]\tTime  0.093 ( 0.094)\tLoss 2.1733e-02 (2.3368e-02)\tAcc@1  99.22 ( 99.56)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][210/391]\tTime  0.102 ( 0.094)\tLoss 1.3751e-02 (2.3228e-02)\tAcc@1 100.00 ( 99.56)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][240/391]\tTime  0.091 ( 0.093)\tLoss 6.7750e-02 (2.3561e-02)\tAcc@1  96.88 ( 99.55)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][270/391]\tTime  0.090 ( 0.093)\tLoss 8.8346e-02 (2.4028e-02)\tAcc@1  97.66 ( 99.53)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][300/391]\tTime  0.089 ( 0.093)\tLoss 3.3647e-02 (2.3830e-02)\tAcc@1  99.22 ( 99.54)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][330/391]\tTime  0.092 ( 0.093)\tLoss 1.0245e-02 (2.3316e-02)\tAcc@1 100.00 ( 99.56)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][360/391]\tTime  0.097 ( 0.093)\tLoss 2.2411e-02 (2.3961e-02)\tAcc@1  99.22 ( 99.55)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][390/391]\tTime  0.082 ( 0.093)\tLoss 1.7764e-02 (2.4055e-02)\tAcc@1 100.00 ( 99.56)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 99.556 || Acc@5 99.962\n","==> Test Accuracy:  Acc@1 78.520 || Acc@5 94.040\n","==> 39.39 seconds to train this epoch\n","\n","\n","----- epoch: 142, lr: 0.0008000000000000003 -----\n","Epoch: [142][  0/391]\tTime  0.277 ( 0.277)\tLoss 4.3644e-02 (4.3644e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [142][ 30/391]\tTime  0.091 ( 0.099)\tLoss 1.9333e-02 (2.4588e-02)\tAcc@1 100.00 ( 99.52)\tAcc@5 100.00 ( 99.97)\n","Epoch: [142][ 60/391]\tTime  0.101 ( 0.096)\tLoss 4.0590e-02 (2.2997e-02)\tAcc@1  99.22 ( 99.55)\tAcc@5 100.00 ( 99.97)\n","Epoch: [142][ 90/391]\tTime  0.090 ( 0.095)\tLoss 1.2129e-02 (2.3574e-02)\tAcc@1 100.00 ( 99.56)\tAcc@5 100.00 ( 99.97)\n","Epoch: [142][120/391]\tTime  0.092 ( 0.094)\tLoss 9.7094e-03 (2.3576e-02)\tAcc@1 100.00 ( 99.55)\tAcc@5 100.00 ( 99.97)\n","Epoch: [142][150/391]\tTime  0.090 ( 0.094)\tLoss 1.4743e-02 (2.4650e-02)\tAcc@1 100.00 ( 99.52)\tAcc@5 100.00 ( 99.96)\n","Epoch: [142][180/391]\tTime  0.092 ( 0.094)\tLoss 3.5405e-02 (2.4322e-02)\tAcc@1  99.22 ( 99.53)\tAcc@5 100.00 ( 99.97)\n","Epoch: [142][210/391]\tTime  0.092 ( 0.093)\tLoss 3.8536e-02 (2.4837e-02)\tAcc@1  98.44 ( 99.51)\tAcc@5 100.00 ( 99.96)\n","Epoch: [142][240/391]\tTime  0.092 ( 0.093)\tLoss 1.8868e-02 (2.4554e-02)\tAcc@1 100.00 ( 99.53)\tAcc@5 100.00 ( 99.96)\n","Epoch: [142][270/391]\tTime  0.086 ( 0.093)\tLoss 1.2736e-02 (2.4273e-02)\tAcc@1 100.00 ( 99.54)\tAcc@5 100.00 ( 99.96)\n","Epoch: [142][300/391]\tTime  0.092 ( 0.093)\tLoss 5.4090e-02 (2.4291e-02)\tAcc@1  98.44 ( 99.54)\tAcc@5 100.00 ( 99.96)\n","Epoch: [142][330/391]\tTime  0.088 ( 0.093)\tLoss 6.4317e-02 (2.4198e-02)\tAcc@1  97.66 ( 99.53)\tAcc@5 100.00 ( 99.96)\n","Epoch: [142][360/391]\tTime  0.094 ( 0.093)\tLoss 4.3519e-02 (2.4517e-02)\tAcc@1  99.22 ( 99.53)\tAcc@5 100.00 ( 99.96)\n","Epoch: [142][390/391]\tTime  0.082 ( 0.093)\tLoss 1.9761e-02 (2.4470e-02)\tAcc@1 100.00 ( 99.53)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 99.526 || Acc@5 99.962\n","==> Test Accuracy:  Acc@1 78.420 || Acc@5 94.160\n","==> 39.39 seconds to train this epoch\n","\n","\n","----- epoch: 143, lr: 0.0008000000000000003 -----\n","Epoch: [143][  0/391]\tTime  0.281 ( 0.281)\tLoss 1.6763e-02 (1.6763e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [143][ 30/391]\tTime  0.092 ( 0.100)\tLoss 1.1252e-02 (2.5186e-02)\tAcc@1 100.00 ( 99.60)\tAcc@5 100.00 ( 99.95)\n","Epoch: [143][ 60/391]\tTime  0.087 ( 0.096)\tLoss 1.9739e-02 (2.4155e-02)\tAcc@1 100.00 ( 99.55)\tAcc@5 100.00 ( 99.96)\n","Epoch: [143][ 90/391]\tTime  0.100 ( 0.095)\tLoss 3.7845e-02 (2.4212e-02)\tAcc@1  98.44 ( 99.50)\tAcc@5 100.00 ( 99.97)\n","Epoch: [143][120/391]\tTime  0.091 ( 0.095)\tLoss 7.1223e-02 (2.5592e-02)\tAcc@1  98.44 ( 99.46)\tAcc@5 100.00 ( 99.96)\n","Epoch: [143][150/391]\tTime  0.093 ( 0.094)\tLoss 8.8925e-03 (2.6369e-02)\tAcc@1 100.00 ( 99.44)\tAcc@5 100.00 ( 99.96)\n","Epoch: [143][180/391]\tTime  0.091 ( 0.094)\tLoss 4.1113e-02 (2.6451e-02)\tAcc@1  98.44 ( 99.42)\tAcc@5 100.00 ( 99.96)\n","Epoch: [143][210/391]\tTime  0.089 ( 0.094)\tLoss 1.4991e-02 (2.5742e-02)\tAcc@1  99.22 ( 99.43)\tAcc@5 100.00 ( 99.96)\n","Epoch: [143][240/391]\tTime  0.103 ( 0.094)\tLoss 1.1829e-02 (2.5630e-02)\tAcc@1 100.00 ( 99.44)\tAcc@5 100.00 ( 99.96)\n","Epoch: [143][270/391]\tTime  0.091 ( 0.094)\tLoss 1.2993e-02 (2.5877e-02)\tAcc@1 100.00 ( 99.45)\tAcc@5 100.00 ( 99.95)\n","Epoch: [143][300/391]\tTime  0.091 ( 0.093)\tLoss 2.8072e-02 (2.5642e-02)\tAcc@1 100.00 ( 99.46)\tAcc@5 100.00 ( 99.95)\n","Epoch: [143][330/391]\tTime  0.091 ( 0.093)\tLoss 2.1666e-02 (2.5349e-02)\tAcc@1  99.22 ( 99.47)\tAcc@5 100.00 ( 99.96)\n","Epoch: [143][360/391]\tTime  0.093 ( 0.093)\tLoss 3.7445e-02 (2.5068e-02)\tAcc@1  99.22 ( 99.50)\tAcc@5 100.00 ( 99.95)\n","Epoch: [143][390/391]\tTime  0.082 ( 0.093)\tLoss 3.4905e-02 (2.5115e-02)\tAcc@1  98.75 ( 99.50)\tAcc@5 100.00 ( 99.95)\n","==> Train Accuracy: Acc@1 99.496 || Acc@5 99.952\n","==> Test Accuracy:  Acc@1 78.450 || Acc@5 94.090\n","==> 39.46 seconds to train this epoch\n","\n","\n","----- epoch: 144, lr: 0.0008000000000000003 -----\n","Epoch: [144][  0/391]\tTime  0.303 ( 0.303)\tLoss 7.6107e-03 (7.6107e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [144][ 30/391]\tTime  0.091 ( 0.100)\tLoss 3.1402e-02 (1.9306e-02)\tAcc@1  99.22 ( 99.77)\tAcc@5 100.00 (100.00)\n","Epoch: [144][ 60/391]\tTime  0.097 ( 0.096)\tLoss 1.3411e-02 (2.1979e-02)\tAcc@1 100.00 ( 99.67)\tAcc@5 100.00 ( 99.97)\n","Epoch: [144][ 90/391]\tTime  0.100 ( 0.095)\tLoss 1.0804e-02 (2.4469e-02)\tAcc@1 100.00 ( 99.54)\tAcc@5 100.00 ( 99.97)\n","Epoch: [144][120/391]\tTime  0.090 ( 0.094)\tLoss 1.8163e-02 (2.3709e-02)\tAcc@1 100.00 ( 99.55)\tAcc@5 100.00 ( 99.97)\n","Epoch: [144][150/391]\tTime  0.097 ( 0.094)\tLoss 2.9540e-02 (2.3309e-02)\tAcc@1  99.22 ( 99.57)\tAcc@5 100.00 ( 99.97)\n","Epoch: [144][180/391]\tTime  0.094 ( 0.094)\tLoss 1.0659e-02 (2.3528e-02)\tAcc@1 100.00 ( 99.56)\tAcc@5 100.00 ( 99.97)\n","Epoch: [144][210/391]\tTime  0.095 ( 0.094)\tLoss 3.0026e-02 (2.3561e-02)\tAcc@1  99.22 ( 99.56)\tAcc@5 100.00 ( 99.98)\n","Epoch: [144][240/391]\tTime  0.096 ( 0.094)\tLoss 3.1809e-02 (2.3917e-02)\tAcc@1 100.00 ( 99.55)\tAcc@5 100.00 ( 99.98)\n","Epoch: [144][270/391]\tTime  0.094 ( 0.094)\tLoss 1.7935e-02 (2.4098e-02)\tAcc@1 100.00 ( 99.54)\tAcc@5 100.00 ( 99.98)\n","Epoch: [144][300/391]\tTime  0.089 ( 0.094)\tLoss 9.3124e-03 (2.4022e-02)\tAcc@1 100.00 ( 99.54)\tAcc@5 100.00 ( 99.97)\n","Epoch: [144][330/391]\tTime  0.094 ( 0.093)\tLoss 2.9625e-02 (2.3870e-02)\tAcc@1  99.22 ( 99.54)\tAcc@5 100.00 ( 99.98)\n","Epoch: [144][360/391]\tTime  0.089 ( 0.093)\tLoss 2.0569e-02 (2.3982e-02)\tAcc@1  99.22 ( 99.53)\tAcc@5 100.00 ( 99.98)\n","Epoch: [144][390/391]\tTime  0.089 ( 0.093)\tLoss 6.9305e-02 (2.4137e-02)\tAcc@1  97.50 ( 99.53)\tAcc@5 100.00 ( 99.98)\n","==> Train Accuracy: Acc@1 99.528 || Acc@5 99.976\n","==> Test Accuracy:  Acc@1 78.400 || Acc@5 94.050\n","==> 39.50 seconds to train this epoch\n","\n","\n","----- epoch: 145, lr: 0.0008000000000000003 -----\n","Epoch: [145][  0/391]\tTime  0.295 ( 0.295)\tLoss 4.4654e-02 (4.4654e-02)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [145][ 30/391]\tTime  0.090 ( 0.099)\tLoss 3.2769e-02 (2.3202e-02)\tAcc@1  99.22 ( 99.42)\tAcc@5 100.00 ( 99.97)\n","Epoch: [145][ 60/391]\tTime  0.093 ( 0.096)\tLoss 5.1685e-02 (2.6497e-02)\tAcc@1  99.22 ( 99.41)\tAcc@5  99.22 ( 99.95)\n","Epoch: [145][ 90/391]\tTime  0.090 ( 0.095)\tLoss 5.6220e-02 (2.5474e-02)\tAcc@1  98.44 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","Epoch: [145][120/391]\tTime  0.092 ( 0.094)\tLoss 2.2739e-02 (2.6221e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","Epoch: [145][150/391]\tTime  0.091 ( 0.094)\tLoss 1.4019e-02 (2.5492e-02)\tAcc@1 100.00 ( 99.49)\tAcc@5 100.00 ( 99.96)\n","Epoch: [145][180/391]\tTime  0.091 ( 0.094)\tLoss 9.8679e-03 (2.5686e-02)\tAcc@1 100.00 ( 99.49)\tAcc@5 100.00 ( 99.95)\n","Epoch: [145][210/391]\tTime  0.101 ( 0.094)\tLoss 6.4098e-03 (2.5778e-02)\tAcc@1 100.00 ( 99.49)\tAcc@5 100.00 ( 99.95)\n","Epoch: [145][240/391]\tTime  0.090 ( 0.093)\tLoss 2.2478e-02 (2.6023e-02)\tAcc@1  99.22 ( 99.49)\tAcc@5 100.00 ( 99.95)\n","Epoch: [145][270/391]\tTime  0.093 ( 0.093)\tLoss 3.4780e-02 (2.5654e-02)\tAcc@1  99.22 ( 99.50)\tAcc@5 100.00 ( 99.95)\n","Epoch: [145][300/391]\tTime  0.091 ( 0.093)\tLoss 3.2652e-02 (2.5962e-02)\tAcc@1  99.22 ( 99.49)\tAcc@5 100.00 ( 99.95)\n","Epoch: [145][330/391]\tTime  0.092 ( 0.093)\tLoss 1.7478e-02 (2.5807e-02)\tAcc@1 100.00 ( 99.49)\tAcc@5 100.00 ( 99.95)\n","Epoch: [145][360/391]\tTime  0.104 ( 0.093)\tLoss 1.5405e-02 (2.5614e-02)\tAcc@1 100.00 ( 99.49)\tAcc@5 100.00 ( 99.95)\n","Epoch: [145][390/391]\tTime  0.085 ( 0.093)\tLoss 9.2220e-03 (2.5302e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 ( 99.95)\n","==> Train Accuracy: Acc@1 99.502 || Acc@5 99.950\n","==> Test Accuracy:  Acc@1 78.330 || Acc@5 94.220\n","==> 39.39 seconds to train this epoch\n","\n","\n","----- epoch: 146, lr: 0.0008000000000000003 -----\n","Epoch: [146][  0/391]\tTime  0.301 ( 0.301)\tLoss 4.4257e-02 (4.4257e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5  99.22 ( 99.22)\n","Epoch: [146][ 30/391]\tTime  0.091 ( 0.099)\tLoss 3.6990e-02 (2.5498e-02)\tAcc@1  99.22 ( 99.60)\tAcc@5 100.00 ( 99.97)\n","Epoch: [146][ 60/391]\tTime  0.094 ( 0.096)\tLoss 1.0197e-02 (2.3816e-02)\tAcc@1 100.00 ( 99.60)\tAcc@5 100.00 ( 99.97)\n","Epoch: [146][ 90/391]\tTime  0.091 ( 0.095)\tLoss 2.4298e-02 (2.4069e-02)\tAcc@1  99.22 ( 99.56)\tAcc@5 100.00 ( 99.98)\n","Epoch: [146][120/391]\tTime  0.092 ( 0.094)\tLoss 9.8440e-03 (2.3641e-02)\tAcc@1 100.00 ( 99.57)\tAcc@5 100.00 ( 99.99)\n","Epoch: [146][150/391]\tTime  0.092 ( 0.094)\tLoss 2.4248e-02 (2.2761e-02)\tAcc@1 100.00 ( 99.60)\tAcc@5 100.00 ( 99.99)\n","Epoch: [146][180/391]\tTime  0.091 ( 0.094)\tLoss 1.5049e-02 (2.2699e-02)\tAcc@1 100.00 ( 99.60)\tAcc@5 100.00 ( 99.99)\n","Epoch: [146][210/391]\tTime  0.087 ( 0.094)\tLoss 1.5705e-02 (2.2348e-02)\tAcc@1 100.00 ( 99.61)\tAcc@5 100.00 ( 99.98)\n","Epoch: [146][240/391]\tTime  0.092 ( 0.093)\tLoss 1.8543e-02 (2.2463e-02)\tAcc@1  99.22 ( 99.60)\tAcc@5 100.00 ( 99.98)\n","Epoch: [146][270/391]\tTime  0.099 ( 0.093)\tLoss 1.0697e-02 (2.2913e-02)\tAcc@1 100.00 ( 99.59)\tAcc@5 100.00 ( 99.97)\n","Epoch: [146][300/391]\tTime  0.090 ( 0.093)\tLoss 3.1921e-02 (2.2782e-02)\tAcc@1  99.22 ( 99.58)\tAcc@5 100.00 ( 99.97)\n","Epoch: [146][330/391]\tTime  0.089 ( 0.093)\tLoss 6.5588e-02 (2.2958e-02)\tAcc@1  99.22 ( 99.57)\tAcc@5  99.22 ( 99.97)\n","Epoch: [146][360/391]\tTime  0.093 ( 0.093)\tLoss 3.2465e-02 (2.3444e-02)\tAcc@1  98.44 ( 99.56)\tAcc@5 100.00 ( 99.97)\n","Epoch: [146][390/391]\tTime  0.082 ( 0.093)\tLoss 2.5190e-02 (2.3286e-02)\tAcc@1  98.75 ( 99.57)\tAcc@5 100.00 ( 99.97)\n","==> Train Accuracy: Acc@1 99.566 || Acc@5 99.970\n","==> Test Accuracy:  Acc@1 78.330 || Acc@5 94.230\n","==> 39.49 seconds to train this epoch\n","\n","\n","----- epoch: 147, lr: 0.0008000000000000003 -----\n","Epoch: [147][  0/391]\tTime  0.300 ( 0.300)\tLoss 1.7047e-02 (1.7047e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [147][ 30/391]\tTime  0.089 ( 0.099)\tLoss 2.8924e-02 (2.2066e-02)\tAcc@1  99.22 ( 99.57)\tAcc@5 100.00 (100.00)\n","Epoch: [147][ 60/391]\tTime  0.092 ( 0.096)\tLoss 8.6541e-02 (2.4570e-02)\tAcc@1  98.44 ( 99.54)\tAcc@5 100.00 ( 99.97)\n","Epoch: [147][ 90/391]\tTime  0.093 ( 0.095)\tLoss 2.1674e-02 (2.3485e-02)\tAcc@1  99.22 ( 99.55)\tAcc@5 100.00 ( 99.98)\n","Epoch: [147][120/391]\tTime  0.091 ( 0.094)\tLoss 1.0264e-02 (2.3197e-02)\tAcc@1 100.00 ( 99.54)\tAcc@5 100.00 ( 99.99)\n","Epoch: [147][150/391]\tTime  0.095 ( 0.094)\tLoss 1.3975e-02 (2.2189e-02)\tAcc@1 100.00 ( 99.56)\tAcc@5 100.00 ( 99.99)\n","Epoch: [147][180/391]\tTime  0.099 ( 0.094)\tLoss 1.1482e-02 (2.2118e-02)\tAcc@1 100.00 ( 99.57)\tAcc@5 100.00 ( 99.99)\n","Epoch: [147][210/391]\tTime  0.096 ( 0.094)\tLoss 2.9767e-02 (2.1806e-02)\tAcc@1  99.22 ( 99.58)\tAcc@5 100.00 ( 99.99)\n","Epoch: [147][240/391]\tTime  0.093 ( 0.093)\tLoss 4.3533e-02 (2.2099e-02)\tAcc@1  98.44 ( 99.57)\tAcc@5 100.00 ( 99.99)\n","Epoch: [147][270/391]\tTime  0.094 ( 0.093)\tLoss 1.1264e-02 (2.2220e-02)\tAcc@1 100.00 ( 99.57)\tAcc@5 100.00 ( 99.98)\n","Epoch: [147][300/391]\tTime  0.090 ( 0.093)\tLoss 5.0962e-02 (2.2281e-02)\tAcc@1  98.44 ( 99.56)\tAcc@5 100.00 ( 99.98)\n","Epoch: [147][330/391]\tTime  0.093 ( 0.093)\tLoss 7.3033e-03 (2.2050e-02)\tAcc@1 100.00 ( 99.58)\tAcc@5 100.00 ( 99.98)\n","Epoch: [147][360/391]\tTime  0.093 ( 0.093)\tLoss 1.5597e-02 (2.2076e-02)\tAcc@1 100.00 ( 99.57)\tAcc@5 100.00 ( 99.98)\n","Epoch: [147][390/391]\tTime  0.090 ( 0.093)\tLoss 1.9601e-02 (2.2236e-02)\tAcc@1 100.00 ( 99.57)\tAcc@5 100.00 ( 99.98)\n","==> Train Accuracy: Acc@1 99.572 || Acc@5 99.982\n","==> Test Accuracy:  Acc@1 78.410 || Acc@5 94.230\n","==> 39.47 seconds to train this epoch\n","\n","\n","----- epoch: 148, lr: 0.0008000000000000003 -----\n","Epoch: [148][  0/391]\tTime  0.291 ( 0.291)\tLoss 6.8575e-02 (6.8575e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [148][ 30/391]\tTime  0.089 ( 0.100)\tLoss 1.2744e-02 (2.6849e-02)\tAcc@1 100.00 ( 99.42)\tAcc@5 100.00 ( 99.97)\n","Epoch: [148][ 60/391]\tTime  0.095 ( 0.096)\tLoss 1.0911e-02 (2.5724e-02)\tAcc@1 100.00 ( 99.47)\tAcc@5 100.00 ( 99.97)\n","Epoch: [148][ 90/391]\tTime  0.090 ( 0.095)\tLoss 5.4938e-02 (2.6096e-02)\tAcc@1  98.44 ( 99.45)\tAcc@5 100.00 ( 99.96)\n","Epoch: [148][120/391]\tTime  0.091 ( 0.094)\tLoss 2.6524e-02 (2.6192e-02)\tAcc@1  99.22 ( 99.46)\tAcc@5 100.00 ( 99.95)\n","Epoch: [148][150/391]\tTime  0.103 ( 0.094)\tLoss 2.1776e-02 (2.5986e-02)\tAcc@1  99.22 ( 99.47)\tAcc@5 100.00 ( 99.95)\n","Epoch: [148][180/391]\tTime  0.097 ( 0.094)\tLoss 1.0546e-02 (2.5282e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 ( 99.96)\n","Epoch: [148][210/391]\tTime  0.089 ( 0.094)\tLoss 1.6908e-02 (2.5763e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.96)\n","Epoch: [148][240/391]\tTime  0.090 ( 0.094)\tLoss 1.6388e-02 (2.5181e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 ( 99.96)\n","Epoch: [148][270/391]\tTime  0.094 ( 0.093)\tLoss 1.7037e-02 (2.5228e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.97)\n","Epoch: [148][300/391]\tTime  0.092 ( 0.093)\tLoss 2.9660e-02 (2.5289e-02)\tAcc@1  99.22 ( 99.48)\tAcc@5 100.00 ( 99.97)\n","Epoch: [148][330/391]\tTime  0.091 ( 0.093)\tLoss 7.4855e-03 (2.5234e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.97)\n","Epoch: [148][360/391]\tTime  0.100 ( 0.093)\tLoss 1.9761e-02 (2.4840e-02)\tAcc@1  99.22 ( 99.51)\tAcc@5 100.00 ( 99.97)\n","Epoch: [148][390/391]\tTime  0.081 ( 0.093)\tLoss 1.8666e-02 (2.4969e-02)\tAcc@1 100.00 ( 99.51)\tAcc@5 100.00 ( 99.97)\n","==> Train Accuracy: Acc@1 99.506 || Acc@5 99.968\n","==> Test Accuracy:  Acc@1 78.300 || Acc@5 94.150\n","==> 39.54 seconds to train this epoch\n","\n","\n","----- epoch: 149, lr: 0.0008000000000000003 -----\n","Epoch: [149][  0/391]\tTime  0.292 ( 0.292)\tLoss 3.9164e-02 (3.9164e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [149][ 30/391]\tTime  0.089 ( 0.100)\tLoss 1.2248e-02 (2.4651e-02)\tAcc@1 100.00 ( 99.37)\tAcc@5 100.00 (100.00)\n","Epoch: [149][ 60/391]\tTime  0.088 ( 0.096)\tLoss 1.2305e-02 (2.5902e-02)\tAcc@1 100.00 ( 99.44)\tAcc@5 100.00 ( 99.96)\n","Epoch: [149][ 90/391]\tTime  0.093 ( 0.096)\tLoss 1.6798e-02 (2.5284e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 ( 99.97)\n","Epoch: [149][120/391]\tTime  0.092 ( 0.095)\tLoss 2.7753e-02 (2.4907e-02)\tAcc@1  99.22 ( 99.52)\tAcc@5 100.00 ( 99.97)\n","Epoch: [149][150/391]\tTime  0.092 ( 0.095)\tLoss 1.5614e-02 (2.5001e-02)\tAcc@1 100.00 ( 99.51)\tAcc@5 100.00 ( 99.97)\n","Epoch: [149][180/391]\tTime  0.101 ( 0.094)\tLoss 1.5341e-02 (2.4735e-02)\tAcc@1 100.00 ( 99.52)\tAcc@5 100.00 ( 99.97)\n","Epoch: [149][210/391]\tTime  0.097 ( 0.094)\tLoss 1.6655e-02 (2.5212e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 ( 99.97)\n","Epoch: [149][240/391]\tTime  0.093 ( 0.094)\tLoss 2.5688e-02 (2.5122e-02)\tAcc@1  98.44 ( 99.51)\tAcc@5 100.00 ( 99.97)\n","Epoch: [149][270/391]\tTime  0.097 ( 0.094)\tLoss 9.8264e-03 (2.5240e-02)\tAcc@1 100.00 ( 99.51)\tAcc@5 100.00 ( 99.97)\n","Epoch: [149][300/391]\tTime  0.088 ( 0.094)\tLoss 1.0484e-02 (2.5022e-02)\tAcc@1 100.00 ( 99.51)\tAcc@5 100.00 ( 99.97)\n","Epoch: [149][330/391]\tTime  0.093 ( 0.094)\tLoss 9.7132e-03 (2.4405e-02)\tAcc@1 100.00 ( 99.53)\tAcc@5 100.00 ( 99.97)\n","Epoch: [149][360/391]\tTime  0.095 ( 0.094)\tLoss 1.1792e-02 (2.4115e-02)\tAcc@1 100.00 ( 99.54)\tAcc@5 100.00 ( 99.97)\n","Epoch: [149][390/391]\tTime  0.082 ( 0.094)\tLoss 1.0806e-02 (2.4135e-02)\tAcc@1 100.00 ( 99.54)\tAcc@5 100.00 ( 99.97)\n","==> Train Accuracy: Acc@1 99.538 || Acc@5 99.974\n","==> Test Accuracy:  Acc@1 78.170 || Acc@5 94.110\n","==> 39.73 seconds to train this epoch\n","\n","Best Top-1 Accuracy: 78.6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bGXAgOkd9SLu"},"source":["# 혹시, 끊어진다면 저장된 checkpoint부터 다시 시작"]},{"cell_type":"code","metadata":{"id":"SdMZidnH9Hcr"},"source":["model = ResNet34(num_classes=num_classes).cuda()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9, nesterov=True, weight_decay=5e-4)\n","scheduler = MultiStepLR(optimizer, milestones=[60, 90, 120], gamma=0.2)\n","\n","checkpoint = load_model(model, path, augmentation_method)\n","start_epoch = checkpoint['epoch']\n","optimizer.load_state_dict(checkpoint['optimizer'])\n","scheduler.load_state_dict(checkpoint['scheduler'])\n","\n","\n","criterion = torch.nn.CrossEntropyLoss().cuda()\n","###########################################################\n","best_acc = 0\n","for epoch in range(start_epoch, epochs):\n","    print(\"\\n----- epoch: {}, lr: {} -----\".format(\n","        epoch, optimizer.param_groups[0][\"lr\"]))\n","\n","    # train for one epoch\n","    start_time = time.time()\n","    train_acc1, train_acc5 = train(train_loader, epoch, model, optimizer, criterion)\n","    test_acc1, test_acc5 = test(test_loader,epoch,model)\n","\n","    elapsed_time = time.time() - start_time\n","    print('==> {:.2f} seconds to train this epoch\\n'.format(elapsed_time))\n","    # learning rate scheduling\n","    scheduler.step()\n","    \n","    # Save model for best accuracy\n","    if best_acc < test_acc1:\n","        best_acc = test_acc1\n","        state = {'epoch': epoch + 1,\n","                 'model': model.state_dict(),\n","                 'optimizer': optimizer.state_dict(),\n","                 'scheduler' : scheduler.state_dict()\n","                }\n","        save_model(state, path, augmentation_method)\n","    save_values(str(epoch + 1), str(train_acc1), str(train_acc5), str(test_acc1), str(test_acc5), path, augmentation_method)\n","        \n","torch.save(model.state_dict(),'model_latest.pt')\n","print(f\"Best Top-1 Accuracy: {best_acc}\")\n","\n"],"execution_count":null,"outputs":[]}]}